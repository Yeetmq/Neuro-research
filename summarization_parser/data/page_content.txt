If you want to ride the next big wave in AI, grab a transformer. Theyre not the shape-shifting toy robots on TV or the trash-can-sized tubs on telephone poles. So, Whats a Transformer Model? A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. Transformer models apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant data elements in a series influence and depend on each other. First described in a 2017 paper from Google, transformers are among the newest and one of the most powerful classes of models invented to date. Theyre driving a wave of advances in machine learning some have dubbed transformer AI. Stanford researchers called transformers foundation models in an August 2021 paper because they see them driving a paradigm shift in AI. The sheer scale and scope of foundation models over the last few years have stretched our imagination of what is possible, they wrote. What Can Transformer Models Do? Transformers are translating text and speech in near real-time, opening meetings and classrooms to diverse and hearing-impaired attendees. Theyre helping researchers understand the chains of genes in DNA and amino acids in proteins in ways that can speed drug design. Transformers can detect trends and anomalies to prevent fraud, streamline manufacturing, make online recommendations or improve healthcare. People use transformers every time they search on Google or Microsoft Bing. The Virtuous Cycle of Transformer AI Any application using sequential text, image or video data is a candidate for transformer models. That enables these models to ride a virtuous cycle in transformer AI. Created with large datasets, transformers make accurate predictions that drive their wider use, generating more data that can be used to create even better models. Transformers made self-supervised learning possible, and AI jumped to warp speed, said NVIDIA founder and CEO Jensen Huang in his keynote address this week at GTC. Transformers Replace CNNs, RNNs Transformers are in many cases replacing convolutional and recurrent neural networks (CNNs and RNNs), the most popular types of deep learning models just five years ago. Indeed, 70 percent of arXiv papers on AI posted in the last two years mention transformers. Thats a radical shift from a 2017 IEEE study that reported RNNs and CNNs were the most popular models for pattern recognition. No Labels, More Performance Before transformers arrived, users had to train neural networks with large, labeled datasets that were costly and time-consuming to produce. By finding patterns between elements mathematically, transformers eliminate that need, making available the trillions of images and petabytes of text data on the web and in corporate databases. In addition, the math that transformers use lends itself to parallel processing, so these models can run fast. Transformers now dominate popular performance leaderboards like SuperGLUE, a benchmark developed in 2019 for language-processing systems. How Transformers Pay Attention Like most neural networks, transformer models are basically large encoderdecoder blocks that process data. Small but strategic additions to these blocks (shown in the diagram below) make transformers uniquely powerful. Transformers use positional encoders to tag data elements coming in and out of the network. Attention units follow these tags, calculating a kind of algebraic map of how each element relates to the others. Attention queries are typically executed in parallel by calculating a matrix of equations in whats called multi-headed attention. With these tools, computers can see the same patterns humans see. Self-Attention Finds Meaning For example, in the sentence: She poured water from the pitcher to the cup until it was full. We know it refers to the cup, while in the sentence: She poured water from the pitcher to the cup until it was empty. We know it refers to the pitcher. Meaning is a result of relationships between things, and self-attention is a general way of learning relationships, said Ashish Vaswani, a former senior staff research scientist at Google Brain who led work on the seminal 2017 paper. Machine translation was a good vehicle to validate self-attention because you needed short- and long-distance relationships among words, said Vaswani. Now we see self-attention is a powerful, flexible tool for learning, he added. How Transformers Got Their Name Attention is so key to transformers the Google researchers almost used the term as the name for their 2017 model. Almost. Attention Net didnt sound very exciting, said Vaswani, who started working with neural nets in 2011. .Jakob Uszkoreit, a senior software engineer on the team, came up with the name Transformer. I argued we were transforming representations, but that was just playing semantics, Vaswani said. The Birth of Transformers In the paper for the 2017 NeurIPS conference, the Google team described their transformer and the accuracy records it set for machine translation. Thanks to a basket of techniques, they trained their model in just 3.5 days on eight NVIDIA GPUs, a small fraction of the time and cost of training prior models. They trained it on datasets with up to a billion pairs of words. It was an intense three-month sprint to the paper submission date, recalled Aidan Gomez, a Google intern in 2017 who contributed to the work. The night we were submitting, Ashish and I pulled an all-nighter at Google, he said. I caught a couple hours sleep in one of the small conference rooms, and I woke up just in time for the submission when someone coming in early to work opened the door and hit my head. It was a wakeup call in more ways than one. Ashish told me that night he was convinced this was going to be a huge deal, something game changing. I wasnt convinced, I thought it would be a modest gain on a benchmark, but it turned out he was very right, said Gomez, now CEO of startup Cohere thats providing a language processing service based on transformers. A Moment for Machine Learning Vaswani recalls the excitement of seeing the results surpass similar work published by a Facebook team using CNNs. I could see this would likely be an important moment in machine learning, he said. A year later, another Google team tried processing text sequences both forward and backward with a transformer. That helped capture more relationships among words, improving the models ability to understand the meaning of a sentence. Their Bidirectional Encoder Representations from Transformers (BERT) model set 11 new records and became part of the algorithm behind Google search. Within weeks, researchers around the world were adapting BERT for use cases across many languages and industries because text is one of the most common data types companies have, said Anders Arpteg, a 20-year veteran of machine learning research. Putting Transformers to Work Soon transformer models were being adapted for science and healthcare. DeepMind, in London, advanced the understanding of proteins, the building blocks of life, using a transformer called AlphaFold2, described in a recent Nature article. It processed amino acid chains like text strings to set a new watermark for describing how proteins fold, work that could speed drug discovery. AstraZeneca and NVIDIA developed MegaMolBART, a transformer tailored for drug discovery. Its a version of the pharmaceutical companys MolBART transformer, trained on a large, unlabeled database of chemical compounds using the NVIDIA Megatron framework for building large-scale transformer models. Reading Molecules, Medical Records Just as AI language models can learn the relationships between words in a sentence, our aim is that neural networks trained on molecular structure data will be able to learn the relationships between atoms in real-world molecules, said Ola Engkvist, head of molecular AI, discovery sciences and RD at AstraZeneca, when the work was announced last year. Separately, the University of Floridas academic health center collaborated with NVIDIA researchers to create GatorTron. The transformer model aims to extract insights from massive volumes of clinical data to accelerate medical research. Transformers Grow Up Along the way, researchers found larger transformers performed better. For example, researchers from the Rostlab at the Technical University of Munich, which helped pioneer work at the intersection of AI and biology, used natural-language processing to understand proteins. In 18 months, they graduated from using RNNs with 90 million parameters to transformer models with 567 million parameters. The OpenAI lab showed bigger is better with its Generative Pretrained Transformer (GPT). The latest version, GPT-3, has 175 billion parameters, up from 1.5 billion for GPT-2. With the extra heft, GPT-3 can respond to a users query even on tasks it was not specifically trained to handle. Its already being used by companies including Cisco, IBM and Salesforce. Tale of a Mega Transformer NVIDIA and Microsoft hit a high watermark in November, announcing the Megatron-Turing Natural Language Generation model (MT-NLG) with 530 billion parameters. It debuted along with a new framework, NVIDIA NeMo Megatron, that aims to let any business create its own billion- or trillion-parameter transformers to power custom chatbots, personal assistants and other AI applications that understand language. MT-NLG had its public debut as the brain for TJ, the Toy Jensen avatar that gave part of the keynote at NVIDIAs November 2021 GTC. When we saw TJ answer questions the power of our work demonstrated by our CEO that was exciting, said Mostofa Patwary, who led the NVIDIA team that trained the model. Creating such models is not for the faint of heart. MT-NLG was trained using hundreds of billions of data elements, a process that required thousands of GPUs running for weeks. Training large transformer models is expensive and time-consuming, so if youre not successful the first or second time, projects might be canceled, said Patwary. Trillion-Parameter Transformers Today, many AI engineers are working on trillion-parameter transformers and applications for them. Were constantly exploring how these big models can deliver better applications. We also investigate in what aspects they fail, so we can build even better and bigger ones, Patwary said. To provide the computing muscle those models need, our latest accelerator the NVIDIA H100 Tensor Core GPU packs a Transformer Engine and supports a new FP8 format. That speeds training while preserving accuracy. With those and other advances, transformer model training can be reduced from weeks to days said Huang at GTC. MoE Means More for Transformers Last year, Google researchers described the Switch Transformer, one of the first trillion-parameter models. It uses AI sparsity, a complex mixture-of experts (MoE) architecture and other advances to drive performance gains in language processing and up to 7x increases in pre-training speed. For its part, Microsoft Azure worked with NVIDIA to implement an MoE transformer for its Translator service. Tackling Transformers Challenges Now some researchers aim to develop simpler transformers with fewer parameters that deliver performance similar to the largest models. I see promise in retrieval-based models that Im super excited about because they could bend the curve, said Gomez, of Cohere, noting the Retro model from DeepMind as an example. Retrieval-based models learn by submitting queries to a database. Its cool because you can be choosy about what you put in that knowledge base, he said. The ultimate goal is to make these models learn like humans do from context in the real world with very little data, said Vaswani, now co-founder of a stealth AI startup. He imagines future models that do more computation upfront so they need less data and sport better ways users can give them feedback. Our goal is to build models that will help people in their everyday lives, he said of his new venture. Safe, Responsible Models Other researchers are studying ways to eliminate bias or toxicity if models amplify wrong or harmful language. For example, Stanford created the Center for Research on Foundation Models to explore these issues. These are important problems that need to be solved for safe deployment of models, said Shrimai Prabhumoye, a research scientist at NVIDIA whos among many across the industry working in the area. Today, most models look for certain words or phrases, but in real life these issues may come out subtly, so we have to consider the whole context, added Prabhumoye. Thats a primary concern for Cohere, too, said Gomez. No one is going to use these models if they hurt people, so its table stakes to make the safest and most responsible models. Beyond the Horizon Vaswani imagines a future where self-learning, attention-powered transformers approach the holy grail of AI. We have a chance of achieving some of the goals people talked about when they coined the term general artificial intelligence and I find that north star very inspiring, he said. We are in a time where simple methods like neural networks are giving us an explosion of new capabilities. Learn more about transformers on the NVIDIA Technical Blog.

==================================================

The transformer model is a type of neural network architecture that excels at processing sequential data, most prominently associated with large language models (LLMs). Transformer models have also achieved elite performance in other fields of artificial intelligence (AI), such as computer vision, speech recognition and time series forecasting. The transformer architecture was first described in the seminal 2017 paper Attention is All You Need by Vaswani and others, which is now considered a watershed moment in deep learning. Originally introduced as an evolution of the recurrent neural network (RNN)-based sequence-to-sequence models used for machine translation, transformer-based models have since attained cutting-edge advancements across nearly every machine learning (ML) discipline. Despite their versatility, transformer models are still most commonly discussed in the context of natural language processing (NLP) use cases, such as chatbots, text generation, summarization, question answering and sentiment analysis. The BERT (or Bidirectional Encoder Representations from Transformers) encoder-decoder model, introduced by Google in 2019, was a major landmark in the establishment of transformers and remains the basis of most modern word embedding applications, from modern vector databases to Google search. Autoregressive decoder-only LLMs, such as the GPT-3 (short for Generative Pre-trained Transformer) model that powered the launch of OpenAIs ChatGPT, catalyzed the modern era of generative AI (gen AI). The ability of transformer models to intricately discern how each part of a data sequence influences and correlates with the others also lends them many multimodal uses. For instance, vision transformers (ViTs) often exceed the performance of convolutional neural networks (CNNs) on image segmentation, object detection and related tasks. The transformer architecture also powers many diffusion models used for image generation, multimodal text-to-speech (TTS) and vision language models (VLMs). The central feature of transformer models is their self-attention mechanism, from which transformer models derive their impressive ability to detect the relationships (or dependencies) between each part of an input sequence. Unlike the RNN and CNN architectures that preceded it, the transformer architecture uses only attention layers and standard feedforward layers. The benefits of self-attention, and specifically the multi-head attention technique that transformer models employ to compute it, are what enable transformers to exceed the performance of the RNNs and CNNs that had previously been state-of-the-art. Before the introduction of transformer models, most NLP tasks relied on recurrent neural networks (RNNs). The way RNNs process sequential data is inherently serialized: they ingest the elements of an input sequence one at a time and in a specific order. This hinders the ability of RNNs to capture long-range dependencies, meaning RNNs can only process short text sequences effectively. This deficiency was somewhat addressed by the introduction of long short term memory networks (LSTMs), but remains a fundamental shortcoming of RNNs. Attention mechanisms, conversely, can examine an entire sequence simultaneously and make decisions about how and when to focus on specific time steps of that sequence. In addition to significantly improving the ability to understand long-range dependencies, this quality of transformers also allows for parallelization: the ability to perform many computational steps at once, rather than in a serialized manner. Being well-suited to parallelism enables transformer models to take full advantage of the power and speed offered by GPUs during both training and inference. This possibility, in turn, unlocked the opportunity to train transformer models on unprecedentedly massive datasets through self-supervised learning. Especially for visual data, transformers also offer some advantages over convolutional neural networks. CNNs are inherently local, using convolutions to process smaller subsets of input data one piece at a time. Therefore, CNNs also struggle to discern long-range dependencies, such as correlations between words (in text) or pixels (in images) that arent neighboring one another. Attention mechanisms dont have this limitation. Understanding the mathematical concept of attention, and more specifically self-attention, is essential to understanding the success of transformer models in so many fields. Attention mechanisms are, in essence, algorithms designed to determine which parts of a data sequence an AI model should pay attention to at any particular moment. Consider a language model interpreting the English text Broadly speaking, a transformer models attention layers assess and use the specific context of each part of a data sequence in 4 steps: Before training, a transformer model doesnt yet know how to generate optimal vector embeddings and alignment scores. During training, the model makes predictions across millions of examples drawn from its training data, and a loss function quantifies the error of each prediction. Through an iterative cycle of making predictions and then updating model weights through backpropagation and gradient descent, the model learns to generate vector embeddings, alignment scores and attention weights that lead to accurate outputs. Transformer models such as relational databases generate query, key and value vectors for each part of a data sequence, and use them to compute attention weights through a series of matrix multiplications. Relational databases are designed to simplify the storage and retrieval of relevant data: they assign a unique identifier (key) to each piece of data, and each key is associated with a corresponding value. The Attention is All You Need paper applied that conceptual framework to processing the relationships between each token in a sequence of text. For an LLM, the models database is the vocabulary of tokens it has learned from the text samples in its training data. Its attention mechanism uses information from this database to understand the context of language. Whereas charactersletters, numbers or punctuation marksare the base unit we humans use to represent language, the smallest unit of language that AI models use is a token. Each token is assigned an ID number, and these ID numbers (rather than the words or even the tokens themselves) are the way LLMs navigate their vocabulary database. This tokenization of language significantly reduces the computational power needed to process text. To generate query and key vectors to feed into the transformers attention layers, the model needs an initial, contextless vector embedding for each token. These initial token embeddings can be either learned during training or taken from a pretrained word embedding model. The order and position of words can significantly impact their semantic meanings. Whereas the serialized nature of RNNs inherently preserves information about the position of each token, transformer models must explicitly add positional information for the attention mechanism to consider. With positional encoding, the model adds a vector of values to each tokens embedding, derived from its relative position, before the input enters the attention mechanism. The nearer the 2 tokens are, the more similar their positional vectors will be and therefore, the more their alignment score will increase from adding positional information. The model thereby learns to pay greater attention to nearby tokens. When positional information has been added, each updated token embedding is used to generate three new vectors. These query, key and value vectors are generated by passing the original token embeddings through each of three parallel feedforward neural network layers that precede the first attention layer. Each parallel subset of that linear layer has a unique matrix of weights, learned through self-supervised pretraining on a massive dataset of text. The transformers attention mechanisms primary function is to assign accurate attention weights to the pairings of each tokens query vector with the key vectors of all the other tokens in the sequence. When achieved, you can think of each token as now having a corresponding vector of attention weights, in which each element of that vector represents the extent to which some other token should influence it. In essence, s vector embedding has been updated to better reflect the context provided by the other tokens in the sequence. To capture the many multifaceted ways tokens might relate to one another, transformer models implement multi-head attention across multiple attention blocks. Before being fed into the first feedforward layer, each original input token embedding is split into h evenly sized subsets. Each piece of the embedding is fed into one of h parallel matrices of Q, K and V weights, each of which are called a query head, key head or value head. The vectors output by each of these parallel triplets of query, key and value heads are then fed into a corresponding subset of the next attention layer, called an attention head. In the final layers of each attention block, the outputs of these h parallel circuits are eventually concatenated back together before being sent to the next feedforward layer. In practice, model training results in each circuit learning different weights that capture a separate aspect of semantic meanings. In some situations, passing along the contextually-updated embedding output by the attention block might result in an unacceptable loss of information from the original sequence. To address this, transformer models often balance the contextual information provided by the attention mechanism with the original semantic meaning of each token. After the attention-updated subsets of the token embedding have all been concatenated back together, the updated vector is then added to the tokens original (position-encoded) vector embedding. The original token embedding is supplied by a residual connection between that layer and an earlier layer of the network. The resulting vector is fed into another linear feedforward layer, where its normalized back to a constant size before being passed along to the next attention block. Together, these measures help preserve stability in training and help ensure that the texts original meaning is not lost as the data moves deeper into the neural network. Eventually, the model has enough contextual information to inform its final outputs. The nature and function of the output layer will depend on the specific task the transformer model has been designed for. In autoregressive LLMs, the final layer uses a softmax function to determine the probability that the next word will match each token in its vocabulary database. Depending on the specific sampling hyperparameters, the model uses those probabilities to determine the next token of the output sequence. Transformer models are most commonly associated with NLP, having originally been developed for machine translation use cases. Most notably, the transformer architecture gave rise to the large language models (LLMs) that catalyzed the advent of generative AI. Most of the LLMs that the public is most familiar with, from closed source models such as OpenAIs GPT series and Anthropics Claude models to open source models including Meta Llama or IBM Granite, are autoregressive decoder-only LLMs. Autoregressive LLMs are designed for text generation, which also extends naturally to adjacent tasks such as summarization and question answering. Theyre trained through self-supervised learning, in which the model is provided the first word of a text passage and tasked with iteratively predicting the next word until the end of the sequence. Information provided by the self-attention mechanism enables the model to extract context from the input sequence and maintain the coherence and continuity of its output. Encoder-decoder masked language models (MLMs), such as BERT and its many derivatives, represent the other main evolutionary branch of transformer-based LLMs. In training, an MLM is provided a text sample with some tokens maskedhiddenand tasked with completing the missing information. While this training methodology is less effective for text generation, it helps MLMs excel at tasks requiring robust contextual information, such as translation, text classification and learning embeddings. Though transformer models were originally designed for, and continue to be most prominently associated with natural language use cases, they can be used in nearly any situation involving sequential data. This has led to the development of transformer-based models in other fields, from fine-tuning LLMs into multimodal systems to dedicated time series forecasting models and ViTs for computer vision. Some data modalities are more naturally suited to transformer-friendly sequential representation than others. Time series, audio and video data are inherently sequential, whereas image data is not. Despite this, ViTs and other attention-based models have achieved state-of-the-art results for many computer vision tasks, including image captioning, object detection, image segmentation and visual question answering. To use transformer models for data not conventionally thought of as sequential requires a conceptual workaround to represent that data as a sequence. For instance, to use attention mechanisms to understand visual data, ViTs use patch embeddings to make image data interpretable as sequences. Train, validate, tune and deploy generative AI, foundation models and machine learning capabilities with IBM watsonx.ai, a next-generation enterprise studio for AI builders. Build AI applications in a fraction of the time with a fraction of the data. Put AI to work in your business with IBMs industry-leading AI expertise and portfolio of solutions at your side. Reinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value. 1 Googles BERT Rolls Out Worldwide (link resides outside ibm.com), Search Engine Journal, Dec 9, 2019

==================================================

First-Order Logic in Artificial Intelligence First-order logic (FOL) is also known as predicate logic. It is a foundational framework used in mathematics, philosophy, linguistics, and computer science. In artificial intelligence (AI), FOL is important for knowledge representation, automated reasoning, and NLP. FOL extends propositional logic b

==================================================

Explain Like Im Five is the best forum and archive on the internet for layperson-friendly explanations. Dont Panic! ELI5 What are transformers in ML? Engineering Archived post. New comments cannot be posted and votes cannot be cast.

==================================================

The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper "Attention Is All You Need". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.
Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.

Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).

