{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(palette='summer')\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_jsonl(input_path, output_path):\n",
    "\n",
    "    \"\"\"Конвертирует файл в JSONL формат с обработкой ошибок\"\"\"\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                json.dump(data, f_out, ensure_ascii=False)\n",
    "                f_out.write('\\n')\n",
    "            except json.JSONDecodeError:\n",
    "                if line.startswith('['):\n",
    "                    try:\n",
    "                        for item in json.loads(line):\n",
    "                            json.dump(item, f_out, ensure_ascii=False)\n",
    "                            f_out.write('\\n')\n",
    "                    except:\n",
    "                        print(f\"Failed to parse array in: {input_path}\")\n",
    "                else:\n",
    "                    print(f\"Invalid JSON line skipped in: {input_path}\")\n",
    "\n",
    "def process_gz_files(source_root=\".\", target_root=\"converted_data\"):\n",
    "    source_path = Path(source_root)\n",
    "    target_path = Path(target_root)\n",
    "\n",
    "    for gz_file in source_path.rglob(\"*.gz\"):\n",
    "        try:\n",
    "            relative_path = gz_file.relative_to(source_path)\n",
    "            output_dir = target_path / relative_path.parent\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            temp_file = output_dir / gz_file.name\n",
    "            final_file = output_dir / gz_file.name.replace(\".gz\", \".json\")\n",
    "\n",
    "            with gzip.open(gz_file, 'rb') as f_in:\n",
    "                with open(temp_file, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            convert_to_jsonl(temp_file, final_file)\n",
    "            temp_file.unlink()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gz_file}: {str(e)}\")\n",
    "\n",
    "process_gz_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_save_records(source_root=\"converted_data\", target_root=\"filtered_data\"):\n",
    "\n",
    "    '''Getting filtered data'''\n",
    "    \n",
    "    source_path = Path(source_root)\n",
    "    target_path = Path(target_root)\n",
    "    \n",
    "    processed_files = set(target_path.rglob(\"*.json\"))\n",
    "    \n",
    "    for src_file in source_path.rglob(\"*.json\"):\n",
    "        relative_path = src_file.relative_to(source_path)\n",
    "        dst_file = target_path / relative_path\n",
    "        \n",
    "        if dst_file.exists():\n",
    "            continue\n",
    "            \n",
    "        dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            with open(src_file, 'r', encoding='utf-8') as f_in, \\\n",
    "                 open(dst_file, 'w', encoding='utf-8') as f_out:\n",
    "\n",
    "                filtered_count = 0\n",
    "                total_count = 0\n",
    "                \n",
    "                for line in f_in:\n",
    "                    line = line.strip()\n",
    "                    total_count += 1\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "                        abstract = record.get('abstract', '')\n",
    "                        \n",
    "                        if len(abstract.split()) >= 200:\n",
    "                            json.dump(record, f_out, ensure_ascii=False)\n",
    "                            f_out.write('\\n')\n",
    "                            filtered_count += 1\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record: {e}\")\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {src_file}: {e}\")\n",
    "            if dst_file.exists():\n",
    "                dst_file.unlink()\n",
    "\n",
    "filter_and_save_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры финальных датасетов:\n",
      "Train: 204180 samples\n",
      "Val: 25521 samples\n",
      "Test: 25525 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "arxiv = load_dataset(\"scientific_papers\", \"arxiv\", \n",
    "                     split=\"train\", \n",
    "                     trust_remote_code=True, \n",
    "                     storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}},\n",
    "                     cache_dir=r\"C:\\Users\\denis\\.cache\\huggingface\\datasets\")\n",
    "\n",
    "\n",
    "arxiv = arxiv.remove_columns(['section_names'])\n",
    "arxiv = arxiv.rename_column('abstract', 'summary')\n",
    "\n",
    "def load_filtered_dataset(data_root=\"filtered_data\"):\n",
    "    data_path = Path(data_root)\n",
    "    return Dataset.from_json([\n",
    "        str(p) for p in data_path.rglob(\"*.json\")\n",
    "    ])\n",
    "\n",
    "patent_dataset = load_filtered_dataset()\n",
    "\n",
    "\n",
    "patent_dataset = patent_dataset.remove_columns(['publication_number', 'application_number'])\n",
    "patent_dataset = patent_dataset.rename_column('abstract', 'summary')\n",
    "patent_dataset = patent_dataset.rename_column('description', 'article')\n",
    "\n",
    "\n",
    "\n",
    "def split_and_combine_datasets(arxiv_ds: Dataset, \n",
    "                              patent_ds: Dataset, \n",
    "                              seed: int = 42,\n",
    "                              train_ratio: float = 0.8,\n",
    "                              val_ratio: float = 0.1) -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Разделяет каждый датасет на train/val/test и объединяет соответствующие части\n",
    "    \n",
    "    Параметры:\n",
    "    arxiv_ds: Датасет arXiv\n",
    "    patent_ds: Датасент патентов\n",
    "    seed: Сид для воспроизводимости\n",
    "    train_ratio: Доля тренировочных данных (0.0-1.0)\n",
    "    val_ratio: Доля валидационных данных (0.0-1.0)\n",
    "    \n",
    "    Возвращает:\n",
    "    (train, val, test) - объединенные датасеты\n",
    "    \"\"\"\n",
    "    \n",
    "    assert np.isclose(train_ratio + val_ratio + (1 - train_ratio - val_ratio), 1.0), \"Пропорции должны суммироваться к 1\"\n",
    "    \n",
    "    def split_single(ds: Dataset) -> tuple[Dataset, Dataset, Dataset]:\n",
    "        train_test = ds.train_test_split(\n",
    "            test_size=1-train_ratio, \n",
    "            seed=seed,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_test = train_test['test'].train_test_split(\n",
    "            test_size=val_ratio/(val_ratio + (1 - train_ratio - val_ratio)), \n",
    "            seed=seed,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return train_test['train'], val_test['train'], val_test['test']\n",
    "    \n",
    "    arxiv_train, arxiv_val, arxiv_test = split_single(arxiv_ds)\n",
    "    patent_train, patent_val, patent_test = split_single(patent_ds)\n",
    "    \n",
    "    combined_train = concatenate_datasets([arxiv_train, patent_train])\n",
    "    combined_val = concatenate_datasets([arxiv_val, patent_val])\n",
    "    combined_test = concatenate_datasets([arxiv_test, patent_test])\n",
    "    \n",
    "    return combined_train, combined_val, combined_test\n",
    "\n",
    "train_ds, val_ds, test_ds = split_and_combine_datasets(\n",
    "    arxiv_ds=arxiv,\n",
    "    patent_ds=patent_dataset,\n",
    "    seed=42,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Размеры финальных датасетов:\")\n",
    "print(f\"Train: {len(train_ds)} samples\")\n",
    "print(f\"Val: {len(val_ds)} samples\")\n",
    "print(f\"Test: {len(test_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'summary'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "def create_test_dataset(text_file: str) -> Dataset:\n",
    "    # Прочитать содержимое файла\n",
    "    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Создать структуру для датасета\n",
    "    data = {\n",
    "        \"article\": [content],  # Весь текст в одной строке\n",
    "        \"summary\": [\"\"]        # Пустые строки для заполнения\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "test_dataset = create_test_dataset(r\"D:\\ethd\\ml\\Neuro-research\\example.txt\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import gc\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "\n",
    "def tokenize_in_chunks(dataset: Dataset, chunk_size=1000, save_dir=\"processed\"):\n",
    "    # Создаем директорию для сохранения\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    num_chunks = total_samples // chunk_size + 1\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Выбираем чанк данных\n",
    "        chunk = dataset.select(range(\n",
    "            i * chunk_size,\n",
    "            min((i + 1) * chunk_size, total_samples)\n",
    "        ))\n",
    "        \n",
    "        # Токенизация\n",
    "        tokenized_chunk = chunk.map(\n",
    "            lambda examples: tokenizer(\n",
    "                examples[\"article\"],\n",
    "                text_target=examples[\"summary\"],\n",
    "                max_length=1024,\n",
    "                truncation=True,\n",
    "                padding=False\n",
    "            ),\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            remove_columns=[\"article\", \"summary\"],\n",
    "            load_from_cache_file=False\n",
    "        )\n",
    "        \n",
    "        # Сохранение чанка\n",
    "        tokenized_chunk.save_to_disk(\n",
    "            os.path.join(save_dir, f\"chunk_{i}\"),\n",
    "            max_shard_size=\"100MB\"\n",
    "        )\n",
    "        \n",
    "        # Очистка памяти\n",
    "        del chunk\n",
    "        del tokenized_chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Processed chunk {i+1}/{num_chunks}\")\n",
    "\n",
    "# Использование\n",
    "tokenize_in_chunks(patent_dataset, chunk_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "class ChunkedDataset:\n",
    "    def __init__(self, chunk_dir, shuffle=True):\n",
    "        self.chunk_files = sorted([\n",
    "            os.path.join(chunk_dir, f) \n",
    "            for f in os.listdir(chunk_dir) \n",
    "            if f.startswith(\"chunk\")\n",
    "        ])\n",
    "        self.shuffle = shuffle\n",
    "        self.current_chunk = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.chunk_files)\n",
    "            \n",
    "        for chunk_file in self.chunk_files:\n",
    "            # Загрузка чанка по требованию\n",
    "            self.current_chunk = load_from_disk(chunk_file)\n",
    "            yield from self.current_chunk\n",
    "            \n",
    "    def get_dataloader(self, batch_size=8, collate_fn=None):\n",
    "        return DataLoader(\n",
    "            self,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "# Использование\n",
    "chunk_dir = r\"processed\"\n",
    "dataset = ChunkedDataset(chunk_dir)\n",
    "dataloader = dataset.get_dataloader(\n",
    "    batch_size=2,\n",
    "    collate_fn=DataCollatorForSeq2Seq(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\denis\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/3bac65d18c99463302d12ca75c2220ea714f9c81ce235f205fa818efe71df6ea?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1745425783&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTQyNTc4M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8zYmFjNjVkMThjOTk0NjMzMDJkMTJjYTc1YzIyMjBlYTcxNGY5YzgxY2UyMzVmMjA1ZmE4MThlZmU3MWRmNmVhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=Sdlz-jfGMGdSmjIi6B9MDw7ewCeGD2965OxOqQ1x2HxYJNPpEHfY9rg2-t%7EhDTk%7EMu0W4pqPLNZeaEH94%7ER0U7H7FFU1NW1-rbRjJA29fr0kyo5Y-kbUGls3MlVvlQIwcDFhGVDGtd1c5UTbL7vAIUaqie6pfsRom6Nh7%7EFM1UW0bv2-GoU5Dqfrb9x7%7EiGZkTQgh9MbgkS5YdA4y8eGdyU08SJYQ1FZDWTutwWaf-91rZCgfJYNyhn7WIevVscCtf-RWbGJTKRf45rHRObkd40MlCQ8KcfjyMCWqd7MfsygtNg62tjVNc0jg3SpZMaBjgVIkj3hq33XHyAOs%7EMp-Q__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/1e46814333b97dfa0f866f58fd15cd7b48ffbe7fd4c1a929caa5f95c7b2fa592?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1745425893&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTQyNTg5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8xZTQ2ODE0MzMzYjk3ZGZhMGY4NjZmNThmZDE1Y2Q3YjQ4ZmZiZTdmZDRjMWE5MjljYWE1Zjk1YzdiMmZhNTkyP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=IPIijnXglcdAgsyvqfnUvpQyBPUJdWCkiiZxI1lBKLILvIXCCYjC8zBRGuUG6ooubMs3gr%7EjeOOz2D7zL52S2qxWaFDi79M75W1IJKb6v2D2CgxylUdYhXkx0GDOI2yBk9NJ6qKFAhSPR%7EIMYGzxtJkq7ga9bHVc3bTWgTbvvEA51uh7g-YDFiubg12nK7WrVzv-6VoeR3HcGy4h74hToLwiL82Qx4x6Ibg1dikHukzmiaJreH5g7UiL0Rfw21GVX-tkw9lmE8ULW%7E-8o173A1cFaYlw2fckewvMhOhWLNSAJoR3tKK6va1T1h%7E5Hjmyi8TDLqAPlyHvyLPcxVULeg__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)\n",
      "Trying to resume download...\n",
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 27 chunks from processed\\train\n",
      "Loading 27 chunks from processed\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denis\\AppData\\Local\\Temp\\ipykernel_16232\\1946071220.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Error while downloading from https://cdn-lfs.hf.co/sshleifer/distilbart-cnn-12-6/1e46814333b97dfa0f866f58fd15cd7b48ffbe7fd4c1a929caa5f95c7b2fa592?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1745425893&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTQyNTg5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9zc2hsZWlmZXIvZGlzdGlsYmFydC1jbm4tMTItNi8xZTQ2ODE0MzMzYjk3ZGZhMGY4NjZmNThmZDE1Y2Q3YjQ4ZmZiZTdmZDRjMWE5MjljYWE1Zjk1YzdiMmZhNTkyP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=IPIijnXglcdAgsyvqfnUvpQyBPUJdWCkiiZxI1lBKLILvIXCCYjC8zBRGuUG6ooubMs3gr%7EjeOOz2D7zL52S2qxWaFDi79M75W1IJKb6v2D2CgxylUdYhXkx0GDOI2yBk9NJ6qKFAhSPR%7EIMYGzxtJkq7ga9bHVc3bTWgTbvvEA51uh7g-YDFiubg12nK7WrVzv-6VoeR3HcGy4h74hToLwiL82Qx4x6Ibg1dikHukzmiaJreH5g7UiL0Rfw21GVX-tkw9lmE8ULW%7E-8o173A1cFaYlw2fckewvMhOhWLNSAJoR3tKK6va1T1h%7E5Hjmyi8TDLqAPlyHvyLPcxVULeg__&Key-Pair-Id=K3RPWS32NSSJCE: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2580)\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='9786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/9786 00:36 < 24:44:21, 0.11 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted. Saving final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "import os\n",
    "import torch\n",
    "\n",
    "MODEL_CHOICES = {\n",
    "    \"tiny\": \"sshleifer/distilbart-cnn-12-6\",\n",
    "    \"base\": \"facebook/bart-base\",\n",
    "    \"distilled\": \"sshleifer/distilbart-cnn-12-6\",\n",
    "    \"custom\": \"patrickvonplaten/bart-tiny-random\"\n",
    "}\n",
    "\n",
    "# Пример использования\n",
    "MODEL_NAME = MODEL_CHOICES[\"tiny\"]\n",
    "\n",
    "CHUNKS_DIR = \"processed\"\n",
    "OUTPUT_DIR = \"bart-finetuned\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "SEED = 42\n",
    "\n",
    "# 1. Инициализация модели с оптимизациями памяти\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,        # Полуточность\n",
    "    low_cpu_mem_usage=True,\n",
    "    gradient_checkpointing=True       # Экономит до 60% памяти\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_fast=True,                   # Используем быстрый токенизатор\n",
    "    model_max_length=MAX_INPUT_LENGTH\n",
    ")\n",
    "\n",
    "def load_chunks_optimized(chunk_dir):\n",
    "    chunk_dirs = [\n",
    "        os.path.join(chunk_dir, d)\n",
    "        for d in sorted(os.listdir(chunk_dir))\n",
    "        if d.startswith(\"chunk\") and os.path.isdir(os.path.join(chunk_dir, d))\n",
    "    ]\n",
    "    \n",
    "    chunk_files = []\n",
    "    for d in chunk_dirs:\n",
    "        filename = \"data-00000-of-00001.arrow\"\n",
    "        file_path = os.path.join(d, filename)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            chunk_files.append(file_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {file_path} not found in directory {d}\")\n",
    "    \n",
    "    if not chunk_files:\n",
    "        raise ValueError(f\"No valid chunk files found in {chunk_dir}\")\n",
    "    \n",
    "    print(f\"Loading {len(chunk_files)} chunks from {chunk_dir}\")\n",
    "    return concatenate_datasets([\n",
    "        Dataset.from_file(f) for f in chunk_files\n",
    "    ])\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "    labels = tokenizer(examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# train_dataset = load_chunks_optimized(os.path.join(CHUNKS_DIR, 'train'))\n",
    "# train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=8)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    pad_to_multiple_of=8,            # Улучшает производительность на Tensor Cores\n",
    "    padding='longest',\n",
    "    max_length=MAX_INPUT_LENGTH\n",
    ")\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*4,  # Увеличил для валидации\n",
    "    gradient_accumulation_steps=4,    # Эмулирует batch_size=32\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    bf16=True,                        # Аппаратное ускорение\n",
    "    seed=SEED,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=8,         # Используем больше ядер CPU\n",
    "    dataloader_pin_memory=True,       # Ускоряет передачу данных в GPU\n",
    "    dataloader_prefetch_factor=2,     # Предзагрузка данных\n",
    "    remove_unused_columns=True,       # Удаляем неиспользуемые столбцы\n",
    "    optim=\"adamw_bnb_8bit\",           # 8-битный оптимизатор\n",
    "    report_to=\"none\"                  # Отключаем логирование для ускорения\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=load_chunks_optimized(os.path.join(CHUNKS_DIR, 'train')),\n",
    "    eval_dataset=load_chunks_optimized(os.path.join(CHUNKS_DIR, 'train')),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted. Saving final model...\")\n",
    "\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "trainer.model.save_pretrained(OUTPUT_DIR, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.5.1+cu121\n",
      "Transformers: 4.50.3\n",
      "Accelerate: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers, accelerate\n",
    "print(f\"PyTorch: {torch.__version__}\")        # Должно быть 2.3.0+\n",
    "print(f\"Transformers: {transformers.__version__}\")  # 4.41.0+\n",
    "print(f\"Accelerate: {accelerate.__version__}\")      # 0.29.3+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Должно быть True\n",
    "print(torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
