{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(palette='summer')\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "def convert_to_jsonl(input_path, output_path):\n",
    "\n",
    "    \"\"\"Конвертирует файл в JSONL формат с обработкой ошибок\"\"\"\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f_in, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                json.dump(data, f_out, ensure_ascii=False)\n",
    "                f_out.write('\\n')\n",
    "            except json.JSONDecodeError:\n",
    "                if line.startswith('['):\n",
    "                    try:\n",
    "                        for item in json.loads(line):\n",
    "                            json.dump(item, f_out, ensure_ascii=False)\n",
    "                            f_out.write('\\n')\n",
    "                    except:\n",
    "                        print(f\"Failed to parse array in: {input_path}\")\n",
    "                else:\n",
    "                    print(f\"Invalid JSON line skipped in: {input_path}\")\n",
    "\n",
    "def process_gz_files(source_root=\".\", target_root=\"converted_data\"):\n",
    "    source_path = Path(source_root)\n",
    "    target_path = Path(target_root)\n",
    "\n",
    "    for gz_file in source_path.rglob(\"*.gz\"):\n",
    "        try:\n",
    "            relative_path = gz_file.relative_to(source_path)\n",
    "            output_dir = target_path / relative_path.parent\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            temp_file = output_dir / gz_file.name\n",
    "            final_file = output_dir / gz_file.name.replace(\".gz\", \".json\")\n",
    "\n",
    "            with gzip.open(gz_file, 'rb') as f_in:\n",
    "                with open(temp_file, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "            convert_to_jsonl(temp_file, final_file)\n",
    "            temp_file.unlink()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gz_file}: {str(e)}\")\n",
    "\n",
    "# process_gz_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering complete!\n"
     ]
    }
   ],
   "source": [
    "def filter_and_save_records(source_root=\"converted_data\", target_root=\"filtered_data\"):\n",
    "\n",
    "    '''Getting filtered data'''\n",
    "    \n",
    "    source_path = Path(source_root)\n",
    "    target_path = Path(target_root)\n",
    "    \n",
    "    processed_files = set(target_path.rglob(\"*.json\"))\n",
    "    \n",
    "    for src_file in source_path.rglob(\"*.json\"):\n",
    "        relative_path = src_file.relative_to(source_path)\n",
    "        dst_file = target_path / relative_path\n",
    "        \n",
    "        if dst_file.exists():\n",
    "            continue\n",
    "            \n",
    "        dst_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            with open(src_file, 'r', encoding='utf-8') as f_in, \\\n",
    "                 open(dst_file, 'w', encoding='utf-8') as f_out:\n",
    "\n",
    "                filtered_count = 0\n",
    "                total_count = 0\n",
    "                \n",
    "                for line in f_in:\n",
    "                    line = line.strip()\n",
    "                    total_count += 1\n",
    "                    if not line:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        record = json.loads(line)\n",
    "                        abstract = record.get('abstract', '')\n",
    "                        \n",
    "                        if len(abstract.split()) >= 200:\n",
    "                            json.dump(record, f_out, ensure_ascii=False)\n",
    "                            f_out.write('\\n')\n",
    "                            filtered_count += 1\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing record: {e}\")\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {src_file}: {e}\")\n",
    "            if dst_file.exists():\n",
    "                dst_file.unlink()\n",
    "\n",
    "# filter_and_save_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.62G/3.62G [07:54<00:00, 7.64MB/s] \n",
      "Downloading data: 100%|██████████| 880M/880M [01:50<00:00, 7.99MB/s] \n",
      "Generating train split: 100%|██████████| 203037/203037 [01:48<00:00, 1874.25 examples/s]\n",
      "Generating validation split: 100%|██████████| 6436/6436 [00:04<00:00, 1488.30 examples/s]\n",
      "Generating test split: 100%|██████████| 6440/6440 [00:03<00:00, 1644.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "arxiv = load_dataset(\"scientific_papers\", \"arxiv\", \n",
    "                     split=\"train\", \n",
    "                     trust_remote_code=True, \n",
    "                     storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}},\n",
    "                     cache_dir=r\"C:\\Users\\denis\\.cache\\huggingface\")\n",
    "\n",
    "\n",
    "arxiv.remove_columns(['publication_number', 'application_number', 'section_names'])\n",
    "\n",
    "def load_filtered_dataset(data_root=\"filtered_data\"):\n",
    "    data_path = Path(data_root)\n",
    "    return Dataset.from_json([\n",
    "        str(p) for p in data_path.rglob(\"*.json\")\n",
    "    ])\n",
    "\n",
    "patent_dataset = load_filtered_dataset()\n",
    "\n",
    "\n",
    "patent_dataset.remove_columns(['publication_number', 'application_number'])\n",
    "patent_dataset = patent_dataset.rename_column('abstract', 'summary')\n",
    "patent_dataset = patent_dataset.rename_column('description', 'article')\n",
    "\n",
    "combined_dataset = concatenate_datasets([arxiv, patent_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'abstract', 'section_names', 'publication_number', 'summary', 'application_number'],\n",
       "    num_rows: 255226\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ethd\\conda\\envs\\neuro-research-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\denis\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [re.sub(r'<[^>]+>|http\\S+', '', text) for text in examples[\"article\"]]\n",
    "    targets = [re.sub(r'[\\U00010000-\\U0010ffff]', '', text) for text in examples[\"summary\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = preprocess_function(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,  # Максимальная длина суммы\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    # model=model,\n",
    "    args=training_args,\n",
    "    # train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
