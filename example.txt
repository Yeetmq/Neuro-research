RayZer: A Self-supervised Large View Synthesis Model

Hanwen Jiang1 Hao Tan2 Peng Wang2 Haian Jin3 Yue Zhao1 Sai Bi2
Kai Zhang2 Fujun Luan2 Kalyan Sunkavalli2 Qixing Huang1 Georgios Pavlakos1
1The University of Texas at Austin 2Adobe Research 3Cornell University

5
2
0
2

y
a
M
1

]

V
C
.
s
c
[

1
v
2
0
7
0
0
.
5
0
5
2
:
v
i
X
r
a

Figure 1. We propose RayZer, a self-supervised multi-view 3D Vision model trained on unlabeled data without any annotations, e.g.,
camera pose labels. At inference, RayZer supports feed-forward novel view synthesis from unposed & uncalibrated images. RayZer
achieves novel view synthesis performance comparable to that of supervised ‚Äúoracle‚Äù methods (GS-LRM and LVSM), which require camera
labels in both training and inference, and even outperforms them when they rely on (potentially noisy) COLMAP camera annotations. We
show two examples on the right, where COLMAP camera annotations lead to consistent failures of GS-LRM and LVSM during inference.

Abstract

We present RayZer, a self-supervised multi-view 3D Vision
model trained without any 3D supervision, i.e., camera poses
and scene geometry, while exhibiting emerging 3D aware-
ness. Concretely, RayZer takes unposed and uncalibrated
images as input, recovers camera parameters, reconstructs
a scene representation, and synthesizes novel views. During
training, RayZer relies solely on its self-predicted camera
poses to render target views, eliminating the need for any
ground-truth camera annotations and allowing RayZer to be
trained with 2D image supervision. The emerging 3D aware-
ness of RayZer is attributed to two key factors. First, we de-
sign a self-supervised framework, which achieves 3D-aware
auto-encoding of input images by disentangling camera and
scene representations. Second, we design a transformer-
based model in which the only 3D prior is the ray struc-
ture, connecting camera, pixel, and scene simultaneously.
RayZer demonstrates comparable or even superior novel
view synthesis performance than ‚Äúoracle‚Äù methods that rely
on pose annotations in both training and testing. Project:
https://hwjiang1510.github.io/RayZer/

1. Introduction

Self-supervised learning has driven the rise of foundation
models, enabling training on vast amounts of unlabeled data

and fueled by the scaling law [38]. This paradigm has proven
highly effective for LLMs [60], VLMs [2], and visual gener-
ation [56]. In contrast, 3D Vision models still rely heavily on
ground-truth 3D geometry and camera pose labels [28, 77],
which are usually estimated from time-consuming optimiza-
tion methods, e.g., COLMAP [65], and are not always per-
fect. This reliance limits both learning scalability and effec-
tiveness. To break free from this constraint, we move beyond
the supervised paradigm and ask: how far can we push a 3D
Vision model without any 3D supervision?

In this paper, we present RayZer, a large multi-view 3D
model trained with self-supervision and exhibiting emerg-
ing 3D awareness. The input of RayZer is unposed and
uncalibrated multi-view images, sampled from continuous
video frames or unordered multi-view captures. RayZer
first recovers the camera parameters, then reconstructs the
scene representation, and finally renders novel views. The
key insight of our self-supervised training is to use the cam-
era poses predicted by RayZer itself to render views that
provide photometric supervision, rather than following the
standard protocol of using ground truth poses for render-
ing [29, 74, 92]. Thus, RayZer can be trained with zero 3D
supervision, i.e., no 3D geometry or camera pose supervi-
sion. During inference, RayZer predicts camera and scene
representations in a feed-forward manner, without requiring
per-scene optimization. We show inference results in Fig. 1.

1

DL3DVVideo FramesUnordered Image SetsUnlabeled DataRendered Novel ViewsDL3DVNVS PSNRLVSMRayZerLVSMLVSMGTRayZerGS-LRM23.6924.3627.00DL3DVRealEstateGS-LRM23.49RayZer27.48GS-LRM24.25 
 
 
 
 
 
 
As RayZer uses the camera poses predicted by itself for
training, this self-supervised task can be interpreted as 3D-
aware image auto-encoding [41, 61, 95]. Initially, RayZer
disentangles input images into camera parameters and scene
representations (reconstruction). It then re-entangles these
predicted representations back into images (rendering). To
facilitate this disentanglement, we control the information
flow. As shown in Fig. 2, we divide all images into two parts:
one set predicts the scene representation (input views), while
the other offers photometric self-supervision (target views).
This is achieved by using estimated poses of the second set
to render the scene representation predicted from the first set,
thereby preventing trivial solutions that are not 3D-aware.

To facilitate self-supervised learning, RayZer is built only
with transformers ‚Äì no 3D representation, hand-crafted ren-
dering equation, or 3D-informed architectures. This design
is motivated by self-supervised large models in other modal-
ities [2, 6, 56], enabling RayZer to flexibly and effectively
learn domain-specific knowledge. The only 3D prior incorpo-
rated in RayZer is the ray structure, which simultaneously
models the relationship between camera, pixels (image), and
scene. Concretely, RayZer first predicts camera poses, which
are then converted into pixel-aligned Pl√ºcker ray maps [57]
to guide the scene reconstruction that follows. This ray-
based representation serves as a strong prior for addressing
the chicken-and-egg problem of structure and motion [68],
effectively allowing the camera and scene representations to
regularize each other during training.

We evaluate RayZer on three datasets, including both
scene-level and object-level data with different camera con-
figurations. We observe that RayZer demonstrates compara-
ble or even better novel view synthesis performance than
‚Äúoracle‚Äù methods [33, 91] that use pose labels in both training
and testing. Interestingly, we identify that potentially noisy
pose annotations from COLMAP can limit the performance
of ‚Äúoracle‚Äù models. The results not only demonstrate the
effectiveness of RayZer, but also shows the potential of 3D
Vision models to break free from supervised learning.

2. Related Work

Large-scale 3D Vision Models. 3D Vision models learn
3D representations and priors from data [15, 23, 39, 58,
59, 71, 72, 93, 94]. Recently, researchers have developed
large-scale models to acquire general 3D knowledge. One
research direction focuses on designing improved model
architectures that incorporate the inductive biases of multi-
view stereo [10, 14, 75, 86] and epipolar geometry [9, 13,
19, 25]. Another line of work leverages full transformer
models that intentionally omit architectural 3D inductive
biases [29, 54, 62]. For example, LEAP [29], LRMs [28,
74, 78, 91, 98], and DUSt3R [20, 43, 76, 77, 83] are the
first works employing transformers to convert 2D images
into 3D representations. SRT [62] and LVSM [33] further

Figure 2. Our proposed self-supervised training framework.
This is an abstract design that we later operationalize with our
RayZer model (illustrated in Fig. 3 and Sec. 4). We divide the input
images into two sets IA and IB. We predict the scene represen-
tation from IA, and use the predicted cameras of IB (shown in
orange) to render the scene. We leverage photometric loss between
raw input IB and its prediction ÀÜIB for training.

replace 3D representations and physical rendering equations
with latent representations and learned rendering functions,
improving performance and scalability. However, they still
require ground-truth camera poses for supervised training
and/or accurate camera annotations during inference. To
achieve scalable supervised learning, MegaSynth [32] and
Stereo4D [34] leverage synthetic data and stereo videos to
expand the data scale, however, curating data for different
tasks can be laborious. In contrast, RayZer explores self-
supervised training to break free from supervised learning.

Self-supervised 3D Representation Learning. Learning
3D-aware representations from unlabeled image data is a
long-standing problem in 3D Vision. One line of work lever-
ages single-view images. However, they either only work
for a specific category [7, 37, 47, 52, 53, 82] or can only
recover partial observations [8, 64, 81]. Some works ex-
plore semi-supervised learning and achieve better scalabil-
ity [30, 84], but performance is still highly restricted to the
model weights, which are initialized by fully supervised
training [85]. The most relevant work is self-supervised
learning from multi-view images [69, 79, 80]. For exam-
ple, Zhou et al. [95], Lai et al. [41], and their following
works [21, 87] use camera motion as 2D or 3D warping
operations to regularize learning. However, this strong in-
ductive bias limits the learning effectiveness. RUST [63] is
a pioneering work in learning latent scene representations
from unposed imagery. RayZer is different in three aspects.
First, RayZer initially estimates camera poses and uses poses
to condition the following latent reconstruction. In contrast,
RUST operates in an inverse pipeline ‚Äì it first reconstructs
the scene and then estimates the camera poses. Second,
RayZer employs different explicit pose representations to
improve information disentanglement and 3D awareness, en-

2

EncodersDecoder‚Ñêùìê‚ÑêùìëRendered	#‚ÑêùìëSceneCameras‚Ñí(‚Ñêùìë,‚Äô‚Ñêùìë)Zero 3D Supervision(camera, scene)for training!ImagesRepresentationsOutput  
abling novel view synthesis by geometrically interpolating
predicted poses. Instead, RUST uses a latent pose represen-
tation, which makes scene-pose disentanglement challeng-
ing and they are not explicitly 3D-aware. Third, RayZer
follows the model architecture of LVSM [33] using pure
self-attention in transformers, which is different from RUST
that follows SRT using convolution and cross-attention.
Optimization-based Unsupervised SfM, SLAM, and NVS.
Although these methods are not directly comparable to
RayZer, we discuss them due to the similar input-output
formulations. In detail, these methods optimize target predic-
tions on a per-scene basis [65, 67], while RayZer is a feed-
forward parametrized model, learning priors by training on
large data. The traditional SfM, SLAM, and NVS methods
are unsupervised [26, 65]. Although generally performing
well, they are restricted by the complicated hand-crafted
workflow, leading to requirements of dense-view inputs [89],
slow speed [65], and sensitivity to hyper-parameters [70].
Recent optimization-based NeRF and 3DGS works can also
perform NVS from unposed images [4, 22, 48, 66]. How-
ever, they do not have learnable model parameters to encode
priors, thus requiring off-the-shelf models trained with 3D
supervision as regularization or providing initialization.

3. Preliminaries

We introduce two important building blocks of RayZer, i.e.,
the latent set scene representation and how to render it.
Latent set scene representation. Compressing data into
tokens in latent space is a common practice in text, im-
age, video, etc. Recently, this representation has also been
extended to 3D research [33, 62, 63, 88]. In contrast to clas-
sical explicit (e.g., meshes and point clouds), implicit (e.g.,
NeRF [51] and SDF [55]), and hybrid (e.g., triplane [8] and
3DGS [40]) representations that are 3D-aware, the latent
set representation is not explicitly 3D-aware. It serves as a
compression of scene information, where the 3D-awareness
properties are fully learned. The latent set scene representa-
tion can be denoted as z ‚àà Rn√ód, where n is the number of
tokens in the set and d is the latent feature dimension.
Rendering latent set scene representation requires a net-
work, say RŒ∏, as introduced by SRT [62] and LVSM [33].
We formulate it as v = RŒ∏(z, r), where r is a ray and v is the
rendered property, e.g., RGB values, of the corresponding
pixel1. This formulation is the same as traditional Graph-
ics rendering techniques [1, 36], as v = R(SCENE, RAY),
where R is a pre-defined and handcrafted rendering equa-
tion, e.g., alpha-blending ray marching in NeRF. Differently,
our ‚Äúrendering equation‚Äù is a learned model parameterized
with weights Œ∏, and our scene representation is a latent token
set as discussed previously. We omit the model parametriza-
tion, e.g., weight Œ∏, in the following description for clarity.

1For improved efficiency and performance, LVSM groups rays from the

same image patch and decodes them jointly.

4. RayZer

In this section, we first introduce RayZer‚Äôs self-supervised
learning framework (Sec. 4.1). Then, we present the details
of the RayZer model architecture (Sec. 4.2).

4.1. RayZer‚Äôs Self-supervised Learning

We first formulate the input and output of RayZer. We then
introduce the self-supervised learning framework.

We focus on the standard setting of modeling static
scenes [65]. The input of RayZer is a set of unposed and
uncalibrated multi-view images I = {Ii ‚àà RH√óW √ó3|i =
1, ..., K}, which can come from unlabeled video frames or
image sets. The output is a parametrization of the inputs,
i.e., camera intrinsics, per-view camera poses, and scene
representation, enabling novel view synthesis. To predict
these representations, we build the RayZer model and train
it with self-supervised learning ‚Äì no 3D supervision, i.e., 3D
geometry, and camera pose annotations during training.

To train RayZer with self-supervision, we control the
data information flow. We split the input images I into two
non-overlapping subsets IA and IB, where IA ‚à™ IB = I
and IA ‚à© IB = ‚àÖ. RayZer uses IA to predict the scene
representation, and use IB for providing supervision. Thus,
RayZer renders images that correspond to IB, denoted as
ÀÜIB, and we apply photometric losses:

L =

1
KB

(cid:88)

ÀÜI‚àà ÀÜIB

(MSE(I, ÀÜI) + Œª ¬∑ Percep(I, ÀÜI)),

(1)

where KB = |IB| is the size (number of images) of IB ,
I ‚àà IB is the image that corresponds to a predicted image
ÀÜI, and Œª is the weight for perceptual loss [35, 46]. The two
sets are randomly sampled during training.

4.2. RayZer Model

Overview. As introduced in Sec. 4.1, RayZer recovers both
camera parameters and the scene representation from un-
posed, uncalibrated input images. A key design element
of RayZer is its cascaded prediction of camera and scene
representations. This is motivated by the fact that even noisy
cameras can be a strong condition for better scene reconstruc-
tion [31, 65, 92], which is analogous to traditional structure-
from-motion methods [65] and is in contrast with recent
reconstruction-first methods [63, 74, 77]. This design can
provide mutual regularization of predicting pose and scene
during training, facilitating self-supervised learning.

RayZer builds a pure transformer-based model, benefit-
ing from its scalability and flexibility. As shown in Fig. 3,
RayZer first tokenizes input images and uses a transformer-
based encoder to predict camera parameters of all views. In
this step, the cameras are represented by their intrinsics and
SE(3) camera poses. This low-dimensional, geometri-

3

 
Figure 3. RayZer self-supervised learning framework. RayZer takes in unposed and uncalibrated multi-view images I and predicts
per-view camera parameters and a scene representation, which supports novel view rendering. (Left) RayZer first estimates camera
parameters, where one view is selected as the canonical reference view (in blue box). RayZer predicts the intrinsics and the relative camera
poses P of all views. The predicted cameras are then converted into pixel-aligned Pl√ºcker ray maps R. (Middle) RayZer uses a subset of
input images, IA, as well as their previously predicted camera Pl√ºcker ray maps, RA, to predict a latent scene representation. Here, the
Pl√ºcker ray maps, RA, serve as an effective condition for scene reconstruction. (Right) RayZer can render a target image given the scene
representation z‚àó and a target camera. During training, we use RB, which is the previously predicted cameras Pl√ºcker ray maps of IB, to
render ÀÜIB. This allows training RayZer end-to-end with self-supervised photometric losses between inputs IB and their renderings ÀÜIB.

cally well-defined parametrization helps disentangle image
information from the camera representation.

RayZer then transforms the SE(3) camera poses and
intrinsics into Pl√ºcker ray maps [57], representing the pre-
dicted cameras as pixel-aligned rays. This ray-based rep-
resentation captures both the 2D ray-pixel alignment and
the 3D ray geometry, providing fine-grained, ray-level de-
tails that encapsulate the physical properties of the camera
model. The ray maps serve as a condition for improving the
reconstruction stage that follows.

From the image and predicted Pl√ºcker rays of IA, RayZer
uses another transformer-based encoder to predict the latent
set scene representation (introduced in Sec. 3 and detailed
later). Then, RayZer uses the previously estimated cameras
of IB to predict ÀÜIB, providing photometric self-supervision
(Eq. 1). We now formally introduce the RayZer model.

For all K input images I =
Image Tokenization.
{Ii ‚àà RH√óW √ó3|i = 1, ..., K}, we patchify them into non-
overlapping patches following ViT [18]. Each patch is in
Rs√ós√ó3, where s is the patch size. We use a linear layer to
encode each patch into a token in Rd, leading to a patch-
aligned token map fi ‚àà Rh√ów√ód for each image, where
h = H/s, w = W/s, and d is the latent dimension.

We then add positional embeddings (p.e.) to the tokens,
enabling the following model to be aware of the spatial
location and the corresponding image index of each token.
Specifically, we combine the sinusoidal spatial p.e. [18] and
the sinusoidal image index p.e. [3] using a linear layer; note
that the image index p.e. is shared among all tokens from
the same image. When training on continuous video frames,
these image index embeddings also encode sequential priors,

which benefits pose estimation. Finally, we reshape the token
maps of all images into a set, denoted as f ‚àà RKhw√ód (recall
that the transformer is invariant to the permutation of tokens).
For brevity, we will use this notation for latent token sets
throughout the rest of the paper.
Camera Estimator. The camera estimator Ecam predicts
camera parameters, i.e., camera poses and intrinsics, for all
input images. We use a learnable camera token in R1√ód as
the initial feature for this prediction for all views. We repeat
the token K times and add them with image index p.e. such
that they correspond to the K images. We denote this camera
feature initialization as p ‚àà RK√ód. We then use the camera
estimator composed of full self-attention transformer layers
to update the camera tokens, as:

{f ‚àó, p‚àó} = Ecam({f , p}),

(2)

where {¬∑, ¬∑} denotes concatenation along the token dimen-
sion (the union set of two token sets), and f ‚àó and p‚àó are the
updated tokens. We note that f ‚àó is not used for the following
computation ‚Äì it is only used as context to update p in the
transformer layers. For clarity, we formulate the transformer
layers as follows:

y0 = {f , p},
yl = TransformerLayerl(yl‚àí1), l = 1, ..., lT
{f ‚àó, p‚àó} = split(ylT ),

(3)

(4)

(5)

where lT is the number of layers, and the split operation
recovers the two token sets, inverting Eq. 3. This notation
remains consistent throughout the rest of the paper.

We then predict the camera parameters for each image
independently. For camera pose prediction, we follow prior

4

All Images ‚ÑêCameraEstimatorPoses ùí´ & IntrinsicsPredicted Pl√ºcker Ray Maps ‚ÑõPredicted ‚Ñõùìê& Input ‚Ñêùìê (Indexed from ‚Ñõ and ‚Ñê)Updated Scene Tokens ùê≥‚àóPredicted Images ·àò‚ÑêùìëImages ‚Ñêùìë(Indexed from ‚Ñê)Camera Tokens ùê©Camera EstimationLatent Scene ReconstructionùìõIndexingRendering (Training)+i.d.p.e.+i.d.p.e.ùêècùêèùüèùêèùüêùêèùüíùêèùüì(‚Ñêùìê Posed & Calibrated)‚Ñí = 1|‚Ñêùìë| œÉ·àòùêº‚àà·àò‚Ñêùìë (MSEùêº,·àòùêº+ùúÜ‚ãÖPercep(ùêº,·àòùêº))SceneTokens ùê≥RenderingDecoderSceneRecon-structorPredicted Pl√ºcker Ray ‚Ñõùìë of ‚Ñêùìë(Indexed from ‚Ñõ)  
works of using relative camera poses to resolve ambigu-
ity [31, 89]. We select one view as the canonical reference
(e.g., with identity rotation and zero translation), while for
every non-canonical view, we predict its relative pose with
respect to the canonical view. We parametrize the SO(3)
rotation using a continuous 6D representation [97], and we
predict the relative pose with a two-layer MLP as follows:

pi = MLPpose ([p‚àó

i , p‚àó

c ]),

(6)

i and p‚àó

where [¬∑, ¬∑] denotes concatenation along the feature dimen-
c (all in Rd) are the camera tokens for image
sion, p‚àó
Ii and the canonical view, respectively. The output pi ‚àà R9
represents the predicted pose parameters, which are then
transformed into an SE(3) pose Pi for image Ii.

For intrinsics prediction, following prior works [24, 41],
we parameterize intrinsics using a single focal length value,
under the assumptions that i) the focal lengths along the x
and y axes are identical, ii) all views share the same intrinsics,
and iii) the principal point is at the image center. We predict
the focal length using a two-layer MLP:

focal = MLPfocal (p‚àó

c ).

(7)

The predicted focal length is then converted into the intrin-
sics matrix K ‚àà R3√ó3.
Scene Reconstructor. As discussed in Sec. 4.1, we predict
the scene representation from image set IA and additionally
condition it on the previously predicted camera parameters
PA = {(Pi, K)|Ii ‚àà IA}. We first convert PA to pixel-
aligned Pl√ºcker rays [57] for each image, denoted as R ‚àà
RK√óH√óW √ó6. Similar to image inputs, we also tokenize
the Pl√ºcker rays into patch-level tokens using a linear layer,
yielding r ‚àà RKhw√ód. We index the image and Pl√ºcker rays
tokens corresponding to the image set IA, denoted as fA and
rA (each in RKAhw√ód, respectively). We fuse these tokens
along the feature dimension with a two-layer MLP:

scene representation predicted from IA. Meanwhile, x‚àó
discarded.

A is

Rendering Decoder. We first define the rendering decoder
and then describe its training usage.

We use a transformer-based decoder with full self-
attention for rendering, following LVSM [33]. For a target
image, we begin by representing it as pixel-aligned Pl√ºcker
rays and tokenize these rays using a linear layer to obtain tar-
get tokens r ‚àà Rhw√ód. Next, we fuse the scene information
by updating the tokens with a decoder Drender comprising
transformer layers:

{r‚àó, z‚Ä≤} = Drender ({r, z‚àó}),

(10)

where z‚Ä≤ is subsequently discarded, while the update rule
of Drender is the same as previously introduced modules.
Finally, we decode the RGB values at the patch level with
an MLP:

ÀÜI = MLPrgb(r‚àó),

(11)

where ÀÜI ‚àà Rhw√ó(3s2). We reshape ÀÜI to recover the 2D spa-
tial structure, yielding a final rendered image in RH√óW √ó3.
During training, we use the predicted Pl√ºcker ray maps
RB, which correspond to ÀÜIB, to render images of ÀÜIB and
then compute the self-supervised loss as defined in Eq. 1.

5. Experiments

In this section, we introduce the experimental setting and
present the evaluation results. For the implementation,
RayZer employs 24 transformer layers, with 8 layers for
each of the camera estimator, scene encoder, and rendering
decoder. We train RayZer with a learning rate of 4 √ó 10‚àí4
with a cosine scheduler for 50,000 iterations and a batch size
of 256. The weight of perceptual loss is Œª = 0.2. For all
experiments, we used a resolution of 256 with a patch size
of 16. More details are in the Appendix.

xA = MLPfuse ([fA, rA]),

(8)

5.1. Experimental Setup

where xA ‚àà RKAhw√ód represents the fused tokens. Impor-
tantly, we use the raw image tokens f rather than the pose
transformer output f ‚àó for this fusion. This design choice
prevents leakage of information from the image set IB, since
the camera estimator transformer producing f ‚àó has access to
a global context that includes tokens from IB.

We then employ a scene reconstructor Escene consisting
of full self-attention transformer layers to predict the latent
scene representation. To initialize this representation, we
use a set of learnable tokens z ‚àà RL√ód, where L denotes the
number of tokens. We formulate the process as follows:

{z‚àó, x‚àó

A} = Escene ({z, xA}).

(9)

The update rule is identical to the transformer layers in the
camera estimator Ecam. Here, z‚àó represents the final latent

We introduce our experimental setup, including datasets,
evaluation protocol and metrics, as well as baseline methods.
Datasets. We use three datasets to evaluate RayZer,
including two scene-level datasets, DL3DV [49] and
RealEstate [96], and an object-level dataset Objaverse [17]
(rendered as videos). We train and test on each dataset sep-
arately. The numbers of input views (IA) and target views
(IB) are set to 16 and 8 for DL3DV, 5 and 5 for RealEstate,
and 12 and 8 for Objaverse, respectively. We sample input
images with the index ranges of 64-96, 128-192, and 50-65
on DL3DV, RealEstate, and Objaverse, respectively. These
values are chosen based on data difficulty, especially camera
baseline, following prior works [9, 91, 98]. We use the offi-
cal DL3DV train-test split, and split RealEstate following [9].
More details can be found in the Appendix.

5

 
Figure 4. Visualization results on RealEstate and DL3DV. We compare RayZer with ‚Äúoracle‚Äù methods GS-LRM and LVSM, which use
COLMAP pose annotations in both training and testing. Our self-supervised RayZer model does not use any pose annotations. Generally,
RayZer performs on par with ‚Äúoracle‚Äù methods (first row), and can outperform them on cases that COLMAP usually struggles to handle,
e.g., glasses and white walls (highlighted with red boxes). The results verify our analysis on the problems of using COLMAP in Sec. 5.2.

Evaluation Protocol and Metrics. We evaluate novel view
synthesis quality. Specifically, the evaluation protocol of
RayZer is different from the ‚Äúoracle‚Äù and supervised meth-
ods, which use ground-truth poses to render images. Instead,
we use predicted poses to render novel views, thereby as-
sessing the compatibility between the predicted poses and
the scene representation. Since the model is trained without
explicit pose annotations, the learned poses exist in a differ-
ent space, and their direct correspondence to standard pose
annotations is unknown. This evaluation protocol follows
RUST [63]. We note that the target views are used only for
pose estimation and not for scene representation prediction,
ensuring that no information leakage occurs.

Baselines. We compare RayZer with two types of meth-
ods, including 1) ‚Äúoracle‚Äù methods, i.e., GS-LRM [91] and
LVSM [33] (encoder-decoder version), that use ground-truth
camera poses during both training (as supervision) and in-
ference (as pre-requisite). LVSM also uses latent set scene
representation. Thus, it serves as the main comparison for the
‚Äúoracle‚Äù methods; 2) supervised method, i.e., PF-LRM [74],
which requires camera supervision to learn pose estimation
and reconstruction; thus, it is pose-free during inference. For
fair comparisons, we use 16 transformer layers in total for
GS-LRM and LVSM. Thus, their number of parameters is
the same as RayZer, except that RayZer has another camera
estimator to handle unposed images. We use 24 transformer
layers for PF-LRM. We also consider the self-supervised

6

Training
Supervision

Inference w.
COLMAP Cam.

Even Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Random Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

‚ÄúOracle‚Äù methods (assume inputs are posed & use pose annotations during training)

GS-LRM 2D + Camera
LVSM
2D + Camera

Yes
Yes

23.49
23.69

0.712
0.723

0.252
0.242

23.02
23.10

0.705
0.703

0.266
0.257

Unsupervised methods (inputs are un-posed & no pose annotations used during training)

RayZer

2D

No

24.36

0.757

0.209

23.72

0.733

0.222

Table 1. Evaluation results on DL3DV. The camera annotations
used by the ‚Äúoracle‚Äù models come from COLMAP. The results are
reported with continuous video frames (ordered) as the input. The
results for the unordered image set input are in Table. 4. The input
and target views can be evenly or randomly sampled from video
frames. We bold our result if it is better than the ‚Äúoracle‚Äù models.

method RUST [63], but since it does not have an official
public implementation, we ablate the key design differences
between RUST and RayZer in Table 7 instead.

5.2. Results
Main results. Table 1-3 summarizes the results on the three
datasets. Remarkably, without any 3D labels (e.g., cam-
era pose annotations) during training, RayZer achieves per-
formance comparable to the best ‚Äúoracle‚Äù model, LVSM.
In fact, RayZer even outperforms LVSM on DL3DV and
RealEstate10k while performing slightly worse on Obja-
verse. We conjecture that this is because the camera poses in
DL3DV and RealEstate are annotated by COLMAP, which
can be imperfect and set an upper bound for ‚Äúoracle‚Äù meth-
ods that are supervised by COLMAP annotations. In contrast,
our self-supervised approach enables the model to learn a

GS-LRMLVSMGTRayZer (ours)View 1View 2GS-LRMLVSMGTRayZer (ours)View 1View 2 
Training
Supervision

Inference w.
GT Cam.

Even Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Random Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

‚ÄúOracle‚Äù methods (assume inputs are posed & use pose annotations during training)

LVSM

2D + GT Cam.

Yes

32.34

0.950

0.050

32.34

0.949

0.051

Supervised methods (inputs are un-posed & use pose annotations during training)

PF-LRM 2D + GT Cam. Yes (render)

25.48

0.882

0.110

25.43

0.881

0.111

Unsupervised methods (inputs are un-posed & no pose annotations used during training)

RayZer

2D

No

31.52

0.945

0.052

31.42

0.943

0.053

Table 3. Evaluation results on Objaverse with continuous video
frames inputs. The camera annotations are Blender ground-truth.
PF-LRM uses ground-truth poses to render novel views, same with
oracle methods, and we evaluate its predicted pose in Table 5.

Training

Inf. w.
Supervision GT Pose

Continuous
Inputs

Even Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Random Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

(1)
(2)

2D
2D

No
No

‚úì
‚úó

24.36
20.56

0.757
0.576

0.209
0.334

23.72
20.02

0.733
0.566

0.222
0.356

Table 4. Evaluating RayZer performance when using continu-
ous or unordered images for training on DL3DV. In evaluations,
the input frames are sampled from continuous video frames. (1)
keeps their temporal continuity (encoded by the image index p.e.)
during training. (2) randomly shuffles the images during training.

on unordered image sets that are often limited in scale and
contain noisy content [45, 73].

5.3. Analysis of Camera Poses
RayZer‚Äôs learned camera pose space. We visualize some
camera poses predicted by RayZer in Fig. 6. Although
RayZer predicts SE(3) camera poses, we observe that these
poses do not exactly match the real-world pose space. This
result indicates that the SE(3) poses, which are later con-
verted into Pl√ºcker ray maps, offer a degree of flexibility.
Since both the rendering decoder and the scene representa-
tion operate in latent space, RayZer remains robust to any
warping between the learned pose space and the actual real-
world poses, as long as the poses are compatible with the
scene representation and the decoder.
3D Awareness of predicted camera poses. We further in-
vestigate whether the pose space learned by RayZer is 3D
aware. To this end, we interpolate the predicted poses of in-
put views to synthesize more novel views, where the camera
pose of a novel view is interpolated from two neighboring
input views. We use ground-truth camera poses to calculate
the interpolation coefficients, checking whether predicted
poses follow the same geometric interpolation rules. We in-
clude the details of the interpolation method in Appendix. As
shown in Table 5, RayZer demonstrates significantly better
performance than PF-LRM and the naive baseline of copying
the nearest rendered input view. These results verify that
poses predicted by RayZer are interpolatable and 3D-aware.
Probing the learned camera pose space. To probe how
much actual pose information is learned by RayZer, we fol-
low RUST [63] to fit a lightweight 2-layer MLP head on
the pose features. We freeze the camera estimator‚Äôs trans-
formers and train the MLP under camera supervision. As
shown in Table 6, our probing outperforms the supervised
baseline (which has the same model architecture and uses

Figure 5. Visualization results on Objaverse. RayZer performs on
par with LVSM and outperforms the supervised method PF-LRM.

Training
Supervision

Inference w.
COLMAP Cam.

Even Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Random Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

‚ÄúOracle‚Äù methods (assume inputs are posed & use pose annotations during training)

GS-LRM 2D + Camera
2D + Camera
LVSM

Yes
Yes

24.25
27.00

0.770
0.851

0.227
0.157

23.21
25.88

0.748
0.828

0.251
0.175

Unsupervised methods (inputs are un-posed & no pose annotations used during training)

RayZer

2D

No

27.48

0.861

0.146

26.32

0.835

0.164

Table 2. Evaluation results on RealEstate with continuous video
frames inputs. The camera annotations come from COLMAP.

pose space that optimally benefits latent reconstruction and
novel view synthesis. This hypothesis is further supported
by the results on Objaverse ‚Äì a synthetic dataset with perfect
pose annotations from the rendering tool ‚Äì where LVSM,
acting as a true oracle, outperforms RayZer. Nonetheless,
the small performance gap showcases the effectiveness of
our self-supervised training. Visualizations in Fig. 4 further
support our conjecture regarding COLMAP‚Äôs noisy poses,
as both LVSM and GS-LRM consistently underperform on
challenging cases that COLMAP usually fails. These results
not only validate our self-supervised learning approach but
also demonstrate its potential to break free from the limita-
tions of supervised learning.
Using unordered image sets for training. RayZer can be
trained on continuous video frames (Table 1-3) or unordered
image sets (Table 4). Note that these two training settings
are applied separately. As shown in Table 4, we observe
that the model trained with unordered image sets performs
worse than the one trained with continuous video frames.
We notice that the difference is at the pose estimation stage ‚Äì
specifically, the image index positional embedding encour-
ages local pose smoothness that benefits the learning of
pose estimation on continuous frames. This finding suggests
that scaling training data using video resources, which are
plentiful online, could be more advantageous than relying

7

View 1View 2View 1View 2PF-LRMLVSMGTRayZer (ours) 
Training
Supervision

Inference w.
GT Pose

Even Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Random Sample
SSIM‚Üë

LPIPS‚Üì

PSNR‚Üë

Supervised methods (inputs are un-posed & use pose annotations during training)

PF-LRM

2D + GT Pose

No

20.63

0.819

0.160

21.27

0.827

0.154

Unsupervised methods (inputs are un-posed & no pose annotations used during training)

RayZer-copy
RayZer

2D
2D

No
No

19.56
27.01

0.812
0.900

0.159
0.075

20.17
26.87

0.820
0.896

0.150
0.078

Table 5. Evaluating 3D awareness of predicted camera poses
on Objaverse. Unlike Table 3, here we render novel views by
interpolating predicted poses of input views, where the interpolation
coefficients are calculated from GT poses. This experiment tests
whether the learned SE(3) poses are geometrically well-defined
and 3D-aware. We also compare against a naive baseline ‚ÄúRayZer-
copy‚Äù that simply copies the nearest rendered input view.

transformers trained from scratch), indicating that RayZer‚Äôs
novel view synthesis self-supervision facilitates a better la-
tent pose space. In contrast, supervised learning struggles
due to the challenges of low-dimensional pose representa-
tion [5, 11, 44, 90, 97].

5.4. Ablation Study
We ablate the main design choices of RayZer from three
aspects, including scene representation, 3D prior, and the
overall model paradigm. As shown in Table 7 (1), when
using the 3DGS representation rather than the latent set rep-
resentation, the training does not converge. This verifies the
optimization difficulty of explicit 3D representation [40, 91]
and demonstrates the flexibility of the latent representation
with its learned rendering decoder.

Table 7 (2) and (3) ablate the prior of camera representa-
tion. Without Pl√ºcker ray maps, we observe a degraded per-
formance in (2), showing the effectiveness of using Pl√ºcker
ray maps to regularize the solution of structure-and-motion
problem. Besides, we observe a slightly better performance
of (3), which directly uses camera tokens p‚àó, compared to
(2). The reason is that the camera tokens p‚àó ‚àà Rd can
leak target image information, while SE(3) poses used in
(2) serve as an information bottleneck to enforce this dis-
entanglement. Moreover, SE(3) poses are geometrically
well-defined, allowing us to interpolate them and generate
novel views along the interpolated camera trajectory, while
the latent camera representation is not directly interpolable.
Table 7 (4) ablates the overall paradigm. When the model
first predicts the latent scene and then estimates poses, we
observe a degraded performance. In detail, the pose estima-
tor takes the scene representation and target image feature
tokens as inputs. The result verifies our insight that pose
estimation can be a strong condition for scene reconstruction,
championing traditional pose-first methods in the context of
self-supervised learning. Note that combining (3) and (4)
will be a model that is similar to RUST conceptually.

6. Conclusion
We introduce RayZer, a self-supervised large multi-view
3D Vision model trained with zero 3D supervision, i.e., no

Figure 6. Visualization of RayZer predicted cameras learned
with self-supervision. We visualize 3 out of 5 rendered views due
to space limit, where the image index is highlighted by its color.

Pose Encoder
(Epose)

Rotation Acc.‚Üë (%)
R@10‚ó¶ R@20‚ó¶ R@30‚ó¶

Translation Acc.‚Üë (%)
t@0.3
t@0.2
t@0.1

DL3DV

RealEstate

supervised
self-supervised

supervised
self-supervised

39.3
47.6

87.0
99.6

63.0
72.5

96.4
99.9

77.8
84.0

99.6
100

15.7
20.8

44.6
61.2

33.1
44.0

59.3
84.2

44.4
60.5

82.5
92.8

37.2
52.7

15.1
20.1

66.0
86.8

19.8
33.6

Objaverse

supervised
self-supervised

46.7
69.2
Table 6. Effectiveness of self-supervised pre-training for pose
estimation. We train a two-layer MLP (with supervised learning)
to read out latent camera tokens p‚àó predicted by the pose encoder
Epose , where the backbone is frozen. At the same time, we also
compare with the baseline where both encoder Epose and the pose
prediction MLP are trained with supervised learning from scratch.

53.8
75.5

Even Sample
PSNR SSIM LPIPS

Random Sample
PSNR SSIM LPIPS

(0) RayZer

24.36

0.757

0.209

23.72

0.733

0.222

(1) Representation - 3DGS + rasterization

‚Äì

‚Äì

failed

‚Äì

‚Äì

(2)
(3)

(4)

Prior - no Pl√ºcker ray, use SE(3) pose
Prior - no explicit pose, use latent camera

22.73
23.13

0.687
0.700

0.249
0.251

21.88
22.36

0.647
0.668

0.274
0.272

Paradigm - scene first, not pose first

13.31

0.338

0.732

13.12

0.337

0.729

Table 7. Ablation study of RayZer designs on DL3DV with con-
tinuous inputs. (1) is a variant uses the 3D Gaussian representation
rather than latent scene representation with its learned rendering
decoder used by RayZer; (2) does not use Pl√ºcker ray maps RA for
conditioning latent reconstruction. Instead, it encodes the SE(3)
poses PA and intrinsics K into tokens as condition; (3) directly
uses the latent camera tokens p‚àó, rather than converting it to any
explicit forms of cameras, to condition the latent scene reconstruc-
tion; (4) first reconstructs latent scene and then estimates pose as
Pl√ºcker ray maps, contrasting our pose-first paradigm.

3D geometry and camera annotations. RayZer achieves
comparable or even better novel view synthesis performance
than prior works that use pose labels in both training and
inference, verifying the feasibility of breaking free from
supervised learning.

8

Rendered Views (3/5)Cameras 
References

[1] Arthur Appel. Some techniques for shading machine render-
ings of solids. In Proceedings of the April 30‚ÄìMay 2, 1968,
spring joint computer conference, pages 37‚Äì45, 1968.
[2] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar,
Alan L Yuille, Trevor Darrell, Jitendra Malik, and Alexei A
Efros. Sequential modeling enables scalable learning for large
vision models. In CVPR, pages 22861‚Äì22872, 2024.
[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.

Is
space-time attention all you need for video understanding? In
ICML, page 4, 2021.

[4] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Vic-
tor Adrian Prisacariu. Nope-nerf: Optimising neural radiance
field with no pose prior. In CVPR, pages 4160‚Äì4169, 2023.
[5] Romain Br√©gier. Deep regression on manifolds: a 3d rotation
case study. In 2021 International Conference on 3D Vision
(3DV), pages 166‚Äì174. IEEE, 2021.

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. NeurIPS, 33:1877‚Äì1901, 2020.
[7] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-gan: Periodic implicit generative
adversarial networks for 3d-aware image synthesis. In CVPR,
pages 5799‚Äì5809, 2021.

[8] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient
geometry-aware 3d generative adversarial networks. In CVPR,
pages 16123‚Äì16133, 2022.

[9] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and
Vincent Sitzmann. pixelsplat: 3d gaussian splats from image
pairs for scalable generalizable 3d reconstruction. In CVPR,
pages 19457‚Äì19467, 2024.

[10] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance field reconstruction from multi-view stereo.
In ICCV, pages 14124‚Äì14133, 2021.

[11] Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen,
Leonidas J Guibas, and He Wang. Projective manifold gra-
dient layer for deep rotation regression. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6646‚Äì6655, 2022.

[12] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
Training deep nets with sublinear memory cost. arXiv preprint
arXiv:1604.06174, 2016.

[13] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-
Jen Cham, and Jianfei Cai. Explicit correspondence match-
ing for generalizable neural radiance fields. arXiv preprint
arXiv:2304.12294, 2023.

[14] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang,
Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei
Cai. Mvsplat: Efficient 3d gaussian splatting from sparse
multi-view images. In ECCV, pages 370‚Äì386. Springer, 2024.
[15] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A unified approach for

9

single and multi-view 3d object reconstruction. In ECCV,
pages 628‚Äì644. Springer, 2016.

[16] Tri Dao.

Flashattention-2: Faster attention with bet-
arXiv preprint

ter parallelism and work partitioning.
arXiv:2307.08691, 2023.

[17] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,
Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe
of annotated 3d objects. In CVPR, pages 13142‚Äì13153, 2023.
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
arXiv preprint
formers for image recognition at scale.
arXiv:2010.11929, 2020.

[19] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitz-
mann. Learning to render novel views from wide-baseline
stereo pairs. In CVPR, pages 4970‚Äì4980, 2023.

[20] Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vin-
cent Leroy, Yohann Cabon, and Jerome Revaud. Mast3r-sfm:
a fully-integrated solution for unconstrained structure-from-
motion. arXiv preprint arXiv:2409.19152, 2024.

[21] Yang Fu, Ishan Misra, and Xiaolong Wang. Mononerf: Learn-
ing generalizable nerfs from monocular videos without cam-
era poses. In International Conference on Machine Learning,
pages 10392‚Äì10404. PMLR, 2023.

[22] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A
Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting.
In CVPR, pages 20796‚Äì20805, 2024.

[23] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-
hinav Gupta. Learning a predictable and generative vector
representation for objects. In ECCV, pages 484‚Äì499. Springer,
2016.

[24] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos,
and Adrien Gaidon. 3d packing for self-supervised monocu-
lar depth estimation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
2485‚Äì2494, 2020.

[25] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu.

Epipolar transformers. In CVPR, pages 7779‚Äì7788, 2020.

[26] Benno Heigl, Reinhard Koch, Marc Pollefeys, Joachim Den-
zler, and Luc Van Gool. Plenoptic modeling and rendering
from image sequences taken by a hand-held camera. In Mus-
tererkennung 1999: 21. DAGM-Symposium Bonn, 15.‚Äì17.
September 1999, pages 94‚Äì101. Springer, 1999.

[27] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and
Yuxuan Chen. Query-key normalization for transformers.
arXiv preprint arXiv:2010.04245, 2020.

[28] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao
Tan. Lrm: Large reconstruction model for single image to 3d.
arXiv preprint arXiv:2311.04400, 2023.

[29] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang.
Leap: Liberate sparse-view 3d modeling from camera poses.
arXiv preprint arXiv:2310.01410, 2023.

[30] Hanwen Jiang, Qixing Huang, and Georgios Pavlakos.
Real3d: Scaling up large reconstruction models with real-
world images. arXiv preprint arXiv:2406.08479, 2024.

 
[31] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke
Zhu. Few-view object reconstruction with unknown cate-
gories and camera poses. In 2024 International Conference
on 3D Vision (3DV), pages 31‚Äì41. IEEE, 2024.

[32] Hanwen Jiang, Zexiang Xu, Desai Xie, Ziwen Chen, Haian
Jin, Fujun Luan, Zhixin Shu, Kai Zhang, Sai Bi, Xin Sun,
et al. Megasynth: Scaling up 3d scene reconstruction with
synthesized data. arXiv preprint arXiv:2412.14166, 2024.

[33] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi,
Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang
Xu. Lvsm: A large view synthesis model with minimal 3d
inductive bias. arXiv preprint arXiv:2410.17242, 2024.
[34] Linyi Jin, Richard Tucker, Zhengqi Li, David Fouhey, Noah
Snavely, and Aleksander Holynski. Stereo4d: Learning how
things move in 3d from internet stereo videos. arXiv preprint
arXiv:2412.09621, 2024.

[35] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution. In
ECCV, pages 694‚Äì711. Springer, 2016.

[36] James T Kajiya. The rendering equation. In Proceedings
of the 13th annual conference on Computer graphics and
interactive techniques, pages 143‚Äì150, 1986.

[37] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Ji-
tendra Malik. Learning category-specific mesh reconstruction
from image collections. In ECCV, pages 371‚Äì386, 2018.
[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for
neural language models. arXiv preprint arXiv:2001.08361,
2020.

[39] Abhishek Kar, Christian H√§ne, and Jitendra Malik. Learning

a multi-view stereo machine. NIPS, 30, 2017.

[40] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and
George Drettakis. 3d gaussian splatting for real-time radiance
field rendering. ACM Trans. Graph., 42(4):139‚Äì1, 2023.
[41] Zihang Lai, Sifei Liu, Alexei A Efros, and Xiaolong Wang.
Video autoencoder: self-supervised disentanglement of static
3d structure and motion. In ICCV, pages 9730‚Äì9740, 2021.
[42] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru
Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haz-
iza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov.
xformers: A modular and hackable transformer modelling li-
brary. https://github.com/facebookresearch/
xformers, 2022.

[43] Vincent Leroy, Yohann Cabon, and J√©r√¥me Revaud. Ground-
ing image matching in 3d with mast3r. In ECCV, pages 71‚Äì91.
Springer, 2024.

[44] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely,
Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh Maka-
dia. An analysis of svd for deep rotation estimation. Advances
in Neural Information Processing Systems, 33:22554‚Äì22565,
2020.

[45] Zhengqi Li and Noah Snavely. Megadepth: Learning single-
view depth prediction from internet photos. In Proceedings
of the IEEE conference on computer vision and pattern recog-
nition, pages 2041‚Äì2050, 2018.

[46] Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely.
Crowdsampling the plenoptic function. In ECCV, pages 178‚Äì
196. Springer, 2020.

[47] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. Sdf-srn:
Learning signed distance 3d object reconstruction from static
images. NeurIPS, 33:11453‚Äì11464, 2020.

[48] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon
Lucey. Barf: Bundle-adjusting neural radiance fields.
In
ICCV, pages 5741‚Äì5751, 2021.

[49] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,
Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al.
Dl3dv-10k: A large-scale scene dataset for deep learning-
based 3d vision. In CVPR, pages 22160‚Äì22169, 2024.
[50] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory
Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael
Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed
precision training. In International Conference on Learning
Representations, 2018.

[51] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view synthe-
sis. Communications of the ACM, 65(1):99‚Äì106, 2021.
[52] Siva Karthik Mustikovela, Varun Jampani, Shalini De Mello,
Sifei Liu, Umar Iqbal, Carsten Rother, and Jan Kautz. Self-
supervised viewpoint learning from image collections. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 3971‚Äì3981, 2020.
[53] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. Hologan: Unsupervised
learning of 3d representations from natural images. In ICCV,
pages 7588‚Äì7597, 2019.

[54] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela
Mishkin, and Mark Chen. Point-e: A system for generat-
ing 3d point clouds from complex prompts. arXiv preprint
arXiv:2212.08751, 2022.

[55] Jeong Joon Park, Peter Florence, Julian Straub, Richard New-
combe, and Steven Lovegrove. Deepsdf: Learning continuous
signed distance functions for shape representation. In CVPR,
pages 165‚Äì174, 2019.

[56] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In ICCV, pages 4195‚Äì4205, 2023.
[57] Julius Plucker. Xvii. on a new geometry of space. Philo-
sophical Transactions of the Royal Society of London, (155):
725‚Äì791, 1865.

[58] Charles R Qi, Hao Su, Matthias Nie√üner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. Volumetric and
multi-view cnns for object classification on 3d data. In CVPR,
pages 5648‚Äì5656, 2016.

[59] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J
Guibas. Frustum pointnets for 3d object detection from rgb-d
data. In CVPR, pages 918‚Äì927, 2018.

[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsuper-
vised multitask learners. OpenAI blog, 1(8):9, 2019.

[61] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams,
et al. Learning internal representations by error propagation,
1985.

10

 
[62] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs
Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario
LuÀáci¬¥c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene
representation transformer: Geometry-free novel view synthe-
sis through set-latent scene representations. In CVPR, pages
6229‚Äì6238, 2022.

[63] Mehdi SM Sajjadi, Aravindh Mahendran, Thomas Kipf, Eti-
enne Pot, Daniel Duckworth, Mario LuÀáci¬¥c, and Klaus Greff.
Rust: Latent neural scene representations from unposed im-
agery. In CVPR, pages 17297‚Äì17306, 2023.

[64] Kyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang,
Charles Herrmann, Pratul Srinivasan, Jiajun Wu, and De-
qing Sun. Vq3d: Learning a 3d-aware generative model on
imagenet. In ICCV, pages 4240‚Äì4250, 2023.

[65] Johannes L Schonberger and Jan-Michael Frahm. Structure-

from-motion revisited. In CVPR, pages 4104‚Äì4113, 2016.

[66] Cameron Smith, David Charatan, Ayush Tewari, and Vin-
cent Sitzmann. Flowmap: High-quality camera poses, in-
trinsics, and depth via gradient descent. arXiv preprint
arXiv:2404.15259, 2024.

[67] Randall C Smith and Peter Cheeseman. On the representa-
tion and estimation of spatial uncertainty. The International
Journal of Robotics Research, 5(4):56‚Äì68, 1986.

[68] Noah Snavely. Lecture 15: Structure from motion. https:
/ / www . cs . cornell . edu / courses / cs5670 /
2017sp/lectures/lec15_sfm.pdf, 2017.

[69] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive
multiview coding. In ECCV, pages 776‚Äì794. Springer, 2020.
[70] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,
and Federico Tombari. Sparf: Neural radiance fields from
sparse and noisy poses. In CVPR, pages 4190‚Äì4200, 2023.

[71] Shubham Tulsiani, Saurabh Gupta, David F Fouhey, Alexei A
Efros, and Jitendra Malik. Factoring shape, pose, and layout
from the 2d image of a 3d scene. In CVPR, pages 302‚Äì310,
2018.

[72] Hsiao-Yu Fish Tung, Ricson Cheng, and Katerina Fragki-
adaki. Learning spatial common sense with geometry-aware
recurrent networks. In CVPR, pages 2595‚Äì2603, 2019.
[73] Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai
Zhang, Gordon Wetzstein, Bharath Hariharan, and Noah
Snavely. Megascenes: Scene-level view synthesis at scale. In
European Conference on Computer Vision, pages 197‚Äì214.
Springer, 2024.

[74] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan,
Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai
Zhang. Pf-lrm: Pose-free large reconstruction model for joint
pose and shape prediction. arXiv preprint arXiv:2311.12024,
2023.

[75] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srini-
vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser.
Ibrnet:
Learning multi-view image-based rendering. In CVPR, pages
4690‚Äì4699, 2021.

[76] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A
Efros, and Angjoo Kanazawa. Continuous 3d perception
model with persistent state. arXiv preprint arXiv:2501.12387,
2025.

[77] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris
Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d
vision made easy. In CVPR, pages 20697‚Äì20709, 2024.
[78] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan,
Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zex-
iang Xu. Meshlrm: Large reconstruction model for high-
quality meshes. arXiv preprint arXiv:2404.12385, 2024.
[79] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Ro-
main Br√©gier, Yohann Cabon, Vaibhav Arora, Leonid Ants-
feld, Boris Chidlovskii, Gabriela Csurka, and J√©r√¥me Revaud.
Croco: Self-supervised pre-training for 3d vision tasks by
cross-view completion. Advances in Neural Information Pro-
cessing Systems, 35:3502‚Äì3516, 2022.

[80] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
Feichtenhofer, and Georgia Gkioxari. Multiview compressive
coding for 3d reconstruction. In CVPR, pages 9065‚Äì9075,
2023.

[81] Jianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong.
3d-aware image generation using 2d diffusion models. In
ICCV, pages 2383‚Äì2393, 2023.

[82] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and
Honglak Lee. Perspective transformer nets: Learning single-
view 3d object reconstruction without 3d supervision. NIPS,
29, 2016.

[83] Jianing Yang, Alexander Sax, Kevin J Liang, Mikael Henaff,
Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt
Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images
in one forward pass. arXiv preprint arXiv:2501.13928, 2025.
[84] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi
Feng, and Hengshuang Zhao. Depth anything: Unleashing
In CVPR, pages
the power of large-scale unlabeled data.
10371‚Äì10381, 2024.

[85] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-
gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything
v2. NeurIPS, 37:21875‚Äì21911, 2025.

[86] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images. In
CVPR, pages 4578‚Äì4587, 2021.

[87] Yanjie Ze, Nicklas Hansen, Yinbo Chen, Mohit Jain, and
Xiaolong Wang. Visual reinforcement learning with self-
supervised 3d representations. IEEE Robotics and Automation
Letters, 8(5):2890‚Äì2897, 2023.

[88] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter
Wonka. 3dshape2vecset: A 3d shape representation for neural
fields and generative diffusion models. ACM Transactions
On Graphics (TOG), 42(4):1‚Äì16, 2023.

[89] Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Rel-
pose: Predicting probabilistic relative rotation for single ob-
jects in the wild. In ECCV, pages 592‚Äì611. Springer, 2022.
[90] Jason Y Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan
Yang, Deva Ramanan, and Shubham Tulsiani. Cameras
as rays: Pose estimation via ray diffusion. arXiv preprint
arXiv:2402.14817, 2024.

[91] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao,
Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large recon-
struction model for 3d gaussian splatting. arXiv preprint
arXiv:2404.19702, 2024.

11

 
[92] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue,
Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon
Wetzstein. Flare: Feed-forward geometry, appearance and
camera estimation from uncalibrated sparse views. arXiv
preprint arXiv:2502.12138, 2025.

[93] Tianyuan Zhang, Zhengfei Kuang, Haian Jin, Zexiang Xu, Sai
Bi, Hao Tan, He Zhang, Yiwei Hu, Milos Hasan, William T.
Freeman, Kai Zhang, and Fujun Luan. Relitlrm: Generative
relightable radiance for large reconstruction models, 2024.

[94] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubin-
stein, Noah Snavely, and William T Freeman. Structure and
motion from casual videos. In ECCV, pages 20‚Äì37. Springer,
2022.

[95] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In CVPR, pages 1851‚Äì1858, 2017.

[96] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely.
Stereo magnification: Learning
view synthesis using multiplane images. arXiv preprint
arXiv:1805.09817, 2018.

[97] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 5745‚Äì5753,
2019.

[98] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong
Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Long-sequence
large reconstruction model for wide-coverage gaussian splats.
arXiv preprint 2410.12781, 2024.

A. Experimental Details

In this section, we introduce more details of RayZer.
Objaverse Data Details. We render Objaverse as contin-
uous videos for training and evaluation. The frames are
rendered with corresponding cameras on a unit sphere with a
constant distance to the object center. Specifically, we render
about 70 frames for azimuth 0‚ó¶ to 360‚ó¶, where the elevation
is randomly sampled between -20‚ó¶ to 60‚ó¶ for each shape
instance. We sample frames with the distance between the
first frame and the last frame being 50 to 65, covering the
camera azimuth rotation for about one cycle.
Camera Interpolation Details. For the experiment of inter-
polating predicted cameras, we use Spherical Linear Inter-
polation (Slerp) for interpolating the camera pose rotation.
This is based on the fact that the camera of Objaverse is
moving at a constant speed. Thus, Slerp ensures the correct
rotation interpolation. We then find the location on the unit
sphere that corresponds to this interpolated rotation angle.
Thus, we ensure the interpolated cameras are still on the
unit sphere, which matches the camera sampling rule for
rendering. In conclusion, this interpolation assumes that 1)
the camera is moving in a constant speed, and 2) the rule of
sampling camera location is known. Thus, this interpolation
is only applicable to the synthetic Objaverse data, and does
not apply to DL3DV and RealEstate.
More Training Details. For all transformer layers in RayZer,
we apply QK-Norm [27] to stabilize the training. We use a la-
tent dimension of 768 for RayZer and all baselines methods.
RayZer and LVSM both use a latent set scene representation
with 3072 tokens. We use mixed precision training [50]
with BF16, further accelerated by FlashAttention-V2 [16] of
xFormers [42] and gradient checkpointing [12].

We train RayZer and all baselines with the same training
protocol. We use 32 A100 GPUs with a total batch size
of 256. During training, we warm up with 3000 iterations,
using a linearly increased learning rate from 0 to 4e ‚àí 4. We
apply a cosine learning rate decay, while the final learning
rate is 1.5e ‚àí 4. We train all baselines with 50, 000 steps.
We clip the gradient with norm larger than 1.0. We follow
all other hyper-parameters of LVSM.
More Model Details. Following LVSM, we do not use bias
terms in linear and normalization layers. We also apply the
depth-wise initialization for transformer layers.
Ablation details. In Table 7 (2), we use a two-layer MLP to
encode the camera pose and intrinsics back to a latent pose
representation in Rd. In detail, for the predicted pose of each
image (in 6D representation [97]), and the camera intrinsics
(as the 4-dimensional focal length and principal points of
x-axis and y-axis), we first concatenate them, getting a 10-
dimensional pose representation. Then, we use the MLP
to map it as a high-dimensional pose feature token. To
predict the target views, we use a set of learnable patch-
aligned spatial tokens shared across all target images as the

12

 
Even Sample
PSNR SSIM LPIPS

Random Sample
PSNR SSIM LPIPS

(0) RayZer

24.36

0.757

0.209

23.72

0.733

0.222

(1)

(2)

first frame as canonical

23.86

0.736

0.224

23.78

0.737

0.225

no curriculum

23.87

0.734

0.226

23.87

0.735

0.226

Table 8. Ablation study of RayZer techniques to train on con-
tinuous video frames. (1) is a variant choosing the first image in
the sequence as the canonical view, rather than choosing the middle
frame. (2) does not use the frame sampling curriculum.

initialization. Thus, the rendering decoder takes in the spatial
tokens, the scene tokens, and the pose token. After using
transformer for updating, we use the updated spatial tokens
to regress the pixel values.

B. RayZer Training with Continuous Inputs

RayZer takes in multi-view image inputs, which can be sam-
pled from either continuous video frames or an unordered
image set. In this section, we present two design choices to
improve self-supervised learning on video frames input.

Canonical View Selection. Prior works [29, 74] usually
select the first image in an image sequence as the canonical
view. In contrast, we select the frame at the middle time-step
as canonical. In this context, the pose prediction MLPpose
initialized with a zero mean for its weights will have a small
pose data variance. Otherwise, when using the first frame
as canonical, the variance can be much larger. Note that
this difference in pose variance can be easily handled with
ground-truth camera supervision, thus, prior works choose
the first image as the canonical view. However, this is more
important for unsupervised methods, like RayZer.

Curriculum. We gradually increase the training difficulty
by sampling video frames with an increasing distance range.
With proper initialization of the model for camera pose esti-
mation, it first learns from images with small camera base-
lines, benefiting the training with larger camera baselines,
that follows. In detail, we use a curriculum with a frame
sampling range of 48-64, 96-128, and 24-32 at the beginning
of training for DL3DV, RealEstate, and Objaverse, respec-
tively. The frame sampling range is linearly increased to
64-96, 128-192, and 48-65 at the end of training for DL3DV,
RealEstate, and Objaverse, respectively. The final frame
sampling range is also used for the evaluation. The sampling
ranges are set based on the difficulty (camera baseline) of
each dataset, following prior works [9, 32, 74, 91, 98].

Experiments. We include ablations in Table 8, where re-
moving any of the previously discussed techniques leads to a
degraded performance. This demonstrates the effectiveness
of our designs of selecting canonical view and using frame
sampling curriculum during training.

Figure 7. Visualization of RayZer failure cases on DL3DV.

C. More Results

In this section, we present more results for discussing
RayZer‚Äôs failure cases and show more visualizations.

Failure Case Pattern. We observe that RayZer can fail when
dealing with fine-grained geometry, complicated materials,
and occlusions. We present the visualization in Fig. 7. In
detail, RayZer fails to handle complicated plant geometry
(first row). This failure is not specific to RayZer ‚Äì GS-LRM
and LVSM also can not handle it. In the second and last
row, RayZer fails to handle multiple stacked glasses and is
not perfect on the specular reflection of the silver teapots.
GS-LRM and LVSM also demonstrate imperfect results. In
the third and fourth rows, all methods, including RayZer, fail
to handle occlusions, where the side view of the exhibition
stand is not observed in input views (third row), and the
chairs in the fourth row have self-occlusion.

More Comparisons. We present more visualization results,
comparing with GS-LRM and LVSM in Fig. 8. RayZer
generally performs on par, while being a self-supervised
method that does not require any camera pose annotations.

More Visualization. We present more visualization results
comparing with ground-truth novel views in Fig. 9-11.

13

GS-LRMLVSMGTRayZer (ours) 
D. More Discussion

Why does RayZer demonstrates strong novel view synthesis
quality while the fine-tuned pose estimation is not perfect
(Table 7 in the main manuscript)? We conjecture RayZer‚Äôs
pose space jointly learns the actual pose information and
3D-aware video frame interpolation at the same time. On
datasets with small camera baselines (RealEstate), which
is easy to learn, RayZer mainly focuses on learning actual
pose estimation. This is supported by the accurate pose
estimation performance on RealEstate. On datasets that
have large camera baselines (DL3DV and Objaverse), where
pose estimation is harder to learn with only self-supervision,
RayZer also leverages video interpolation cues together with
pose estimation to perform novel view synthesis.

Thus, the method to further enhance disentanglement of
interpolation and pose estimation would be an important
future direction. In RayZer, using unordered image sets for
training and using continuous video frames for training can
be two extreme cases in the spectrum for learning this disen-
tanglement. In detail, learning on continuous video frames
with using image index positional embeddings strongly en-
courages the camera pose local smoothness to enhance train-
ing performance; while training on unordered image sets
fully discards this prior. Finding a balance between the two
and designing a better method to encourage the camera pose
local smoothness is a promising avenue to solve the structure-
and-motion problem with learning SE(3) camera poses in
the real-world space.

Figure 8. Visual compression of RayZer and ‚Äúoracle‚Äù methods
on DL3DV.

14

GS-LRMLVSMGTRayZer (ours) 
Figure 9. Visual compression with ground-truth novel views on DL3DV. The first row of each sample is the target novel views, and the
second row are images rendered by RayZer.

15

GTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctions 
Figure 10. Visual compression with ground-truth novel views on RealEstate. The first row of each sample is the target novel views, and
the second row are images rendered by RayZer.

16

GTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctions 
Figure 11. Visual compression with ground-truth novel views on Objaverse. The first row of each sample is the target novel views, and
the second row are images rendered by RayZer.

17

GTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctionsGTTargetViewsRayZerPredi-ctions5
2
0
2

y
a
M
1

]
h
p
-
t
n
a
u
q
[

1
v
1
0
7
0
0
.
5
0
5
2
:
v
i
X
r
a

A log-depth in-place quantum Fourier transform that rarely
needs ancillas

Gregory D. Kahanamoku-Meyer1, John Blue1, Thiago Bergamaschi2, Craig Gidney3, and
Isaac L. Chuang1

1Massachusetts Institute of Technology
2University of California at Berkeley
3Google, Inc.

When designing quantum circuits for a given unitary, it can be much cheaper to achieve a good ap-
proximation on most inputs than on all inputs. In this work we formalize this idea, and propose that
such ‚Äúoptimistic quantum circuits‚Äù are often sufficient in the context of larger quantum algorithms.
For the rare algorithm in which a subroutine needs to be a good approximation on all inputs, we
provide a reduction which transforms optimistic circuits into general ones. Applying these ideas, we
build an optimistic circuit for the in-place quantum Fourier transform (QFT). Our circuit has depth
O(log(n/œµ)) for tunable error parameter œµ, uses n total qubits, i.e. no ancillas, is logarithmically local
for input qubits arranged in 1D, and is measurement-free. The circuit‚Äôs error is bounded by œµ on
all input states except an O(œµ)-sized fraction of the Hilbert space. The circuit is also rather simple
and thus may be practically useful. Combined with recent QFT-based fast arithmetic constructions
(arxiv.org/2403.18006), the optimistic QFT yields factoring circuits of nearly linear depth using only
2n + O(n/ log n) total qubits. Applying our reduction technique, we also construct the first approx-
imate QFT to achieve the asymptotically optimal depth of O(log(n/œµ)) with a sublinear number of
ancilla qubits, well-controlled error on all inputs, and no intermediate measurements.

1 Introduction

In classical computing, there exist an abundance of al-
gorithms which perform better in the average case than
in the worst case. Occasional ‚Äúbad‚Äù (inefficient) in-
puts do not substantially detract from the overall per-
formance of an algorithm if the cost can be amortized
across many function calls. Moreover, there exist strate-
gies for avoiding an algorithm‚Äôs worst-case behavior en-
tirely:
it may be possible to detect bad inputs early
and switch to an algorithm which is more performant
on them, or to solve the worst case by building off of
the solution of a related average-case input‚Äîa tech-
nique known as a worst-case to average-case reduction
[Imp95, BT06, Gol11, AGG+24].

However, the application of these techniques to the
design of quantum circuits requires overcoming consid-
erable obstacles. One of the most fundamental is that
we cannot conditionally execute different algorithms de-
pending on the input‚Äîwe may receive a superposition
of ‚Äúgood‚Äù and ‚Äúbad‚Äù inputs, and thus we must follow
every code path, every time! On the other hand, quan-
tum mechanics provides us with unique tools to over-

Gregory D. Kahanamoku-Meyer: gkm@mit.edu

come such obstacles. By linearity a sufficiently good
approximation of a desired unitary can be used in place
of the exact unitary, without meaningfully affecting the
measurement results at the end of a quantum compu-
tation. Typically, this fact has been used to find ap-
proximate circuits whose maximum error over arbitrary
input states is small. Unfortunately, such constructions
cannot leverage the fact that much of the complexity
in a quantum circuit may be due to a small ‚Äúcorner
case‚Äù subspace of possible inputs, that is more difficult
to approximate.

In this work, we explore the design of quantum cir-
cuits which give a good approximation of the desired
unitary only on most, but not all, of the input Hilbert
space. We call these ‚Äúoptimistic quantum circuits.‚Äù1
We propose to use such circuits in two different ways:
either 1) as drop-in replacements for the desired uni-
tary, if it can be shown (or perhaps heuristically as-
sumed) that pathological inputs are unlikely to occur,
or 2) as the core of a worst-to-average case reduction

1We do not use the term ‚Äúaverage case‚Äù for our circuits because
the average case is defined with respect to a distribution over
inputs, which depends on the context in which an algorithm is
being used. We define optimistic quantum circuits in a way that
is context- and even basis-independent (see Definition 2).

1

 
 
 
 
 
 
 
Figure 1: Design of the optimistic QFT. Starting with a ‚Äúblocked‚Äù version of a standard approximate QFT circuit (a,i), we add
an identity of the form QFT‚Ä†QFT (labeled ‚ÄùC‚Äù, yellow border) and then move approximately commuting blocks past each other
to yield the optimistic QFT (a,ii). The blocks approximately commute because applying QFT‚Ä† is equivalent to quantum phase
estimation, which yields a superposition of values numerically close to the input state |Xi‚ü© (b,i). The quantum phase estimation
fails when Xi is near 0 or 2m because the superposition wraps around mod 2m (b,ii).

which yields a provably good approximation even on
worst-case inputs. We give an explicit way of construct-
ing such a reduction.

Applying these ideas, we construct an optimistic cir-
cuit for the quantum Fourier transform (QFT), which
has a set of features that are provably impossible to
achieve simultaneously for a ‚Äúfull‚Äù approximate QFT.
For error parameter œµ and input size n qubits, the cir-
cuit has depth O(log(n/œµ)), uses no ancilla qubits, uses
gates with range at most O(log(n/œµ)) for qubits ar-
ranged in 1D, and uses no intermediate measurements.
This can be compared to previous logarithmic-depth
QFT constructions, which require O(n) [Hal02] or even
O(n log n) [CW00] ancilla qubits and also use long-range
gates; very recently, [BSW25] also presented a clever
new construction which uses measurement and feed-
forward to achieve logarithmic depth with only nearest-
neighbor connectivity and O(n) ancilla qubits. The key
idea behind our construction is that leveraging quan-
tum phase estimation on blocks of qubits allows us to
approximately map the output state on that block back
to the input, which we may then use to control phase
rotations on a neighboring block (Fig 1a). This phase

estimation fails on basis states for which the phase es-
timation ‚Äúwraps around‚Äù modulo 2m, where m is the
block size (Fig 1b);
fortunately, the error from this
wraparound is small on the vast majority of the Hilbert
space and thus the construction yields an optimistic
QFT.

We show that our optimistic QFT can be used with
recent QFT based arithmetic circuits to construct an
optimistic multiplier [KY24], and that this multiplier is
sufficient to implement Shor‚Äôs algorithm for factoring
with high probability of success. We also instantiate
the reduction discussed earlier, and show that a ran-
domized reduction yields an approximate QFT having
low error on arbitrary inputs in depth O(log(n/œµ)) us-
ing n + O(n/ log(n/œµ)) total qubits. A straightforward
purification of the randomized reduction yields a deran-
domized approximate QFT with the same depth and
3n + O(n/ log(n/œµ)) total qubits.

We begin below with the general construction for op-
timistic quantum circuits (Section 2), then specialize
to present an ancilla-free logarithmic-depth optimistic
circuit construction for the QFT (Section 3). This con-
struction provides an optimistic multiplication capabil-

2

QFT‚Ä†(a, i) Linear-depth approximate QFT(b, i) Estimating Xi via QFT‚Ä†Good case:Bad case:(b, ii) Population of QFT‚Ä†(a, ii) Log-depth optimistic QFTQFT‚Ä†{Blocks of m = O(log n) qubits{{{{AABBBBBCCBBC 
ity that reduces resources required for factoring (Sec-
tion 4). An approximate QFT which works on arbitrary
inputs is also given (Section 5) before we conclude with
an outlook (Section 6). Proofs are largely deferred to
the appendices.

2 Optimistic quantum circuits

Intuitively, an optimistic quantum circuit is one in
which the error is small on most basis states (for any
basis). Definitionally, it is more convenient to consider
the average error over all basis states, as that turns out
to be a basis-independent quantity. Thus, we begin by
presenting such a definition and then in Lemma 1 show
that the definition captures the intuition of small error
on all but a few states in any basis.

Definition 1 (Optimistic quantum circuits). Let U be
a unitary operation on a Hilbert space H. Consider a
quantum circuit C which induces a unitary eU on H. C
is an optimistic quantum circuit for U with error bound
œµ if, for any complete set of basis states {|œïi‚ü©} for H,

1
dim H

X

i

| eU |œïi‚ü© ‚àí U |œïi‚ü© |2 < œµ

(1)

This formulation is convenient because it gives us a
clear method for showing that a particular construction
is an optimistic circuit with error œµ: pick a basis, and
show that the average error over states in this basis is
less than œµ. It is less obvious, but true, that this defini-
tion is basis-independent. In order to show basis inde-
pendence more clearly, we present a second equivalent
definition of optimistic quantum circuits:

Definition 2 (Equivalent to Definition 1). For U, eU,
H, and C as in Definition 1, C is an optimistic quantum
circuit for U with error bound œµ if

|| eU ‚àí U||2
F
dim H

‚â§ œµ

(2)

where ||E||2

F = Tr (cid:2)E ‚Ä†E(cid:3) is the Frobenius norm squared.
Two features of Definition 1 are crucial for this basis-
independence to hold: first, that the per-state error is
measured not via fidelity, but instead via length of the
error vector, which captures errors in phase; and second,
that we are concerned with the average error over basis
states, rather than the maximum.

Although the maximum error over basis states is
essentially unbounded, our original intuition for opti-
mistic quantum circuits says that the size of the sub-
space in which these large errors occur must be small.
Indeed, for an optimistic quantum circuit with error
parameter œµ, the number of basis states for which the

error is O(1) is bounded by at most O(œµ) ¬∑ dim H. We
formalize this idea in the following Lemma.

Lemma 1 (Size of large-error subspaces). For U, eU, H,
and C as in Definition 1, consider a subspace Hbad ‚äÜ H
with associated projector Œ†Hbad. Let the error on this
subspace be ¬µ = ||( eU ‚àí U)Œ†Hbad ||2
F / dim Hbad. Then, if
C is an optimistic circuit for U with error parameter œµ,
dim Hbad ‚â§ dim H ¬∑ œµ/¬µ.

Proof. Observe that for any subspace H‚Ä≤ ‚äÜ H, the error
on H (as defined in Definition 1) is bounded from below
by the error on the subspace, scaled by their relative
size:

|| eU ‚àí U||2
F
dim H

‚â•

dim H‚Ä≤
dim H

||( eU ‚àí U)Œ†H‚Ä≤||2
F
dim H‚Ä≤

!

Substituting the definitions of ¬µ and œµ yields the desired
result.

Because in any basis the error is small on all but a few
states, in practice it should almost always be possible
to use optimistic quantum circuits directly in place of
exact ones. In Section 4 we show explicitly that this is
the case for an optimistic multiplication circuit, in the
context of Shor‚Äôs algorithm for factoring. But perhaps
in some cases the input states will be pathologically
concentrated in the large-error subspace; to handle such
cases, we next show a generic reduction by which worst-
case approximate circuits can be constructed from op-
timistic circuits.

2.1 Reduction: worst-case approximate circuits
from optimistic circuits

From Definition 1 (or Lemma 1), it follows that an op-
timistic circuit‚Äôs error is expected to be small when ap-
plied to random input states. In consideration of that
fact, the core idea of our reduction is to apply a random
unitary V before the optimistic circuit, and then apply
ÀÜV ‚Ä† = UV ‚Ä†U ‚Ä† on the other side. The action of V effec-
tively turns every input state into a random state, and
ÀÜV ‚Ä† inverts the randomization. If efficient circuits exist
for both V and ÀÜV ‚Ä†, the entire operation ÀÜV ‚Ä† eUV ‚âà U can
be implemented efficiently.

Applying truly random unitaries (i.e. drawn from the
Haar measure) would be very expensive; the key result
of this subsection is a proof that we really only need V
to be drawn from a unitary 1-design, which is one of the
easiest random unitary distributions to construct. This
fact provides freedom to choose a distribution that is
convenient in practice; for example, in Section 5 we are
able to find a 1-design for the optimistic QFT where
both V and ÀÜV ‚Ä† have logarithmic-depth circuits requir-
ing few ancillas.

3

 
 
We formalize our randomized reduction in the fol-
lowing theorem; later, in Theorem 2, we show that if
the 1-design is a uniform distribution over some set of
unitaries {Vi}, the reduction can be derandomized via
purification.2

Theorem 1 (Randomized reduction). Consider an op-
timistic quantum circuit C with error parameter œµ, and
U, eU, and H defined as in Definition 1. Furthermore
let d be a unitary 1-design on H. Then for any state
|œà‚ü© ‚àà H, ÀÜV ‚Ä† eUV |œà‚ü© is a good approximation of U |œà‚ü©
with high probability over the distribution of V :

h

| ÀÜV ‚Ä† eUV |œà‚ü© ‚àí U |œà‚ü© |2i

‚â§ œµ

E
V ‚àºd

(3)

where ÀÜV = UV U ‚Ä† and EV ‚àºd[¬∑] denotes the expectation
value over unitaries V drawn from d.
Proof. Observe that by definition of ÀÜV , ÀÜV ‚Ä†UV = U.
Letting E = eU ‚àí U to simplify notation this yields
| ÀÜV ‚Ä†EV |œà‚ü© |2i

| ÀÜV ‚Ä† eUV |œà‚ü© ‚àí U |œà‚ü© |2i
h

(4)

h

= E
V ‚àºd

E
V ‚àºd

Observe that ÀÜV ‚Ä† is unitary and thus does not affect
the value of the norm, and can be dropped. Then by
linearity of the expectation operator and the definition
of the norm of a quantum state,
| ÀÜV ‚Ä†EV |œà‚ü© |2i

(cid:2)V ‚Ä†E ‚Ä†EV (cid:3) |œà‚ü© .

(5)

h

= ‚ü®œà| E
V ‚àºd

E
V ‚àºd

(cid:2)V ‚Ä†E ‚Ä†EV (cid:3) =
By definition of a unitary 1-design, EV ‚àºd
I ¬∑ Tr (cid:2)E ‚Ä†E(cid:3) / dim H. Thus using Definition 2 of an opti-
mistic quantum circuit with error parameter œµ we have

h

| ÀÜV ‚Ä† eUV |œà‚ü© ‚àí U |œà‚ü© |2i

= Tr (cid:2)E ‚Ä†E(cid:3) / dim H ‚â§ œµ

E
V ‚àºd

(6)

As alluded to earlier, we may derandomize the re-
duction of Theorem 1 via purification, by encoding the
distribution of unitaries into the quantum state of a
control register. This idea is formalized in the following
theorem.

Theorem 2 (Derandomized reduction). Consider an
optimistic quantum circuit C with error parameter œµ,
and let U, eU, and H defined as in Definition 1. Now
consider a unitary 1-design consisting of the uniform
distribution over a discrete set of k unitaries {Vi} in-
dexed by i, and let V ‚Ä≤ = P
i |i‚ü© ‚ü®i| ‚äó Vi. Then, for ar-
bitrary states |œà‚ü© ‚àà H, the unitary U ‚Ä≤ = ÀÜV ‚Ä≤‚Ä†(I ‚äó eU)V ‚Ä≤
applied to the state 1‚àö
k

i |i‚ü© ‚äó |œà‚ü© has error

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(U ‚Ä≤ ‚àí I ‚äó U)

1
‚àö
k

X

i

|i‚ü© ‚äó |œà‚ü©

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

‚â§ œµ

2For simplicity, we focus on purifying 1-designs over uniform

distributions, as the purification is easily implementable.

4

Proof. We begin by understanding the action of the de-
randomized unitary U ‚Ä≤:
(cid:18) X

X

(cid:19)

‚äó ÀÜV ‚Ä†

i eUVi |œà‚ü©

(7)

‚äó |œà‚ü©

=

U ‚Ä≤

|i‚ü©
‚àö
k

i

|i‚ü©
‚àö
k

i

Which then allows us to expand the error, and relate it
to the randomized case:

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(U ‚Ä≤ ‚àí I ‚äó U)

(cid:18) X

i

|i‚ü©
‚àö
k

‚äó |œà‚ü©

2

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

X

i

‚äó ( ÀÜV ‚Ä†

|i‚ü©
‚àö
i eUVi ‚àí U) |œà‚ü©
k
| ÀÜV ‚Ä† eUV |œà‚ü© ‚àí U |œà‚ü© |2i

h

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

=

‚â§ œµ

= E
V ‚àºd

(8)

(9)

(10)

where in the last line, we leveraged Theorem 1.

We remark that the notion of average error œµ dis-
œµ) in diamond dis-

cussed above, implies an error O(
tance when viewed as a channel.

‚àö

2.2 Warm-up example: optimistic quantum ad-
dition

As a first example of an optimistic quantum circuit,
we give an argument for why a low-depth circuit may
approximate the addition unitary, for most inputs. Let
the addition unitary be

ADD |a‚ü© |b‚ü© |0‚ü© = |a‚ü© |s‚ü© ,

where a and b are n-bit integers, and s = a+b is an n+1
bit integer. The construction employs some common
notation used when describing addition circuits. Denote
the bits of an integer x as x = xn‚àí1xn‚àí2 . . . x1x0, where
x0 is the least-significant bit. The bits of s are then:

c0 = 0

ci+1 = MAJ(ai, bi, ci) for 0 ‚â§ i ‚â§ n ‚àí 1
si = ai ‚äï bi ‚äï ci for 0 ‚â§ i ‚â§ n ‚àí 1
sn = cn

(11)

(12)

(13)

(14)

where the MAJ function equals one if the majority of
its inputs equal one, and zero otherwise. The values ci
are referred to as the carries.

As is well known, equations (11)-(14) show that ad-
dition is non-local, in that we cannot compute the out-
put bit si just by looking at a local neighborhood of
bits around index i. Instead, information is propagated
from one end of the input all the way to the other via
the carry bits. This non-locality is a feature that will
show up again when we examine the QFT in the follow-
ing section. Additionally, this non-locality places con-
straints on the possible circuits we can use to implement

 
 
addition; it is impossible to achieve logarithmic depth
for addition circuits without some kind of long-range
information flow. However, the passing of this informa-
tion can be accomplished in many ways besides direct
use of geometrically long-range gates, including through
classical means, via measurement and feedforward op-
erations [BSW25], such that quantum operations may
remain more local.

The approach we take here is different, and is mo-
tivated by the intuition that the fraction of inputs for
which that information needs to be propagated over a
large distance should be small. Thus, the propagation of
carries can simply be cut off after some number of bits.
A logarithmic-size cutoff yields a logarithmic-depth cir-
cuit that also only uses logarithmically-local gates‚Äîyet
for most inputs, propagating carries a logarithmic dis-
tance is all that is needed.

Because there already exist exact adders that simul-
taneously achieve low qubit count and depth [TK08,
RV25], the benefits of using optimistic circuits in this
simple example are essentially just locality of gates. In
the next section we describe an optimistic construc-
tion for the quantum Fourier transform that yields dra-
matic improvements over non-optimistic constructions
in depth, locality, and qubit count, simultaneously.

3 Ancilla-free logarithmic-depth opti-
mistic QFT

In this section we describe in detail our construction
for the optimistic quantum Fourier transform (QFT).
We begin with some discussion of the structure of the
QFT, and build off of that to define our optimistic QFT
circuit, which we denote gQFT. We prove that our con-
struction is an optimistic circuit with error parameter
œµ; we also explicitly identify the ‚Äúlarge-error‚Äù subspace
in the computational and Fourier bases (see Lemma 1).
The quantum Fourier transform is typically expressed

in terms of Fourier basis states:

|Œ¶x‚ü© ‚â° QFT |x‚ü© =

1
‚àö
2n

2n‚àí1
X

y=0

e2œÄixy/2n

|y‚ü©

(15)

where n is the number of qubits.3 Perhaps surprisingly
given the form of Equation 15, Fourier basis states are
product states; they can also be expressed thus:

|Œ¶x‚ü© =

n‚àí1
O

i=0

(cid:0)|0‚ü© + e2œÄi 0.xixi+1¬∑¬∑¬∑ |1‚ü©(cid:1)

(16)

3In this work we exclusively discuss the QFT modulo 2n.

is the ith bit of x, and 0.xixi+1 ¬∑ ¬∑ ¬∑ =
where xi
2ix/2n mod 1 is a binary fraction [NC11].4 It was ob-
served early in the study of the QFT that an approxi-
mation of |Œ¶x‚ü© with fidelity œµ can be achieved by simply
truncating the binary fraction in each qubit‚Äôs phase to
m = O(log(n/œµ)) bits [Cop02]. This yields a standard
construction for the approximate QFT: iterate through
each qubit, applying a Hadamard gate to qubit i and
then a series of controlled phase rotations from qubits
i + 1 through i + m ‚àí 1. This construction requires
only O(n log(n/œµ)) gates, but has linear depth, because
the controlled phase rotations on each qubit rely on the
following qubits still being in their initial (input) state.
Equation 16 may seem to suggest that information
remains local in the QFT, because each qubit‚Äôs out-
put state only depends on O(log(n/œµ)) nearby qubits.
Perhaps the approximate QFT should thus be achiev-
able via a low-depth circuit of entirely local gates. Re-
latedly, it has recently been shown that the QFT has
small entanglement [CSW23]. However, a simple coun-
terexample shows that information from a small sub-
set of the qubits can affect the entire output. Con-
sider the two states |œï0‚ü© = QFT |000 ¬∑ ¬∑ ¬∑ 000‚ü© and |œï1‚ü© =
QFT |111 ¬∑ ¬∑ ¬∑ 111‚ü©. From Equation 16, it is straightfor-
ward to see that |œï0‚ü© is simply a collection of |+‚ü© states.
Surprisingly, all but the last few qubits of |œï1‚ü© are ex-
ponentially close to the |+‚ü© state as well!
It is just
the last O(log n) qubits that non-negligibly distinguish
|œï0‚ü© from |œï1‚ü©, so QFT‚Ä† must propagate the informa-
tion from these qubits across the entire register to yield
|000 ¬∑ ¬∑ ¬∑ 000‚ü© or |111 ¬∑ ¬∑ ¬∑ 111‚ü©. Thus via a light-cone ar-
gument, it is impossible to construct a circuit for the
quantum Fourier transform that both has low depth
and uses entirely local gates (in 1D). The key to our
optimistic QFT is to recognize that this long-range be-
havior is in some sense atypical. Indeed, we find that
it is possible to construct a low-depth, entirely local
circuit which approximates the QFT well on the vast
majority of basis states; we do so in the following.

Let us divide the n-qubit input register into blocks,
each having m = O(log(n/œµ)) qubits. For a computa-
tional basis state |x‚ü©, we denote the m-bit integer value
on block i as Xi, such that x = P
i 2miXi. Now, we can
write a blockwise version of Equation 16:

"2m‚àí1
X

O

Pn/m
j=i

œâ

#
(Xj /2m(j‚àíi))Yi |Yi‚ü©

|Œ¶x‚ü© =

(17)

i

Yi=0

where œâ = e2œÄi/2m

. The choice of our block size

4It is important to note that with the indexing of Equation 16,
x and y in Equation 15 have different endianness. Because it is
natural for the circuits we discuss, we use that indexing through-
out this work. If needed, the bit reversal unitary on n qubits can
be implemented with a single layer of n/2 SWAP gates.

5

 
m = O(log n/œµ) is convenient because it allows us to
approximate this state to within œµ by simply dropping
all but the first two terms of the sum in the exponent.
Denoting the ith block of |Œ¶x‚ü© as [|Œ¶x‚ü©]i, we have

[|Œ¶x‚ü©]i ‚âà

2m‚àí1
X

Yi=0

œâ(Xi+Xi+1/2m)Yi |Yi‚ü©

(18)

Equation 18 suggests a ‚Äúblockwise‚Äù version of the stan-
dard circuit for computing the approximate QFT, shown
in Figure 1(a,i):
iterate through the blocks, apply-
ing a local m-bit QFT to block i (yielding the state
P
œâXiYi |Yi‚ü©) and then a phase rotation of œâXi+1Yi/2m
between neighboring blocks i and i + 1. This circuit has
linear depth because the operations on block i ‚àí 1 need
access to Xi, so we cannot transform block i of the input
until block i ‚àí 1 is complete.

Yi

i‚ü© having X ‚Ä≤

In order to achieve low depth we must break this
chain of dependencies, and we do so by attempting to
estimate Xi directly from [|Œ¶x‚ü©]i. The study of quan-
tum phase estimation (QPE) suggests that applying a
length-m inverse QFT to [|Œ¶x‚ü©]i would be a promis-
ing strategy: we expect QPE to yield a superposition
|fXi‚ü© whose population is peaked on computational ba-
sis states |X ‚Ä≤
i close to the value of the phase
factor Xi + Xi+1/2m in Equation 18 (see Nielsen &
Chuang, section 5.2.1 [NC11]). Thus, intuitively, we
should be able to approximately apply the phase rota-
tion œâXiYi‚àí1/2m
by using the state |fXi‚ü©, with only a
small phase error. Unfortunately, there is a subtle but
critical problem: the distribution of population in |fXi‚ü©
wraps around modulo 2m [Ral21].5 If Xi is too close to
0 or 2m, a non-negligible portion of the distribution will
be off by roughly 2m and produce a large error in the
phase (see Figure 1(b,ii)). The problem is not with our
phase estimation procedure; it is simply impossible in
general to estimate Xi from only [|Œ¶x‚ü©]i for all x. This
is where we can use the power of optimistic circuits. We
may simply allow some states to have large error, ob-
serving that because the distribution in |X ‚Ä≤
i‚ü© is highly
peaked, for most inputs the error from wraparound will
be small, and thus the average error over basis states
will be small.

Following this intuition, we may construct the op-
timstic QFT circuit, showing in Figure 1(a,ii). We first
compute [|Œ¶x‚ü©]i on blocks with even i, which is straight-
forward to do in parallel because the odd-indexed blocks
remain in their initial state. We then apply QFT‚Ä† to the
even blocks, yielding |fXi‚ü© on those blocks [Fig. 1(b,i)],

5Mathematically, for basis states |X ‚Ä≤

i ‚àí
Xi|2m is small. Here | ¬∑ |2m is the Lee metric: |z|2m = min(z mod
2m, ‚àíz mod 2m) which is the definition of absolute value that is
most sensible in this context.

i‚ü© with high weight, |X ‚Ä≤

i+1Yi/2m

which allows us to compute [|Œ¶x‚ü©]i for odd i. Impor-
tantly, when the phase rotation œâX ‚Ä≤
is a good
approximation of œâXi+1Yi/2m
there is negligible phase
kickback to |fXi‚ü© and the state on the blocks with even i
is approximately unchanged. Thus a final QFT applied
to the even blocks returns them to [|Œ¶x‚ü©]i, yielding the
desired approximation of Œ¶x on the entire register. We
explicitly record the steps of this circuit in Algorithm 1,
and in the following theorem we formalize that it is an
optimistic circuit for the QFT with error parameter œµ.

Theorem 3 (Optimistic quantum Fourier transform).
The circuit defined in Algorithm 1 is an optimistic cir-
cuit with error parameter œµ for QFT2n .

Proof. See Appendix A.

Algorithm 1: The optimistic quantum Fourier
transform.
Input: Quantum register with n qubits; error

parameter œµ

Let m = O(log n/œµ) be the block size.

1. Apply QFT2m to the even-indexed blocks.

2. Apply the phase rotation exp (2œÄiXi+1Yi/22m)

for all even i.

3. Apply QFT2m to the odd-indexed blocks, and

apply QFT‚Ä†

2m to the even-indexed blocks.

4. Apply the phase rotation exp (2œÄiXi+1Yi/22m)

for all odd i.

5. Apply QFT2m to the even-indexed blocks.

Observe the following interesting fact: we noted
earlier that a logarithmically-local, logarithmic depth
approximate QFT on all basis states is not possi-
ble, by light cone argument.
Interestingly, the states
|000 ¬∑ ¬∑ ¬∑ 000‚ü© and |111 ¬∑ ¬∑ ¬∑ 111‚ü© discussed earlier as exam-
ples in which the QFT transmits information at long
range are precisely the type of states in which the
phase estimation trick breaks down! One may won-
der, whether this long-range behavior is somehow im-
portant for the QFT‚Äôs power as a building block for
quantum algorithms. In the following section, we show
that this is not the case, by using the optimistic QFT to
construct an optimistic circuit for integer multiplication
and showing that that circuit is sufficient for quantum
integer factorization via Shor‚Äôs algorithm.

Before moving on, we briefly discuss the cost of this
optimistic QFT circuit. The circuit has five layers,
each of which has depth O(log(n/œµ)) using the standard
linear-depth exact construction for the size-m QFT

6

 
blocks [NC11]. Each layer can be implemented with
gates of range at most O(log(n/œµ) for qubits arranged
in 1D. The algorithm requires no ancilla qubits, and no
measurements.

4 Optimistic multiplication and factor-
ing

Integer multiplication is a core operation in large array
of quantum algorithms. In this section, we consider the
setting of Shor‚Äôs algorithm for factoring [Sho97] using
in-place modular multiplication of a quantum register
by a classical constant c, where c is coprime to the mod-
ulus N :

|y‚ü© ‚Üí |cy mod N ‚ü© .

(19)

It has been shown that this operation can be performed
via four QFTs plus some other operations which use
2n + o(n) total qubits and have total depth O(nŒµ) for a
tunable Œµ > 0 (note that this is distinct from the error
parameter œµ discussed elsewhere in this work, and the
constant factors hidden by the asymptotic notation be-
come larger as Œµ becomes closer to zero) [KY24]. Sur-
prisingly, the bottleneck is the QFTs: all previously-
known QFT constructions would dominate either the
depth or qubit count of this multiplier. Our QFT con-
structions completely address this bottleneck, yielding a
multiplier with depth O(nŒµ) and 2n + o(n) total qubits.
We note that this can be achieved straightforwardly by
using our randomized approximate QFT from Section 5,
yielding a factoring algorithm with depth O(n1+Œµ) yet
only 2n+o(n) total qubits, by far the best known depth
for a factoring circuit using so few qubits. However, it
turns out that the reduction to handle worst-case inputs
is not even necessary: as an example of the applicabil-
ity of optimistic circuits, we show in Appendix C that
the optimistic QFT can be used directly in place of the
QFTs in the multiplications of the factoring circuit, and
the probability of successfully finding the factors will re-
main high.

Note that our QFT should not be used for the final
QFT in the quantum phase estimation procedure: only
for the multiplications. The quantum phase estimation
algorithm only uses one explicit QFT, and linear depth
is acceptable for that singular use. By using the stan-
dard QFT construction one qubit can be recycled for
the entire |x‚ü© register, and such recycling is crucial for
achieving qubit count 2n + o(n) [Bea03].

5 Approximate QFT via reduction

We now apply the reduction described in Section 2.1
to the optimistic QFT, yielding approximate QFT cir-

cuits that have small error on arbitrary inputs. We
present two variations of the reduction, both main-
taining depth O(log(n/œµ)) which is asymptotically op-
timal (for measurement-free circuits [CW00]): a ran-
domized construction using a sublinear number of an-
cillas, and a derandomized one using 3n + o(n) ancil-
las. The randomized construction seems to be the first
approximate QFT to achieve this depth using a sub-
linear number of ancilla qubits; the purified construc-
tion similarly seems to be the first deterministic cir-
cuit (no randomization, nor measurements and feed-
forward) to achieve this depth using 3n + o(n) total
qubits [CW00, Hal02, BSW25].

For the 1-design d in the reduction (see Section 2.1),
we uniformly sample two random integers r1 and r2 be-
2n Z r2
tween 0 and 2n‚àí1, and then choose V (r1, r2) = X r1
2n ,
where X2n and Z2n are the Weyl-Heisenberg general-
ization of the X and Z Pauli operators to a Hilbert
space of dimension 2n. Mathematically, d is the uniform
distribution over the Weyl-Heisenberg group, which is
well known to form a 1-design [GSW21]. For intu-
ition, X r1
2n |x‚ü© = |(x + r1) mod 2n‚ü© corresponds to the
addition of the integer r1 into the register, and Z r2
2n
corresponds to a ‚Äúphase gradient‚Äù rotation Z r2
2n |x‚ü© =
e2œÄir2x/2n
|x‚ü©. Altogether, the action of V (r1, r2) on a
computational basis state is

V (r1, r2) |x‚ü© = e2œÄir2x/2n

|(x + r1) mod 2n‚ü©

(20)

In the case of the optimistic QFT, this choice of 1-design
is particularly nice because the unitary ÀÜV ‚Ä† = QFT ¬∑ V ‚Ä† ¬∑
QFT‚Ä† (see Section 2.1) is essentially the same operator,
with the arguments exchanged:

ÀÜV ‚Ä†(r1, r2) = V (r2, ‚àír1)

(21)

(This is a direct result of the fact that QFT is the Weyl-
Heisenberg generalization of the Hadamard gate). Ap-
plying Theorem 1, the optimistic QFT together with
this 1-design yields a randomized approximate QFT
with expected error œµ for any input state, with the ex-
pectation taken over the randomness in the 1-design.

In the abstract circuit model, the phase rotation Z r2
2n
can be implemented via one layer of single-qubit phase
rotations. Thus only the implementation of the inte-
ger addition X r1
2n may contribute non-negligibly to the
overall circuit cost. We formalize this discussion in the
following theorem:

Theorem 4 (The Randomized Approximate QFT).
There exists a randomized circuit for the approximate
quantum Fourier transform on n qubits with gate cost
O(n log(n/œµ)), depth O(log(n/œµ)), and ancilla qubit
count O(n/ log(n/œµ)), where œµ is the expectation value
of the error on an arbitrary input state.

7

 
Proof. We instantiate the circuit described in the pre-
vious paragraphs, using the classical-quantum adder
of [TK08] with block size m = O(log(n/œµ)) for the inte-
ger addition X r1
2n .

Finally, we may apply the purification of Theorem 2.
The qubit cost of the purification is large enough that
the cost of the adders in the application of V can be
made negligible, yielding theorem 5. We first give a
lemma about implementing controlled phase gradients.

Lemma 2 (Cost of approximate controlled phase gradi-
ents). There is a quantum circuit of depth O(log(n/Œ¥))
that approximates the operation

|x‚ü© |z‚ü© 7‚Üí e2œÄixz/2n

|x‚ü© |z‚ü©

to error Œ¥ in the operator norm.

Proof. See Appendix D.

Theorem 5 (The Unitary Approximate QFT). There
exists a unitary circuit for the approximate quantum
Fourier transform on n qubits with error œµ, with gate
count O(n log(n/œµ)), depth O(log(n/œµ)), and total qubit
count 3n + O(n/ log n).

Proof. This follows from implementing the controlled
Weyl-Heisenberg operators using Lemma 2 for the phase
gradient, and the quantum-quantum adder of [TTK10]
for the addition. We defer a more detailed description
to Appendix D.

Before concluding, we note an interesting connec-
tion to existing work on verification of the approximate
quantum Fourier transform.
It was previously shown
that given black-box access to an approximate quantum
Fourier transform with unknown error characteristics, it
is possible to verify that the black box is sufficient for
quantum phase estimation with only a polynomial num-
ber of calls to the black box [LdW22]. The strategy is
to first verify that the error is well behaved on random
inputs, and then when doing phase estimation apply a
phase rotation Z r
2m for some random r, and then clas-
sically subtract the value r from the phase estimation
result. This corresponds to a special case of our reduc-
tion: if the output state will be measured immediately
after the application of the QFT, the application of ÀÜV ‚Ä†
can be replaced with a classical subtraction. Further-
more, the X r1
2m part of V only results in a phase on
the output states, which doesn‚Äôt affect the results of
the subsequent measurement, and thus in this context
X r1
2m does not need to be performed, leaving only Z r2
2m
as in the verification protocol. The fact that the error
of the black box is well-behaved on random inputs can
be thought of as verifying that the black box performs
an optimistic QFT; indeed, our optimistic QFT circuit
can be thought of as instantiating exactly such a QFT.

6 Discussion and outlook

Standard techniques from classical computing do not
always map well onto quantum computing. This is
true for classical amortized algorithms, which have bet-
ter average- than worst-case complexity but no obvious
mapping to quantum circuits. On the other hand, quan-
tum computing provides us with new and different ways
of solving algorithmic problems, which we can leverage
to get around such obstables.
In this work we intro-
duce a new way of designing fast quantum circuits, via
the use of circuits that approximate a desired unitary
well on most, but not all, input states. We demonstrate
the power of our technique through the construction of a
circuit for the approximate quantum Fourier transform.
Conceptually, we build on a long history of insight
[MU05, MR95] gained from randomized algorithms
for classical computers. Randomization is now being
recognized as a powerful concept for quantum algo-
rithms as well, including acceleration of quantum sim-
ulation [Cam17, Cam19, NBAG24, COS19, ZZS+22,
CB24, CHKT20] and even quantum phase estimation
[LdW22], quantum signal processing, and quantum sin-
gular value transformations [MR25]. The potential for
randomized quantum algorithms is especially intriguing
because coherent errors can add up differently from in-
coherent ones, and crossing back and forth across quan-
tum and classical boundaries can open new opportu-
nities for algorithmic cost reductions. Mathematically,
it is useful to keep in mind regarding quantum states
and operators that the expected error of an approximate
unitary applied to a random state is proportional to the
Frobenius norm of the difference. Optimistic quantum
circuits are one direction in which to utilize these ideas.
Considering the wealth of classical algorithms having
better average- than worst-case complexity, we believe
that there may be a wide range of quantum circuits
that can be optimized with the technique of optimistic
quantum circuits. One case in which its application
may be particularly straightforward is in the design of
‚Äúsemiclassical‚Äù circuits, in which a classical function is
applied to a superposition of inputs. One example is the
optimistic adder of Section 2.2. Another example is the
computation of modular inverses, which are commonly
used in quantum algorithms for cryptography and num-
ber theory problems [PZ03, RNSL17, HJN+20]. The
standard classical algorithms for this problem (the Eu-
clidean algorithm and binary GCD) have the frustrating
property that their runtime depends on the inputs in an
unpredictable way. Optimistic quantum circuits may
provide a way of achieving good performance for these
algorithms, by simply accepting an incorrect answer on
the inputs on which the algorithm does not terminate
quickly!

8

 
A number of open questions stem from our optimistic
QFT construction as well. In this work we do not ex-
tensively explore the circuit‚Äôs practical implementation;
there are many circuit optimizations that can be ap-
plied, such as using phase gradient states to implement
the local QFTs on blocks of m qubits [KSV02, Gid16,
Gid18, NSM20]. The parameters can also be tuned, for
example the block size could differ between the odd and
even blocks because they contribute to the error in dif-
ferent ways. It would also be interesting to explore how
the locality of the optimistic QFT circuit can be used
to reduce routing cost in a practical factoring circuit.
Finally, an exciting direction for future research is the
study of how optimistic circuits can inform our under-
standing of the fundamental characteristics of certain
unitaries, and vice versa. For example, the existence
of a logarithmic-depth, logarithmically-local optimistic
QFT is intimately related to the fact that the exact
QFT has low entanglement [CSW23]; it would be inter-
esting to explore whether optimistic circuits can teach
us similar lessons about other unitaries.

References

Contributions and acknowledgements

GKM created the optimistic QFT circuit; CG pointed
out all log-depth approximations of the QFT using only
local gates must fail badly on certain inputs. All au-
thors contributed ideas to the construction of the worst-
to-average-case reduction techniques; JB and TB de-
vised the proofs of the error bounds for the reductions.
GKM wrote the first draft of the manuscript; all au-
thors contributed to editing, improving, and polishing
the manuscript.

GKM and ILC were supported in part by the Co-
design Center for Quantum Advantage, funded by the
Department of Energy as part of the National Quantum
Initiative. GKM also acknowledges support in part by
the generosity of Google. JB acknowledges support of
the Doc Bedard Fellowship of the Laboratory for Phys-
ical Sciences. This work was completed in part while
TB was a student researcher at Google. We thank John
Martyn for useful discussions regarding error bounds for
randomized circuits.

[AGG+24] Vahid R. Asadi, Alexander Golovnev, Tom Gur, Igor Shinkar, and Sathyawageeswar Subramanian.
Quantum Worst-Case to Average-Case Reductions for All Linear Problems. In Proceedings of the 2024
Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Proceedings, pages 2535‚Äì2567. Society
for Industrial and Applied Mathematics, January 2024.

[Bea03]

Stephane Beauregard. Circuit for Shor‚Äôs algorithm using 2n+3 qubits. Quantum Information & Com-
putation, 3(2):175‚Äì185, March 2003.

[BSW25] Elisa Baumer, David Sutter, and Stefan Woerner. Approximate quantum fourier transform in logarithmic

depth on a line. 2025.

[BT06]

[Cam17]

[Cam19]

[CB24]

Andrej Bogdanov and Luca Trevisan. Average-Case Complexity. Found. Trends Theor. Comput. Sci.,
2(1):1‚Äì106, October 2006.

Earl Campbell. Shorter gate sequences for quantum computing by mixing unitaries. Phys. Rev. A,
95:042306, Apr 2017.

Earl Campbell. Random compiler for fast hamiltonian simulation. Phys. Rev. Lett., 123:070503, Aug
2019.

Chi-Fang Chen and Fernando G. S. L. BrandÀúao. Average-case speedup for product formulas. Commu-
nications in Mathematical Physics, 405(2), February 2024.

[CHKT20] Chi-Fang Chen, Hsin-Yuan Huang, Richard Kueng, and Joel A. Tropp. Concentration for random

product formulas. PRX Quantum, 2020.

[Cop02]

D. Coppersmith. An approximate Fourier transform useful in quantum factoring, January 2002.

[COS19] Andrew M. Childs, Aaron Ostrander, and Yuan Su. Faster quantum simulation by randomization.

Quantum, 3:182, September 2019.

[CSW23]

Jielun Chen, E.M. Stoudenmire, and Steven R. White. Quantum Fourier Transform Has Small Entan-
glement. PRX Quantum, 4(4):040318, October 2023.

[CW00]

R. Cleve and J. Watrous. Fast parallel circuits for the quantum Fourier transform. In Proceedings 41st
Annual Symposium on Foundations of Computer Science, pages 526‚Äì536, November 2000.

9

 
[Gid16]

Craig Gidney. Turning Gradients into Additions into QFTs. https://algassert.com/post/1620, July
2016.

[Gid18]

Craig Gidney. Halving the cost of quantum addition. Quantum, 2:74, June 2018.

[Gol11]

Oded Goldreich. Notes on Levin‚Äôs Theory of Average-Case Complexity.
In Oded Goldreich, editor,
Studies in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and Com-
putation: In Collaboration with Lidor Avigad, Mihir Bellare, Zvika Brakerski, Shafi Goldwasser, Shai
Halevi, Tali Kaufman, Leonid Levin, Noam Nisan, Dana Ron, Madhu Sudan, Luca Trevisan, Salil
Vadhan, Avi Wigderson, David Zuckerman, pages 233‚Äì247. Springer, Berlin, Heidelberg, 2011.

[GSW21] Matthew A. Graydon, Joshua Skanes-Norman, and Joel J. Wallman. Clifford groups are not always

2-designs, August 2021.

[Hal02]

Lisa Hales. The quantum Fourier transform and extensions of the Abelian hidden subgroup problem.
arXiv:quant-ph/0212002, 2002.

[HJN+20] Thomas H¬®aner, Samuel Jaques, Michael Naehrig, Martin Roetteler, and Mathias Soeken.

Improved
Quantum Circuits for Elliptic Curve Discrete Logarithms. In Jintai Ding and Jean-Pierre Tillich, editors,
Post-Quantum Cryptography, Lecture Notes in Computer Science, pages 425‚Äì444, Cham, 2020. Springer
International Publishing.

[Imp95]

R. Impagliazzo. A personal view of average-case complexity. In Proceedings of Structure in Complexity
Theory. Tenth Annual IEEE Conference, pages 134‚Äì147, June 1995.

[KSV02] A. Kitaev, A. Shen, and M. Vyalyi. Classical and Quantum Computation, volume 47 of Graduate Studies

in Mathematics. American Mathematical Society, May 2002.

[KY24]

Gregory D. Kahanamoku-Meyer and Norman Y. Yao. Fast quantum integer multiplication with zero
ancillas, March 2024.

[LdW22] Noah Linden and Ronald de Wolf. Average-Case Verification of the Quantum Fourier Transform Enables

Worst-Case Phase Estimation. Quantum, 6:872, December 2022.

[MR95]

Rajeev Motwani and Prabhakar Raghavan. Randomized Algorithms. Cambridge University Press, 1995.

[MR25]

John M. Martyn and Patrick Rall. Halving the cost of quantum algorithms with randomization. npj
Quantum Information, 11(47), 2025.

[MU05] Michael Mitzenmacher and Eli Upfal. Probability and Computing: Randomized Algorithms and Proba-

bilistic Analysis. Cambridge University Press, 2005.

[NBAG24] Kouhei Nakaji, Mohsen Bagherimehrab, and Al¬¥an Aspuru-Guzik. High-order randomized compiler for

hamiltonian simulation. PRX Quantum, 5:020330, May 2024.

[NC11]

Michael A. Nielsen and Isaac L. Chuang. Quantum Computation and Quantum Information: 10th
Anniversary Edition. Cambridge University Press, USA, 10th anniversary ed edition, 2011.

[NSM20] Yunseong Nam, Yuan Su, and Dmitri Maslov. Approximate quantum Fourier transform with O ( n log(

n )) T gates. npj Quantum Information, 6(1):1‚Äì6, March 2020.

[PZ03]

[Ral21]

John Proos and Christof Zalka. Shor‚Äôs discrete logarithm quantum algorithm for elliptic curves. Quantum
Info. Comput., 3(4):317‚Äì344, July 2003.

Patrick Rall. Faster Coherent Quantum Algorithms for Phase, Energy, and Amplitude Estimation.
Quantum, 5:566, October 2021.

[RNSL17] Martin Roetteler, Michael Naehrig, Krysta M. Svore, and Kristin Lauter. Quantum Resource Estimates
for Computing Elliptic Curve Discrete Logarithms. In Tsuyoshi Takagi and Thomas Peyrin, editors,
Advances in Cryptology ‚Äì ASIACRYPT 2017, pages 241‚Äì270, Cham, 2017. Springer International Pub-
lishing.

[RV25]

[Sho97]

Maxime Remaud and Vivien Vandaele. Ancilla-free Quantum Adder with Sublinear Depth, January
2025.

Peter W. Shor. Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a
Quantum Computer. SIAM Journal on Computing, 26(5):1484‚Äì1509, October 1997.

10

 
[TK08]

Yasuhiro Takahashi and Noboru Kunihiro. A fast quantum circuit for addition with few qubits. Quantum
Info. Comput., 8(6):636‚Äì649, July 2008.

[TTK10] Yasuhiro Takahashi, Seiichiro Tani, and Noboru Kunihiro. Quantum addition circuits and unbounded

fan-out. Quantum Info. Comput., 10(9):872‚Äì890, September 2010.

[ZZS+22] Qi Zhao, You Zhou, Alexander F. Shaw, Tongyang Li, and Andrew M. Childs. Hamiltonian Simulation

with Random Inputs. Physical Review Letters, 129(27):270502, December 2022.

11

 
Appendix

A Proof of optimistic QFT

Here we provide a proof of Theorem 3, restated here:

Theorem 3 (Optimistic quantum Fourier transform). The circuit defined in Algorithm 1 is an optimistic circuit
with error parameter œµ for QFT2n .

Proof. Following Definition 2, we desire to show that

1
2n || gQFT ‚àí QFT||2

F ‚â§ œµ

(22)

Consider the linear-depth approximate QFT on block size m described in Appendix B. By Lemma 4, it induces

a unitary QFT‚Ä≤ for which

1
2n ||QFT‚Ä≤ ‚àí QFT||2

F ‚â§

4œÄ2
3

¬∑

l n
m

m

¬∑

1
2m

(23)

Thus we proceed by bounding || gQFT ‚àí QFT‚Ä≤||2
F , which we may use to prove the theorem via the triangle inequality.
In particular, we show that the linear-depth approximate QFT can be transformed into the optimistic QFT by
commuting certain gates past each other, and then in Lemma 3 show that doing so maintains a small Frobenius
norm.

The transformation is depicted graphically in Figure 1. Starting with the linear-depth approximate QFT (see
Appendix B), we first add a resolution of the identity of the form QFT ¬∑ QFT‚Ä† to the end of the circuit on each
even-indexed block. Now, on every even-indexed block i is the sequence of gates depicted in Figure 2(a): a phase
rotation between blocks i ‚àí 1 and i, QFT on block i, a phase rotation between blocks i and i + 1, and finally QFT‚Ä†
on block i. We denote this block of four gates as W . The key transformation is to move the first phase rotation
past the other gates to the end of the sequence, producing a new sequence W ‚Ä≤ (Fig. 2(b)).

Let gQFTj be the ‚Äúintermediate‚Äù circuit in which j of the blocks W in the linear-depth QFT have been replaced

with W ‚Ä≤. Then Lemma 3 implies that

Then, by triangle inequality, making all ‚åàn/m‚åâ replacements yields an error

1
2n || gQFTi ‚àí gQFTi‚àí1||2

F ‚â§

‚Ñ¶(m)
2m

1
2n || gQFT ‚àí QFT‚Ä≤||2

F ‚â§

n2
m2 ¬∑

‚Ñ¶(m)
2m =

‚Ñ¶(n2)
m2m

Letting m = log(n2/œµ) = O(log(n/œµ)) yields

which is what we desired to show.

1
2n || gQFT ‚àí QFT‚Ä≤||2

F ‚â§ œµ

(24)

(25)

(26)

Lemma 3 (Optimistic commutation of phase gates). For the operations W and W ‚Ä≤ depicted in Figure 2, applied
3m qubits, we have

||W ‚àí W ‚Ä≤||2

F ‚â§ 22m ¬∑ ‚Ñ¶(m)

Proof. First, as per Definitions 1 and 2, we have

||W ‚àí W ‚Ä≤||2

F =

2m‚àí1
X

Yi‚àí1,Xi,Xi+1

|(W ‚àí W ‚Ä≤)(|Yi‚àí1‚ü© ‚äó |Xi‚ü© ‚äó |Xi+1‚ü©)|2

12

(27)

(28)

 
Figure 2: The low-level ‚Äúoptimistic commutation‚Äù used to convert a linear-depth approximate QFT into the optimistic QFT (see
Figure 1). This transformation is performed on every even-indexed block of m qubits; by Lemma 3, the error (measured in Frobenius
norm) of making this replacement is small. Here, œâ = exp (2œÄi/2m) and X and Y are the integer values of the computational
basis states of the two blocks to which the gate is applied.

where Yi‚àí1, Xi, Xi+1 are each m-bit integers. (We denote the first (top) block of qubits in Figure 2 as block i ‚àí 1,
the middle as i, and the last (bottom) as i + 1 in reference to the larger context of Theorem 3). Noting that both
W and W ‚Ä≤ only apply phase rotations proportional to the values of Yi‚àí1 and Xi+1, we can rewrite the above sum
in terms of new unitaries W Yi‚àí1,Xi+1 and W
which only operate on block i, and the values of Yi‚àí1 and
Xi+1 parametrize the unitaries:

‚Ä≤
Yi‚àí1,Xi+1

||W ‚àí W ‚Ä≤||2

F =

2m‚àí1
X

2m‚àí1
X

Xi

Yi‚àí1,Xi+1

|(W Yi‚àí1,Xi+1 ‚àí W

‚Ä≤
Yi‚àí1,Xi+1

) |Xi‚ü© |2

Thus we now bound |(W Yi‚àí1,Xi+1 ‚àí W
By inspection of Figure 2, we have

‚Ä≤
Yi‚àí1,Xi+1

) |Xi‚ü© |2 as a function of Xi.

W Yi‚àí1,Xi+1 |Xi‚ü© = œâXiYi‚àí1/2m

|fXi‚ü©

(29)

(30)

where |fXi‚ü© = P Œ±X ‚Ä≤
|Xi‚ü© (see Figure 1(b)). This notation provides a straightforward way of expressing the action of W

i‚ü© is the result of applying QFT, then the second phase rotation œâXi+1Yi/2m

|X ‚Ä≤

i

, then QFT‚Ä† to
on |Xi‚ü©:

‚Ä≤
Yi‚àí1,Xi+1

W

‚Ä≤
Yi‚àí1,Xi+1

|Xi‚ü© = œâXiYi‚àí1/2m X

(œâYi‚àí1/2m

)(X ‚Ä≤

i‚àíXi)Œ±X ‚Ä≤

i

|X ‚Ä≤

i‚ü© .

(31)

We may thus write:

|(W Yi‚àí1,Xi+1 ‚àí W

‚Ä≤
Yi‚àí1,Xi+1

) |Xi‚ü© |2 =

X ‚Ä≤
i

2m‚àí1
X

X ‚Ä≤
i

|(1 ‚àí (œâYi‚àí1/2m

)(X ‚Ä≤

i‚àíXi))|2|Œ±X ‚Ä≤

i

|2.

(32)

We proceed by separating this sum into two parts: the ‚Äúclose‚Äù part in which |X ‚Ä≤
and thus X ‚Ä≤
quantum phase estimation allows us to bound the size of both parts of this sum.

i = min (Xi, 2m ‚àí Xi)
i has not wrapped around mod2m; and the ‚Äúfar‚Äù part in which it has wrapped around. The study of

i ‚àí Xi| < X ‚àó

For the close part, when |Œ±X ‚Ä≤

i

|2 is large, we are guaranteed that |X ‚Ä≤

i ‚àí Xi < 2m ‚àí 1. This implies that the phase difference |(1 ‚àí (œâYi‚àí1/2m

0 < X ‚Ä≤
this idea quantitative. Using the fact that |1 ‚àí eiŒ¥| ‚â§ |Œ¥|, we have

i ‚àí Xi| is (relatively) small because, by design,
i‚àíXi))|2 is small. We now make

)(X ‚Ä≤

|(1 ‚àí (œâYi‚àí1/2m

)(X ‚Ä≤

i‚àíXi))|2 ‚â§ |2œÄYi‚àí1‚àÜ/22m|2 =

4œÄ2Y 2
24m ‚àÜ2

i‚àí1

(33)

where ‚àÜ = X ‚Ä≤
[NC11]), we have that

i ‚àí Xi. Meanwhile, from Equation 18 and quantum phase estimation (see Eq. 5.29 of Sec. 5.2.1 of

|Œ±‚àÜ+Xi| ‚â§ min

(cid:18)

1
2|‚àÜ ‚àí Xi+1/2m|

, 1

(cid:19)

13

(34)

WW' 
This implies that for the ‚Äúclose‚Äù part of the sum, we have:

X

|(1 ‚àí (œâYi‚àí1/2m

X ‚Ä≤
i
i‚àíXi|<X ‚àó

i

|X ‚Ä≤

)(X ‚Ä≤

i‚àíXi))|2|Œ±X ‚Ä≤

i

|2 <

4œÄ2Y 2
i‚àí1
24m+2

Ô£Æ

Ô£∞

‚àí1
X

‚àÜ=‚àíX ‚àó
i

‚àÜ2
‚àÜ2 + 2 +

‚àÜ2
(‚àÜ ‚àí 1)2

Ô£π

Ô£ª

X ‚àó
iX

‚àÜ=2

Noting the following three facts:

X ‚àó
iX

‚àÜ=2

‚àÜ2
(‚àÜ ‚àí 1)2 =

X ‚àó
i ‚àí1
X

‚àÜ=1

(‚àÜ + 1)2
‚àÜ2

‚àû
X

‚àÜ=1

1/‚àÜ2 < 2

X ‚àó
iX

‚àÜ=1

1/‚àÜ < ln X ‚àó

i + 1 < O(m)

we have

X

|(1 ‚àí (œâYi‚àí1/2m

X ‚Ä≤
i
i‚àíXi|<X ‚àó

i

|X ‚Ä≤

)(X ‚Ä≤

i‚àíXi))|2|Œ±X ‚Ä≤

i

|2 <

4œÄ2Y 2
24m+2 (2X ‚àó

i‚àí1

i + O(n))

(35)

(36)

(37)

(38)

(39)

The ‚Äúfar‚Äù part of the sum is simpler to evaluate, because we only have a very loose bound on |X ‚Ä≤

i‚àíXi))| ‚â§ 2œÄYi‚àí1/2m. This is balanced by the fact that the |Œ±X ‚Ä≤

i ‚àí Xi| < 2m
| are generally small.

i

and thus |(1 ‚àí (œâYi‚àí1/2m
We have (see [NC11] Section 5.2.1):

)(X ‚Ä≤

X

‚àÜ>X ‚àó
i

|Œ±‚àÜ+Xi |2 <

1
2(X ‚àó
i ‚àí 1)

This yields

X

|(1 ‚àí (œâYi‚àí1/2m

X ‚Ä≤
i
i‚àíXi|‚â•X ‚àó

i

|X ‚Ä≤

)(X ‚Ä≤

i‚àíXi))|2|Œ±X ‚Ä≤

i

|2 <

4œÄ2Y 2
22m

i‚àí1

1
2(X ‚àó
i ‚àí 1)

Now combining Eqs. 32, 39, and 41, we have

|(W Yi‚àí1,Xi+1 ‚àí W

‚Ä≤
Yi‚àí1,Xi+1

) |Xi‚ü© |2 <

2œÄ2Y 2
22m

i‚àí1

(cid:18) X ‚àó
i

22m +

(cid:19)

1
X ‚àó
i ‚àí 1

Finally using this expression in Equation 29, and again using Equation 38, we have

||W ‚àí W ‚Ä≤||2

F = 2œÄ2 ¬∑ 2m ¬∑

2m‚àí1
X

Yi‚àí1

Y 2
i‚àí1
22m

2m‚àí1
X

Xi

(cid:18) X ‚àó
i

22m +

(cid:19)

1
X ‚àó
i ‚àí 1

< 22m ¬∑ ‚Ñ¶(m)

(40)

(41)

(42)

(43)

B Linear-depth approximate QFT

In the construction of the optimistic QFT, we reference an approximate QFT construction in which operations are
applied on blocks of qubits. In Algorithm 2 we record this construction explicitly, and in Lemma 4 we bound its
error.

Lemma 4 (Frobenius error of linear-depth approximate QFT). Let QFT‚Ä≤ be the unitary induced by Algorithm 2
with block size m, and let QFT be the exact QFT. Then

1
2n ||QFT‚Ä≤ ‚àí QFT||2

F ‚â§

4œÄ2
3

¬∑

l n
m

m

¬∑

1
2m

(44)

(cf. Definition 2).

14

 
Algorithm 2: The approximate quantum Fourier transform.

Input: Quantum register with n qubits; block size m

Let imax = ‚åàn/m‚åâ. For i from 0 to imax ‚àí 1:

1. Apply QFT2m to block i.

2. Apply the phase rotation exp (2œÄiXi+1Yi/22m), where Xi+1 and Yi are the values of blocks i + 1 and i

respectively as integers in the computational basis.

Proof. First, note that

||QFT‚Ä≤ ‚àí QFT||2

F =

2n‚àí1
X

x=0

|QFT‚Ä≤ |x‚ü© ‚àí QFT |x‚ü© |2

(45)

for computational basis states |x‚ü© (see Def. 1). The result of applying Algorithm 2 to such a state |x‚ü© is given in
Equation 18, reproduced (in slightly different form) here:

QFT‚Ä≤ |x‚ü© =

imaxO

2m‚àí1
X

i=0

Yi=0

œâ(Xi+Xi+1/2m)Yi |Yi‚ü©

where Xi is the ith block of x and œâ = exp 2œÄi/2m. The action of the exact QFT is

QFT |x‚ü© =

imaxO

2m‚àí1
X

i=0

Yi=0

Pimax
j=i

œâ

(Xj /2m(j‚àíi))Yi |Yi‚ü©

Thus we have

||QFT‚Ä≤ ‚àí QFT||2

F =

X

X

2m‚àí1
X

x

i

Yi=0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

œâ(Xi+Xi+1/2m)Yi ‚àí œâ

Pn/m
j=i

(Xj /2m(j‚àíi))Yi

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Using the definition œâ = e2œÄi/2m

and the fact that |eiŒ± ‚àí eiŒ≤| ‚â§ |Œ± ‚àí Œ≤|, we have

||QFT‚Ä≤ ‚àí QFT||2

F ‚â§

X

X

2m‚àí1
X

x

i

Yi=0

Ô£Æ

Ô£∞

2œÄ
2m

n/m
X

j=i+2

Ô£π
2

(Xj/2m(j‚àíi))Yi

Ô£ª

The Xj are all at most 2m ‚àí 1, so we can bound Pn/m

j=i+2 Xj/2m(j‚àíi) ‚â§ 1/2m, yielding

||QFT‚Ä≤ ‚àí QFT||2

F ‚â§

X

X

2m‚àí1
X

x

i

Yi=0

(cid:21)2

(cid:20) 2œÄ
22m Yi

‚â§ 2n ¬∑ imax ¬∑

4œÄ2
24m ¬∑

2 ¬∑ 23m
6

or, finally,

which is what we desired to show.

1
2n ||QFT‚Ä≤ ‚àí QFT||2

F ‚â§

4œÄ2
3

¬∑

l n
m

m

¬∑

1
2m

(46)

(47)

(48)

(49)

(50)

(51)

C Proof of factoring via optimistic QFT

Here we show that using optimistic QFTs to build an optimistic multiplier, as described in Section 4, is sufficient
for factoring with high probability.

15

 
Theorem 6 (Factoring with the optimistic multiplier). Consider the modular exponentiation circuit described
in [KY24], Appendix E; which implements the unitary U |x‚ü© |r‚ü© ‚Üí |x‚ü© |rgx mod N ‚ü© for classical integers g and N .
Now consider factoring via Shor‚Äôs algorithm [Sho97], by performing quantum period finding using said circuit applied
to the state P
x |x‚ü© |r‚ü© for a classically-chosen random integer r such that 1 < r < N . Suppose that circuit yields
the factors of an integer N with some probability p. Replacing all QFTs in the circuit with optimistic QFTs with
‚àö
error parameter œµ will yield the factors with probability (1 ‚àí O(

œµn))p.

Proof. Inside of the QFT-based in-place modular multiplications on input |y‚ü© (Eq. 19; see [KY24], Appendix E), the
QFTs are applied to computational basis states |y‚ü©, |cy mod N ‚ü©, and |w‚ü© where w = ((c ‚àí 1)y mod N )/N expressed
as an n + O(log n) bit binary fraction.6 Observe that |cy mod N ‚ü© and |w‚ü© will take on a unique value for each
value of y (since c is coprime to N ). Therefore by the triangle inequality, the Frobenius error of the optimistic
multiplier can be bounded from above by the sum of the Frobenius error of the optimistic QFTs, which is bounded
by Definition 2. Thus an optimistic multiplier with error parameter O(œµ) can be formed from four optimistic QFTs
with error parameter œµ.

We may express any one optimistic multiplication in the modular exponentiation as

|y‚ü© ‚Üí

q

1 ‚àí œµ‚Ä≤

y |cy mod N ‚ü© +

q

œµ‚Ä≤
y |e‚ü©

(52)

where |e‚ü© is some error vector (not necessarily a computational basis state). Because every intermediate value in
the modular exponentiation is the product of some integer with the random integer r, the average value of œµ‚Ä≤
y is
equal to the error of the optimistic multiplier on a random input state, which by Definition 1 and the argument in
the previous paragraph is O(œµ). The overall modular multiplication circuit on input exponent x and random input
multiplier r can then be expressed as

|x‚ü© |r‚ü© ‚Üí |x‚ü© (p1 ‚àí œµ‚Ä≤‚Ä≤

x |rgx mod N ‚ü© + pœµ‚Ä≤‚Ä≤

x |e‚Ä≤‚ü©)

(53)

where again |e‚Ä≤‚ü© is some arbitrary error state. Because the exponentiation performs O(n) multiplications, the error
œµ‚Ä≤‚Ä≤
x in the exponentiation is on average (over choices of r) O(nœµ). Importantly, the bits of x are only used as controls
and never touched by the optimistic QFTs making up the multiplier circuit, so the error is only on the output
register.

Now we are prepared to examine the action of this optimistic modular exponentiation unitary on a uniform
superposition of values x, as it would be used in Shor‚Äôs algorithm. Our target state, which is the result of the
modular exponentiation in an exact circuit, is

The actual state resulting from the modified circuit is

|œàShor‚ü© =

1
2n/2

X

x

|x‚ü© |rgx mod N ‚ü© .

|œàavg‚àícase‚ü© =

1
2n/2

X

x

|x‚ü© (p1 ‚àí œµ‚Ä≤‚Ä≤

x |rgx mod N ‚ü© + pœµ‚Ä≤‚Ä≤

x |ex‚ü©)

(54)

(55)

for unknown error vectors |ex‚ü© orthogonal to |rgx mod N ‚ü©. The fidelity between these two states is then, on average
over choices of r,

| ‚ü®œàShor|œàavg‚àícase‚ü© |2 ‚â• (

p1 ‚àí œµ‚Ä≤‚Ä≤

x)2 ‚â• 1 ‚àí O(nœµ).

(56)

1
2n

X

x

‚àö

Thus, the measurement output distributions in the two circuits are O(
are returned with probability p in the original algorithm, they will be returned with probability (1 ‚àí O(
the modified algorithm, which is what we desired to show.

nœµ) close in trace distance. If the factors
nœµ))p in

‚àö

Remark‚Äî Intuitively, the random factor r should not even be necessary, as we do not expect the intermediate
values in the exponential to have any particular structure that biases them towards basis states with high error in
the optimistic QFT.

6The QFT is applied to |w‚ü© two separate times, yielding the four total QFTs mentioned earlier. Note that any ‚ÄúQFTs‚Äù known to be

applied to the |0‚ü© state are replaced with a layer of Hadamard gates.

16

 
D Proofs from Section 5

In this appendix we provide the proofs deferred from Section 5, starting with the approximate controlled phase
gradient.

Lemma 2 (Cost of approximate controlled phase gradients). There is a quantum circuit of depth O(log(n/Œ¥)) that
approximates the operation

|x‚ü© |z‚ü© 7‚Üí e2œÄixz/2n

|x‚ü© |z‚ü©

to error Œ¥ in the operator norm.

Proof. The construction is extremely similar to the standard approximate QFT [Cop02]. First, note that

e2œÄixz/2n

|x‚ü© |z‚ü© = |x‚ü©

Ô£∂

e2œÄi(0.xj ...xn‚àí1)¬∑zj |zj‚ü©

Ô£∏ .

Ô£´

Ô£≠

n‚àí1
O

j=0

In a similar manner to the QFT, this can be implemented using O(n2) controlled Z rotation gates, many of which
will be rotations through very small angles. Just as in the approximate QFT, we simply drop the small rotation
gates, and implement the transformation

|x‚ü© |z‚ü© 7‚Üí |x‚ü©

Ô£´

Ô£≠

n‚àí1
O

e2œÄi(0.xj ...xj+m‚àí1)¬∑zj |zj‚ü©

Ô£∏

Ô£∂

j=0

where m = log(2œÄn/Œ¥). This can be performed with O(n log(n/Œ¥) controlled Z rotation gates. Furthermore, by
grouping these gates in the correct way, the circuit can be implemented in depth O(log(n/Œ¥)). In the first time step,
we perform the required two-qubit gate between qubit j of the first register and qubit j of the second register, for
0 ‚â§ j ‚â§ n ‚àí 1. In the second time step, we perform the required two-qubit gate between qubit j of the first register
and qubit j ‚àí 1 of the second register, for 1 ‚â§ j ‚â§ n ‚àí 1, and so on, for m steps.

To bound the error, we note that we must simply sum the operator norms between each of the dropped controlled
rotations and the identity. We see that for a controlled Z rotation of angle Œ±, ‚à•I ‚àídiag(1, 1, 1, eiŒ±)‚à• = |1‚àíeiŒ±| ‚â§ |Œ±|.

Thus, the total error is bounded by

n‚àím‚àí1
X

n‚àí1
X

j=0

k=m+j

œÄ2‚àí(k‚àíj) ‚â§ œÄ

n‚àím‚àí1
X

j=0

2‚àím+1 ‚â§ 2œÄn2‚àím = Œ¥.

Next, we give further details on the construction of Theorem 5.

Theorem 5 (The Unitary Approximate QFT). There exists a unitary circuit for the approximate quantum Fourier
transform on n qubits with error œµ, with gate count O(n log(n/œµ)), depth O(log(n/œµ)), and total qubit count 3n +
O(n/ log n).
Proof. We must now implement V ‚Ä≤(r1, r2) = P
2n Z r2
2n , which corresponds to controlled
versions of the Weyl-Heisenberg operators. The operator P
2n is simply a quantum-quantum addition,
which we implement using the construction from [TTK10]. This circuit has depth O(log n) and requires O(n/ log n)
ancillas. The operator P

2n is the controlled phase gradient operation discussed in Lemma 2.

|r1, r2‚ü© ‚ü®r1, r2| ‚äó X r1
|r1‚ü© ‚ü®r1|‚äóX r1

|r2‚ü© ‚ü®r2| ‚äó Z r2

r1,r2

r1

r2

Let eU denote the derandomized optimistic QFT circuit with perfect controlled phase gradients and error parameter
œµ/9, and let U ‚Ä≤ denote an implementation of eU using the approximate controlled-phase gradient operations as
described in Lemma 2 with error parameter

œµ/3. By the triangle inequality and Theorem 2, for all states |œà‚ü©

‚àö

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(U ‚Ä≤ ‚àí I ‚äó U)

1
‚àö
k

X

i

|i‚ü© ‚äó |œà‚ü©

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

‚â§ œµ.

Thus, U ‚Ä≤ satisfies the conditions of the theorem.

17
