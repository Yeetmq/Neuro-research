{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def search_arxiv(\n",
    "    query: str, \n",
    "    max_results: int = 2, \n",
    "    sort_by: str = \"submittedDate\", \n",
    "    sort_order: str = \"descending\"\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Поиск статей в arXiv.org с сортировкой [[3]][[6]]\n",
    "    \n",
    "    Args:\n",
    "        query: Поисковый запрос (например, \"quantum computing\")\n",
    "        max_results: Максимальное количество результатов\n",
    "        sort_by: \"submittedDate\", \"lastUpdatedDate\" или \"relevance\"\n",
    "        sort_order: \"ascending\" или \"descending\"\n",
    "    \n",
    "    Returns:\n",
    "        Список словарей с метаданными статей\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start\": 0,\n",
    "        \"max_results\": max_results,\n",
    "        \"sortBy\": sort_by,\n",
    "        \"sortOrder\": sort_order\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"AcademicResearchTool/1.0 (https://github.com/your-repo)\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "    \n",
    "    # Парсинг XML\n",
    "    namespace = {\"atom\": \"http://www.w3.org/2005/Atom\"}\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    results = []\n",
    "    for entry in root.findall(\"atom:entry\", namespace):\n",
    "        results.append({\n",
    "            \"title\": entry.find(\"atom:title\", namespace).text,\n",
    "            \"summary\": entry.find(\"atom:summary\", namespace).text,\n",
    "            \"authors\": [\n",
    "                author.find(\"atom:name\", namespace).text \n",
    "                for author in entry.findall(\"atom:author\", namespace)\n",
    "            ],\n",
    "            \"published\": entry.find(\"atom:published\", namespace).text,\n",
    "            \"link\": entry.find(\"atom:id\", namespace).text,\n",
    "            \"pdf_url\": entry.find(\n",
    "                \"atom:link[@title='pdf']\", \n",
    "                namespace\n",
    "            ).attrib.get(\"href\", \"\")\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_text_from_pdf_url(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        text = extract_text(\"temp.pdf\")\n",
    "        return text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Ошибка: {str(e)}\"\n",
    "    \n",
    "def responsing(text: str, max_results: int) -> list:\n",
    "    all_texts = []\n",
    "    url = \"http://localhost:5000/translate\"\n",
    "    data = {\n",
    "        \"q\": text,\n",
    "        \"source\": \"ru\",\n",
    "        \"target\": \"en\"\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    result = response.json()\n",
    "    text = result['translatedText']\n",
    "    print(text)\n",
    "    arxiv_results = search_arxiv(text, max_results=5)\n",
    "    print(\"arXiv results:\")\n",
    "    for res in arxiv_results:\n",
    "        all_texts.append(extract_text_from_pdf_url(res['pdf_url']))\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "print(responsing('трансформеры', 2)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer\n",
      "arXiv results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "r\n",
      "a\n",
      "\n",
      "M\n",
      "8\n",
      "2\n",
      "\n",
      "]\n",
      "I\n",
      "\n",
      "N\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "1\n",
      "v\n",
      "3\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      ".\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "NetSSM: Multi-Flow and State-Aware Network Trace\n",
      "Generation using State-Space Models\n",
      "\n",
      "ANDREW CHU, University of Chicago\n",
      "XI JIANG, University of Chicago\n",
      "SHINAN LIU, University of Chicago\n",
      "ARJUN BHAGOJI, IIT Bombay\n",
      "FRANCESCO BRONZINO, École Normale Supérieure de Lyon\n",
      "PAUL SCHMITT, California Polytechnic State University, San Luis Obispo\n",
      "NICK FEAMSTER, University of Chicago\n",
      "\n",
      "Access to raw network traffic data is essential for many computer networking tasks, from traffic modeling to\n",
      "performance evaluation. Unfortunately, this data is scarce due to high collection costs and governance rules.\n",
      "Previous efforts explore this challenge by generating synthetic network data, but fail to reliably handle multi-\n",
      "flow sessions, struggle to reason about stateful communication in moderate to long-duration network sessions,\n",
      "and lack robust evaluations tied to real-world utility. We propose a new method based on state-space models\n",
      "called NetSSM that generates raw network traffic at the packet-level granularity. Our approach captures\n",
      "interactions between multiple, interleaved flows – an objective unexplored in prior work – and effectively\n",
      "reasons about flow-state in sessions to capture traffic characteristics. NetSSM accomplishes this by learning\n",
      "longer than existing transformer-based approaches. Evaluation results\n",
      "from and producing traces 8\n",
      "show that our method generates high-fidelity traces that outperform prior efforts in existing benchmarks. We\n",
      "also find that NetSSM’s traces have high semantic similarity to real network data regarding compliance with\n",
      "standard protocol requirements and flow and session-level traffic characteristics.\n",
      "\n",
      "and 78\n",
      "\n",
      "×\n",
      "\n",
      "×\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "The demand for representative, scalable network data is constant, driven by critical applications\n",
      "such as security analysis, traffic modeling, and performance evaluation [2, 18, 20, 28, 29, 35, 41, 43].\n",
      "Unfortunately, acquiring large-scale, high-fidelity network data is difficult due to data governance\n",
      "rules, legal restrictions, and the high collection costs [1, 10, 30, 44, 48]. In response, methods have\n",
      "been developed to generate synthetic network data that accurately replicates real networks. These\n",
      "approaches allow researchers and practitioners to test, evaluate, and model network scenarios\n",
      "while minimizing collection overhead and obstacles in accessibility.\n",
      "\n",
      "Existing methods for generating synthetic network data output this data in two forms: (1)\n",
      "sequences of single or multiple derived network traffic attributes, such as flow statistics (e.g., duration,\n",
      "average packet size), packet header fields (e.g., IP flags, addresses), or metadata (e.g., web page views,\n",
      "event types) and (2) raw packet capture (PCAP) traces. Generators producing traffic attributes\n",
      "are comparatively lightweight and can be used to replicate arbitrarily long-duration sessions of\n",
      "networked communication. Generators producing raw packets capture the verbose, inter-, and\n",
      "intra-packet interactions in a flow, and commodity packet analyzers (e.g., Wireshark, tcpdump) can\n",
      "manually analyze their resulting PCAPs.\n",
      "\n",
      "Unfortunately, current methods for either output format have limitations that impact their\n",
      "practical use. Traffic attribute generators cannot reason about the raw contents of stateful protocols,\n",
      "such as TCP, and require retraining to learn the patterns of new targets in a session. Raw packet\n",
      "generators are limited in the length of traces they can train on and produce and, thus, may\n",
      "not capture meaningful communication between nodes beyond initial connection setup. Further,\n",
      "neither generator type can reliably produce data for sessions comprised of more than a single flow,\n",
      "preventing them from being applied to various workloads in the real world, where interleaved,\n",
      "multi-flow communication is common (e.g., distributed systems, IoT). Finally, current methods\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\ffor evaluating the quality of synthetic network data (i.e., statistical similarity to real-world traces\n",
      "and downstream performance of ML models trained on synthetic data) are insufficient. Synthetic\n",
      "data that perform well in, or towards, these evaluations can still fall short in scenarios that require\n",
      "analysis of multi-flow interactions or stateful behaviors in network traffic (e.g., QoE estimation [39],\n",
      "application fingerprinting [25]). Thus, determining the criteria for what qualities or characteristics\n",
      "make synthetic network data “good” is an ongoing area of research.\n",
      "\n",
      "In this paper, we present NetSSM, a raw packet generator for network traffic data built on the\n",
      "recently proposed structured selective state-space model (Mamba) architecture. NetSSM bridges\n",
      "the gap between traffic attribute and raw packet generators by combining the former’s length-\n",
      "scaling capabilities with the latter’s comprehensive packet-level detail. This enables NetSSM to\n",
      "capture a substantially wider range of target events while retaining the ability to capture inter-\n",
      "and intra-packet dependencies across any protocol and layer. Furthermore, the sequential, stateful\n",
      "nature of how NetSSM learns network data allows it to generate sessions comprised of multiple\n",
      "interleaved flows with high fidelity, addressing a critical limitation of existing methods.\n",
      "\n",
      "To evaluate NetSSM, we train model variants on social media, video conferencing, and video\n",
      "streaming traffic. We first assess its performance using established metrics of synthetic network\n",
      "data fidelity (statistical similarity and downstream ML performance). We then evaluate NetSSM\n",
      "through a new lens of semantic similarity to test how well its generated data aligns with the\n",
      "behavioral characteristics of real-world network communication. This analysis aims to to offer a\n",
      "more functional and application-oriented perspective on the quality of synthetic data, emphasizing\n",
      "its practical utility beyond statistical resemblance. Our main contributions are:\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "•\n",
      "\n",
      "×\n",
      "\n",
      "×\n",
      "\n",
      "longer, and producing traces up to 78\n",
      "\n",
      "Synthetic multi-flow sessions. The recurrent structure of NetSSM enables it to produce\n",
      "traces that replicate the behavior of sessions comprised of standalone and interleaved flows\n",
      "with high fidelity. Multi-flow session generation is a new contribution that has been either\n",
      "unexplored or unreliable in prior generators.\n",
      "Capturing flow-state-dependent session events. NetSSM trains using a context window\n",
      "more than 8\n",
      "longer than existing transformer-based\n",
      "raw packet generators, respectively. This enables it to learn from and output traces that\n",
      "capture flow-state-dependent events occurring later in a session that rely on early connection\n",
      "setup, or multiple interactions between flows and/or packets.\n",
      "Superior performance on existing benchmarks. Evaluated on statistical similarity to\n",
      "ground truth traces and accuracy of downstream ML models trained on synthetic data,\n",
      "NetSSM outperforms both the current state-of-the-art traffic attribute generator (NetShare\n",
      "[49]) and raw packet generator (NetDiffusion [21]). For similarity, NetSSM achieves a Jensen-\n",
      "Shannon Divergence of 0.02 versus 0.16 and 0.04 for NetShare and NetDiffusion, respectively.\n",
      "Classification models trained using NetSSM data yield consistently high accuracy. A random\n",
      "forest classifier trained on entirely synthetic NetSSM data achieves an accuracy of 0.97 on\n",
      "held-out ground truth data. In contrast, the same classifiers trained on NetDiffusion and\n",
      "NetShare data yield 0.16 and 0.13 under the same conditions, respectively.\n",
      "Protocol-adherent and behaviorally accurate traffic. NetSSM generates synthetic traffic\n",
      "with high semantic similarity to real traces. Specifically, this traffic (1) shows robust session-\n",
      "level compliance with standard TCP protocol requirements and (2) captures characteristic,\n",
      "application-specific traffic patterns. For (1), NetSSM faithfully replicates correct stateful be-\n",
      "havior (e.g., handshakes, sequence progression, advanced options) and also captures common\n",
      "real-world anomalies (e.g., partial teardowns, conflicting flags). For (2), even when presented\n",
      "\n",
      "2\n",
      "\n",
      "\fwith complex, multi-flow traffic comprised of multiple steps (e.g., setup with content distribu-\n",
      "tion network endpoints before video segment downloads in video streaming traffic), NetSSM\n",
      "replicates these phases with high fidelity.\n",
      "\n",
      "2 RELATED WORK\n",
      "\n",
      "Techniques for generating synthetic network data aim to replicate the characteristics of real-\n",
      "world communication between networked devices, either through higher-level traffic attributes\n",
      "about packets or a session, or raw packet captures. Generation methods can be categorized to two\n",
      "main approaches: traffic attribute generators and raw packet generators.\n",
      "\n",
      "2.1 Traffic attribute generators\n",
      "\n",
      "Traffic attribute generators use either simulation or machine learning (generative adversarial\n",
      "networks (GANs), denoising diffusion, transformers) to produce traffic attributes. Simulation-based\n",
      "approaches were the earliest method for synthesizing network data, operating on a user-defined\n",
      "template that dictates a network’s topology, link specifications, and workload. A user replays\n",
      "existing traces or workloads on this specified configuration, and receives traffic attributes relevant\n",
      "to the simulated communication. Notable efforts in this approach include NS-3 [16], Cisco’s TRex [8],\n",
      "and others [3, 5, 24], which remain popular due to their configurability and relatively low resource\n",
      "footprint for most use cases, meeting many broad, general requirements for synthetic traffic.\n",
      "Unfortunately, prior work [6, 45] has shown that these methods’ simulated traffic typically lacks the\n",
      "variability and unpredictability inherent in actual network conditions. Thus, the resulting output\n",
      "may fail to capture the nuances of real-world traffic exchange.\n",
      "\n",
      "Machine learning-based approaches use time-series prediction models that learn from signals\n",
      "in a given continuous stream of input. This allows them to isolate the fine-grained variations of\n",
      "a single or few traffic attributes and produce data statistically similar to real-world traffic. The\n",
      "data produced by these approaches have been shown to improve the performance of downstream\n",
      "ML-based tasks (e.g., service recognition, anomaly detection). The ML components of these genera-\n",
      "tors also have lower training and inference resource overhead than other methods using higher\n",
      "complexity architectures. As each training sample and generated output is only a single or small\n",
      "set of continuous values, these generators are not limited by input length and can learn from and\n",
      "produce arbitrarily long sequences. In GAN-based approaches, modifications made to the original\n",
      "GAN architecture further prevent these models from “forgetting” dependencies or signals occurring\n",
      "early in a sequence that typically occurs in these models. Lin et al. ’s DoppelGANger [27] was\n",
      "the first GAN-based traffic attribute generator and produces sequences of single traffic attribute\n",
      "values. NetShare, a more recent approach by Yin et al. , builds on DoppelGANger to output more\n",
      "expressive sets of aggregate traffic attributes (e.g., duration, packet count), or more comprehensive\n",
      "sets of packet-level header field values (e.g., time-to-live [TTL], protocol flags) [49]. Zhang et al. ’s\n",
      "NetDiff uses both diffusion and transformers to try to better encode patterns in traffic attributes\n",
      "and use this encoding to better inform generation, specifically for mobile network data [50]. One\n",
      "limitation of these models is that when modeling raw packet contents, they only support learning\n",
      "and generating values from Layer 3 and below (plus transport-layer port numbers) in the OSI model.\n",
      "Thus, they cannot model interactions or attributes in stateful protocols (e.g., TCP).\n",
      "\n",
      "2.2 Raw packet generators\n",
      "\n",
      "Raw packet generators use simulation or machine learning (diffusion, transformers), to output\n",
      "synthetic network traffic in the form of verbose, content level PCAPs. The same simulation-based\n",
      "approaches described in the previous section can be used to produce raw traces, where the simulated\n",
      "communication between nodes is collected (versus summarized to yield traffic attributes) and\n",
      "\n",
      "3\n",
      "\n",
      "\fwritten to a trace. Unfortunately, the same notable shortcoming in expressiveness exists for these\n",
      "simulators when applied in this domain.\n",
      "\n",
      "Machine learning-based approaches train on raw packet data, and generate the byte-level values\n",
      "that comprise the packets of a session. Whereas traffic attribute generators are designed to learn\n",
      "from and capture variations in values over time implicit in a given time series, raw packet generators\n",
      "learn from and capture the inter- and intra-packet relationships contained in a trace’s raw contents,\n",
      "from which traffic attributes can be extracted. Operating on the packet level, these generators\n",
      "can also model protocols at any layer. Evaluated under the same metrics, raw packet generators\n",
      "have been shown to have comparable or better statistical similarity and downstream ML-task\n",
      "performance than traffic attribute generators. Further, the verbose PCAP format outputted by these\n",
      "models is the most versatile for later analysis and feature extraction. Jiang et al. ’s NetDiffusion uses\n",
      "a text-to-image diffusion model with image representations of network traces to generate images\n",
      "following text prompts that define traffic characteristics, which can then be converted back to\n",
      "binary PCAP form for analysis [21]. Qu et al. created TrafficGPT, a transformer decoder model that\n",
      "formulates raw packet generation as a token-based sequence generation task [36]. Given a starting\n",
      "token or prompt of raw traffic bytes in hexadecimal format, TrafficGPT trains on and produces\n",
      "sequences of up to 12,032 tokens, which can then be converted to binary PCAP. Most recently,\n",
      "Chu and Jiang et al. proposed using SSMs, specifically Mamba-1, to generate synthetic traces [7],\n",
      "which our work builds on. A key drawback to existing diffusion and transformer-based raw packet\n",
      "1281\n",
      "generators is their relatively short limit in training context and output length (1,024 and 113\n",
      "packets for NetDiffusion and TrafficGPT, respectively), which may fail to capture target events in\n",
      "sessions. NetDiffusion specifically also requires applying a post-generation heuristic to enforce\n",
      "byte-level corrections for protocol compliance before its traces can be used.\n",
      "\n",
      "−\n",
      "\n",
      "3 STATE SPACE MODELS FOR NETWORK TRAFFIC GENERATION\n",
      "\n",
      "Much communication between networked devices is stateful, and these exchanges may span long\n",
      "sequences of packets for multiple steps (e.g., setup, payload download, teardown). Our choice of\n",
      "Mamba [9, 13], a line of selective structured SSMs, accommodates these characteristics to generate\n",
      "high-fidelity, synthetic network traces. In this section, we provide background on the foundations\n",
      "and properties of SSMs and, specifically, the Mamba model (Section 3.1). We also motivate and\n",
      "compare Mamba against the existing approaches in raw packet generators (Section 3.2).\n",
      "\n",
      "3.1 State Space Models and Mamba\n",
      "\n",
      "SSMs are probabilistic graphical models built on the control engineering concept of a state\n",
      "space [23]. SSMs share the same objective (modeling discrete observations over time) as Hidden\n",
      "Markov Models but differ in that they use continuous, as opposed to discrete, latent variables. Like\n",
      "many deep-learning models, SSMs encode a hidden state that is representative of the prior observed\n",
      "context of an input sequence using recurrent scans. Specifically, SSMs use first-order ordinary linear\n",
      "differential equations to capture the relationship (output) between unobserved variables (state)\n",
      "and a series of continuous observations (input), irrespective of time (i.e., is linear time-invariant\n",
      "[LTI]). As the model observes more data, it encodes a representation of the state that captures the\n",
      "prior context of inputs. This state is then used to calculate an output for a given input and can be\n",
      "both discretized to be calculated as a recurrent neural network (RNN) in linear time and unrolled\n",
      "to a convolutional neural network (CNN) for efficient training. Unfortunately, SSMs suffer from\n",
      "the same pitfall of other recurrently updating networks, in that over time, information about data\n",
      "earlier in a context becomes increasingly compressed in the hidden state, leading to the “vanishing\n",
      "\n",
      "1Using packet lengths of 94/106 tokens from our evaluation case studies, for TrafficGPT’s max gen. length of 12,032 tokens.\n",
      "\n",
      "4\n",
      "\n",
      "\fgradient,” where the model can no longer recall dependencies between inputs. Work by Gu et\n",
      "al. [14] and Voelker et al. [47] offer a possible solution to this challenge by fixing the state matrix\n",
      "used in SSMs, resulting in improved model performance for recalling long-range dependencies.\n",
      "Follow-up works by Gu et al. provide additional improvements to the SSM, improving training\n",
      "efficiency for practical use via convolutional kernel (S4 [15]) and sequence modeling performance\n",
      "via a selection mechanism and a fixed state matrix (Mamba, Mamba-2 [9, 13]).\n",
      "\n",
      "Specifically, Mamba achieves this by implementing two modifications to the general SSM that\n",
      "provide structure and selection. Mamba implements structure by replacing the general SSM state\n",
      "matrix (typically randomly initialized) with a HiPPO matrix [14], introducing a probability measure\n",
      "that dictates how the SSM state is compressed. This, in effect, remedies the vanishing gradient and\n",
      "improves the Mamba SSM’s ability to model long-range dependencies in sequences. For selection,\n",
      "the general LTI SSM lacks expressiveness, i.e., all discrete inputs compressed in the state affect the\n",
      "state with equal weighting. In language modeling, this prevents semantically important “keywords”\n",
      "from more heavily influencing the SSM state and developing a better understanding of input.\n",
      "Mamba improves expressiveness by removing the LTI quality of the general SSM and makes the\n",
      "model time-variant, in which the state is calculated using learned (rather than fixed) functions\n",
      "of the inputs. Mamba’s structure and selection modifications to the general SSM architecture\n",
      "provide competitive performance against conventional transformer-based approaches for sequence\n",
      "modeling, with better scaling (linear versus quadratic).\n",
      "\n",
      "3.2 Why Mamba?\n",
      "\n",
      "We use the Mamba architecture because it is inherently suited to the nature of network data.\n",
      "Specifically, much of networked communication is stateful. Communication between hosts often\n",
      "explicitly depends on the sequential exchange of packets to ensure correct data assembly and\n",
      "to maintain the connection itself. This can be mapped to the recurrent quality of the state-space\n",
      "architecture, in which the model sequentially updates the hidden state on each new input. In\n",
      "our application of Mamba to synthetic trace generation, this provides the means for NetSSM to\n",
      "effectively learn from and produce sessions composed of multiple flows. In contrast, prior traffic\n",
      "attribute and raw packet generators can only operate within the scope of single-flow sessions.\n",
      "\n",
      "The architecture’s convolutional unrolling further complements the network domain by enabling\n",
      "updates to be performed in parallel, helping the model to train over substantially long sequences\n",
      "(e.g., PCAPs) while still implicitly capturing sequential dependencies. As such, Mamba is a much\n",
      "more “natural” fit for modeling network data compared to prior methods for generating raw packet\n",
      "traces. Diffusion-based approaches require abstracting network data to a different domain (images),\n",
      "and further generate traces based on signals from the entire trace, neglecting the sequential delivery\n",
      "of network traffic. Transformer-based models likewise learn input semantics in a completely\n",
      "parallel fashion, where attention is calculated per token of a sequence, against all other tokens\n",
      "in the sequence simultaneously, also not strictly sequentially. The completely parallel nature of\n",
      "computation for either approach is also resource-intensive. NetSSM can generate traces roughly\n",
      "times longer than TrafficGPT. This is a key improvement, as it\n",
      "10\n",
      "allows NetSSM to capture flow-state-dependent sessions events that manifest only after substantial\n",
      "setup has occurred. For instance, in our analysis of Netflix streaming traffic (Section 5.3.3), we\n",
      "found across various collection scenarios that content segments consistently were downloaded\n",
      "only after\n",
      "\n",
      "2,250 packets, when setup had completed.\n",
      "\n",
      "longer than NetDiffusion and 78\n",
      "\n",
      "×\n",
      "\n",
      "×\n",
      "\n",
      "∼\n",
      "\n",
      "4 NETSSM\n",
      "\n",
      "Motivated by shortcomings in existing synthetic network data generators and strong alignment\n",
      "between the operation and capabilities of SSMs and the qualities of networked communications, we\n",
      "\n",
      "5\n",
      "\n",
      "\fFig. 1. Overview of the NetSSM pipeline.\n",
      "\n",
      "present NetSSM, a new raw packet generator. To create NetSSM, we adapt the Mamba-2-backbone,\n",
      "training it from scratch on packets’ raw byte values to synthesize raw packets. Figure 1 provides\n",
      "an overview of the NetSSM pipeline. We provide details for each step of the general pipeline\n",
      "below. Specifics for NetSSM variants used in our evaluation are in their corresponding subsections\n",
      "(Sections 5.1, 5.2, and 5.3, respectively).\n",
      "\n",
      "4.1 Pre-processing Networking Data\n",
      "\n",
      "Input to NetSSM is a sequence of raw bytes comprising the packets in a session trace. Specifically,\n",
      "networking data in PCAP form is parsed to a representative format to align with the token-based,\n",
      "sequence generation objective of the Mamba SSM.\n",
      "\n",
      "[\n",
      "\n",
      "]\n",
      "\n",
      "0, 255\n",
      "\n",
      "4.1.1 Tokenization. We define a custom tokenizer using Huggingface Tokenizers [34] that one-to-\n",
      "one maps the decimal values of the raw bytes comprising each packet to a corresponding token ID\n",
      "in range\n",
      ". In this way, NetSSM reasons about the raw contents of networking traffic close\n",
      "to its original form. This differs from prior work where network data is represented/tokenized at\n",
      "the flow level [26], as a mix of packet-level and flow attributes [36], or created using a tokenization\n",
      "algorithm that may map raw bytes to tokens using logic suited to a different domain (i.e., WordPiece\n",
      "from NLP) [33]. Our tokenizer also defines label special tokens (e.g., <|facebook|> <|meet|>,\n",
      "<|netflix|>) and a packet special token (<|pkt|>) to allow NetSSM to differentiate between\n",
      "traffic dynamics of different workloads, and packet boundaries in sessions.\n",
      "\n",
      "4.1.2 Creating input data. We extract input to NetSSM from labeled (i.e., the workload/service\n",
      "type of collected traffic is known) collections of PCAPs based on the desired modeling granularity,\n",
      "i.e., single-flow or multi-flow sessions. For single-flow sessions, we split the original PCAP into\n",
      "multiple PCAPs, each corresponding to a comprising flow based on connection (i.e., five-tuple:\n",
      "source IP, source port, destination IP, destination port, IP protocol). No pre-processing is needed\n",
      "for multi-flow sessions. We parse each PCAP to convert the raw bytes comprising each packet to a\n",
      ") in string form, with <|pkt|> special tokens\n",
      "sequence of 8-bit decimal values (i.e., value\n",
      "0, 255\n",
      "]\n",
      "delimiting each packet, and the PCAP’s corresponding label special token prepended to the string.\n",
      "Finally, we use the custom NetSSM tokenizer to tokenize the parsed, string-based PCAP data to a\n",
      "format consumable by NetSSM, producing one input sample for each PCAP in a dataset.\n",
      "\n",
      "∈ [\n",
      "\n",
      "4.2 Pre-training NetSSM\n",
      "\n",
      "Training data created using the above process are fed into NetSSM to learn the semantics of\n",
      "packets, flows, and, correspondingly, sessions. To detail, NetSSM treats generating network traffic\n",
      "data as an unsupervised sequence generation problem. During training, the model optimizes towards\n",
      "the standard cross-entropy loss function which, measures how well the predicted probabilities for\n",
      "a token at a specific index match the correct token. For our experiments with NetSSM, we train\n",
      "the packet generation model using a batch size of one input/training sample, which allows each\n",
      "sample to be 100,000 tokens in length (the maximum length supported for our experiment setup).\n",
      "\n",
      "6\n",
      "\n",
      "Ground TruthPCAPs<|netﬂix|>160......<|pkt|><|pkt|>244Mamba-2NetSSM<|label|> ... <|pkt|>SeedLengthn tokensSyntheticPCAPPre-processingTokenizationPre-trainingGeneration\fThis batch size maximizes the length of packet sequences (i.e., context length) our model learns\n",
      "from, where 100,000 tokens correspond to a context of at least 943 packets (when using packet\n",
      "representations from our case studies).\n",
      "\n",
      "4.3 Generating Synthetic Traces\n",
      "\n",
      "Trace generation requires two arguments: a generation seed and length. The generation seed\n",
      "matches the format of NetSSM’s training samples – a label special token followed by a sequence\n",
      "of any number of full or partial packets represented by their raw-byte contents in decimal form\n",
      "(e.g., <|amazon|> 188 34 203... <|pkt|>). The seed is used to “prompt” NetSSM for generation,\n",
      "equivalent to the “start token” or string in NLP generative models. The generation length dictates\n",
      "the output length (in tokens) NetSSM generates. Using the generation seed, NetSSM’s packet\n",
      "model begins autoregressively generating the raw bytes comprising subsequent packets of the\n",
      "synthetic trace. This procedure continues until the given generation length is satisfied. NetSSM\n",
      "then constructs the intermediate synthetic trace, concatenating the sequence of generated packets\n",
      "represented by their raw bytes in decimal format, and prepending the label special token. This\n",
      "format is then converted to a complete PCAP binary for practical use and downstream evaluation.\n",
      "\n",
      "5 EVALUATION\n",
      "\n",
      "We evaluate the quality of synthetic data produced by NetSSM in three key areas: (1) statistical\n",
      "similarity between generated and real traffic, (2) downstream utility of generated data towards\n",
      "training and improving ML-for-networking models, and (3) semantic similarity between generated\n",
      "and real traffic. Previous traffic attribute and raw packet generators are measured using metrics\n",
      "of statistical similarity and downstream performance. We introduce semantic similarity as an\n",
      "additional aspect that should be considered when evaluating synthetic network data models or\n",
      "systems. For each evaluation, we train a specialized NetSSM model, following the overarching\n",
      "configuration outlined in Section 4.2, to model the traffic dynamics of the respective workloads\n",
      "and objectives. Detailed results and analysis for each case study are presented below.\n",
      "\n",
      "5.1 Statistical Similarity\n",
      "\n",
      "We first evaluate NetSSM using the conventional metric of statistical similarity, which assesses\n",
      "the byte-wise matching between generated synthetic traces and the ground truth traces used for\n",
      "training. In this study, we train a NetSSM model on single-flow traces collected from various\n",
      "types of multimedia traffic. We specifically focus on single-flow traffic because it allows for a more\n",
      "precise and uncontaminated statistical comparison. By isolating individual flows, we can accurately\n",
      "compare the real and synthetic traces based on the specific type of media traffic without interference\n",
      "from other flows or sessions, which would complicate the analysis. Further, prior efforts evaluate\n",
      "the statistical similarity of their approaches on single-flow traffic, allowing us to directly compare\n",
      "NetSSM’s performance with these methods. After training, we generate synthetic traces and\n",
      "compare them to their ground truth counterparts. Our findings show that NetSSM’s synthetic\n",
      "traces exhibit high statistical similarity to real data at the content level (byte-wise comparisons).\n",
      "NetSSM outperforms previous synthetic network trace generation methods in various statistical\n",
      "metrics, highlighting its superior ability to replicate real-world traffic characteristics.\n",
      "\n",
      "5.1.1\n",
      "Setup. We evaluate the statistical similarity of traces produced by (1) a base NetSSM model\n",
      "that trains on and produces continuous sessions, and (2) a fine-tuned version of this base model,\n",
      "optimized to generate packets specific to distinct flow stages. Here, we wish to examine if additional\n",
      "fine-tuning can yield additional performance improvements, particularly in generating these distinct\n",
      "phases or components of authentic network traces. Our rationale stems from the observation that\n",
      "different stages of a network flow, such as the TLS handshake phase (characterized by SYN, SYN-ACK,\n",
      "\n",
      "7\n",
      "\n",
      "\fTable 1. Overview of datasets used to train and evaluate NetSSM.\n",
      "\n",
      "Dataset\n",
      "\n",
      "Evaluation Task(s)\n",
      "\n",
      "Source\n",
      "\n",
      "Content Type\n",
      "\n",
      "Size\n",
      "\n",
      "Classification\n",
      "\n",
      "# uniqe sub-groups\n",
      "\n",
      "Raw\n",
      "\n",
      "# Captures\n",
      "\n",
      "Multimedia Traffic\n",
      "\n",
      "Statistical Sim.\n",
      "Downstream Util.\n",
      "\n",
      "Bronzino et al. [4]\n",
      "MacMillan et al. [32] Video Conferencing\n",
      "Jiang et al. [22]\n",
      "\n",
      "Video Streaming\n",
      "\n",
      "Social Media\n",
      "\n",
      "Netflix Streaming\n",
      "\n",
      "Semantic Sim.\n",
      "\n",
      "Bronzino et al. [4]\n",
      "\n",
      "Video Streaming\n",
      "\n",
      "* composed of single flows.\n",
      "\n",
      "4\n",
      "3\n",
      "3\n",
      "\n",
      "1\n",
      "\n",
      "6.36 GiB\n",
      "17.36 GiB\n",
      "5.40 GiB\n",
      "\n",
      "216.36 GiB\n",
      "\n",
      "10,032∗\n",
      "13,911∗\n",
      "3,896∗\n",
      "\n",
      "5,882†\n",
      "\n",
      "† composed of multiple flows.\n",
      "\n",
      "and ACK packets) and the data transmission phase (dominated by PUSH and ACK packets), exhibit\n",
      "unique patterns and behaviors. Additional, fine-tuning can be especially useful for applications\n",
      "that require synthetic data tailored to specific flow stages, such as generating synthetic traces for\n",
      "data transmission or session termination to study key network behaviors (e.g., session termination\n",
      "indicators). We detail the setup for either model below.\n",
      "Base model. We train our NetSSM model for single-flow generation using a dataset comprised\n",
      "of session traces collected from 10 distinct applications, falling into three overarching categories:\n",
      "video streaming [4], video conferencing [32], and social media [22]. Table 1 presents an overview\n",
      "of this data. We first pre-process the data from each source, using pcap-splitter [40] to split\n",
      "original PCAPs into their comprising single-flow PCAPs based on 5-tuple, and parse them into\n",
      "the string representations of their raw bytes in decimal form, as described in Section 4.1.2. We fix\n",
      "each packet to be represented by 94 tokens, corresponding to the maximum practical lengths of\n",
      "the Ethernet (14 bytes), IPv4 (20 bytes excluding options), and TCP headers (60 bytes including\n",
      "TCP extensions). To fit the scope of this case study, we train this NetSSM variant on TCP traffic\n",
      "only, as session handshakes/setup predominantly requires the state maintaining quality of a TCP\n",
      "connection. We do not consider TCP payload in this case study, as this data is becoming increasingly\n",
      "encrypted [11, 17, 19] and thus would be noise our model would not learn from.\n",
      "\n",
      "We create a custom tokenizer following the configuration described in Section 4.1.1, defining 10\n",
      "label special tokens corresponding to the 10 distinct applications in our dataset. We then tokenize\n",
      "all string representations resulting from splitting our data to their single-flows. This results in a\n",
      "final dataset of 27,839 samples. We pre-train the single-flow packet NetSSM model on the created\n",
      "dataset using a single NVIDIA A40 48GB GPU for 30 epochs with a gradient clip value of 1.0 and\n",
      "4. All other AdamW parameters are left at default.\n",
      "AdamW optimizer with learning rate of 5\n",
      "We use the same configuration as the smallest publicly available 130 million parameter pre-trained\n",
      "Mamba-2 (dimension of 768, 24 layers), but instead use our custom tokenizer. The training process\n",
      "results in a model with cross-entropy loss converging at 0.42 nats.\n",
      "\n",
      "10−\n",
      "\n",
      "×\n",
      "\n",
      "We generate traces using the process detailed in Section 4.3 with the trained NetSSM model,\n",
      "producing a corresponding synthetic trace for each real trace used during training. Specifically,\n",
      "we use the first packet from the real training trace, represented in decimal form, along with its\n",
      "corresponding label as the seed (e.g., <|amazon|> 188 34 203... <|pkt|>). We set the generation\n",
      "length to be the number of tokens needed to represent the total number of packets of a corresponding\n",
      "real trace. This ensures that the generated trace contains a similar number of packets to the real\n",
      "trace, providing a consistent basis for evaluating the synthetic version’s statistical similarity.\n",
      "Fine-tuned model. We train the fine-tuned NetSSM model by first creating sub-datasets from\n",
      "the original dataset described above, that isolate the packets relevant to specific stages of a flow’s\n",
      "lifetime. These sub-datasets focus on distinct phases of network communication, such as session\n",
      "initiation, data exchange, and session termination. We then use these phase-specific data to fine-\n",
      "tune the base single-flow NetSSM model from the 30-epoch pre-training checkpoint, using the\n",
      "same next-token prediction objective as the original model but with phase-specific packets as\n",
      "\n",
      "8\n",
      "\n",
      "\fTable 2. Byte-wise statistical similarity for traces of various generators. NetSSM generated traces are\n",
      "most statistically similar to real traffic, with both its base and fine-tuned versions achieving at least 2\n",
      "lower\n",
      "divergence and distance values across all metrics as compared to the next best method.\n",
      "\n",
      "×\n",
      "\n",
      "Generation Method\n",
      "\n",
      "Random Generation (flow statistics)\n",
      "Random Generation (raw packets)\n",
      "NetShare\n",
      "NetDiffusion†\n",
      "TrafficGPT*\n",
      "NetSSM (base)\n",
      "NetSSM (fine-tuned)\n",
      "\n",
      "† Post-generation correction applied.\n",
      "\n",
      "Jensen-Shannon Divergence\n",
      "\n",
      "Total Variation Distance\n",
      "\n",
      "Hellinger Distance\n",
      "\n",
      "↓\n",
      "\n",
      "↓\n",
      "\n",
      "↓\n",
      "\n",
      "Evaluation Metric\n",
      "\n",
      "0.67\n",
      "0.82\n",
      "0.16\n",
      "0.04\n",
      "0.16*\n",
      "0.02\n",
      "0.02\n",
      "\n",
      "0.80\n",
      "0.99\n",
      "0.16\n",
      "0.04\n",
      "—\n",
      "0.02\n",
      "0.01\n",
      "\n",
      "0.76\n",
      "0.95\n",
      "0.18\n",
      "0.05\n",
      "—\n",
      "0.02\n",
      "0.02\n",
      "\n",
      "* As reported in [36].\n",
      "\n",
      "input. This allows the model to capture the intra-packet and flow dynamics unique to each phase,\n",
      "leading to improvements in both the quality and flexibility of output. When generating data with\n",
      "the fine-tuned models, We chain outputs from one phase-specific model to the next. Specifically,\n",
      "the final packet produced by the handshake model serves as the seed for the subsequent data\n",
      "transmission model, while the final packet generated by the data transmission model acts as the\n",
      "seed for the subsequent session teardown model.\n",
      "\n",
      "5.1.2 Content-Level Results. We evaluate NetSSM’s generation fidelity by analyzing the statistical\n",
      "similarity between the generated and real traffic data at the content level. This similarity is quantified\n",
      "using three key metrics: Jensen-Shannon Divergence (JSD), Total Variation Distance (TVD), and\n",
      "Hellinger Distance (HD), which measure distributional distances, with lower values indicating\n",
      "closer alignment to real-world data.\n",
      "\n",
      "For NetSSM and NetDiffusion, we perform byte-wise comparisons by converting the generated\n",
      "and ground truth PCAPs into standardized binary representations using nPrint [18], which supports\n",
      "up to 1,024 packets per PCAP (the maximum for NetDiffusion). We then consider the three statistical\n",
      "measures across all TCP header values, to enable uniform comparison. For TrafficGPT, we rely on\n",
      "the results reported in their work [36], as they do not provide open-source code, limiting direct\n",
      "comparison. For NetShare, which generates traffic attributes, we derive the ground truth traffic\n",
      "attributes from the real traces and compute the statistical distances between these and the generated\n",
      "traffic attributes. Though NetShare generate traffic attributes and NetDiffusion, TrafficGPT, and\n",
      "NetSSM generate raw traces, it is still appropriate to compare their resulting statistical measures\n",
      "as they measure distributional distances rather than absolute values. This ensures the evaluation\n",
      "captures how closely each model’s generated traffic replicates real-world traffic patterns within\n",
      "its respective granularity. Finally, we include two lower-bound baselines representing worst-case\n",
      "scenarios: random generation of flow statistics and raw packets, which serve as benchmarks for\n",
      "poor fidelity. Table 2 summarizes the results of the comparisons.\n",
      "\n",
      "We find that NetSSM consistently outperforms the other methods in all metrics. As expected,\n",
      "random generation of traffic attributes and raw packets produces the highest distances, with the\n",
      "JSD, TVD, and HD values ranging from 0.67 to 0.99. NetShare achieves moderate performance with\n",
      "HD of 0.18 and 0.16 for both the JSD and TVD, but falls short in accurately capturing fine-grained\n",
      "packet details. NetDiffusion performs well with post-generation correction applied, achieving low\n",
      "distances (JSD of 0.04) but still lags behind NetSSM. TrafficGPT, while comparable to NetShare in\n",
      "some cases, lacks complete data for comparison. NetSSM demonstrates the highest fidelity, with\n",
      "distance values as low as 0.01 to 0.02, highlighting its ability to generate synthetic traces that at\n",
      "the byte-level, closely resemble real traffic.\n",
      "\n",
      "9\n",
      "\n",
      "\f(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "Fig. 2. Accuracy of random forest classifiers trained on varying proportions of real/synthetic data.\n",
      "Models trained on NetSSM data demonstrate significant improvements in test accuracy compared to both\n",
      "NetShare and NetDiffusion. Colored shading highlights areas where the corresponding model achieves higher\n",
      "accuracy than the next best baseline.\n",
      "\n",
      "5.2 Downstream Utility\n",
      "\n",
      "We next examine the performance of ML-for-networking models trained with synthetic data\n",
      "to assess the quality of this data in practical applications. While measures of statistical similarity\n",
      "provide insight into how closely synthetic data replicates the raw composition of real-world data,\n",
      "they should not be the only metrics used to evaluate the quality of generated traffic. Specifically,\n",
      "when calculated at the high-granularity (e.g., at the byte-level as detailed in the previous section),\n",
      "these measures may not capture how well synthetic data can be used in application. Thus, examining\n",
      "how well synthetic data can be used in various downstream tasks (e.g., network traffic classification,\n",
      "anomaly detection, or intrusion detection) provides further validation of data fidelity.\n",
      "\n",
      "Specifically, we train two classifiers that focus on (1) application-level classification, and (2)\n",
      "service-type-level classification. For (1), the objective is to classify network traffic into specific,\n",
      "fine-grained applications (e.g., differentiating YouTube from Amazon traffic). For (2), we group\n",
      "traffic into broader categories such as differentiating between media types (e.g., video streaming,\n",
      "web browsing), that encapsulates more general traffic patterns.\n",
      "\n",
      "5.2.1\n",
      "Setup. The model pipeline, hardware, and parameters of this case study follow exactly with\n",
      "the setup described in Section 5.1.1. To test the utility of synthetic data in augmenting model\n",
      "training, we create downstream training datasets composed of both real data, and synthetic data\n",
      "from NetSSM, NetDiffusion, and NetShare. These datasets have different mixing rates that represent\n",
      "the proportion of synthetic data used to replace original real data in the dataset. We create a new\n",
      "dataset at each 10% inclusive increments, resulting in 33 downstream training datasets. For example,\n",
      "a downstream training dataset with a 20% mixing rate contains 80% real data, and 20% synthetic\n",
      "data. We train three different types of ML classifiers (Decision Trees [DT], Random Forest [RF],\n",
      "and Support Vector Machines [SVM]) on these downstream datasets, resulting in a corresponding\n",
      "33 models. Finally, we test each models’ performance on held out samples of completely real, and\n",
      "completely synthetic data to assess their performance and generalization across different training\n",
      "and testing environments. In each scenario, we analyze if a generator’s synthetic data can maintain\n",
      "and/or improve classification accuracy when mixed into the training data at various rates.\n",
      "\n",
      "5.2.2 Results. Figure 2 shows the accuracy of Random Forest (RF) models trained for application\n",
      "and service-type-level traffic classification using the mixed downstream datasets from NetSSM,\n",
      "NetDiffusion and NetShare, and tested on application and both completely real, and completely\n",
      "synthetic data. The vertical dashed line in each sub-figure highlights the maximum accuracy gain in\n",
      "each scenario. We focus on only RF model performance to simplify the presentation, as the results\n",
      "\n",
      "10\n",
      "\n",
      "0.00.51.0MixingRate0.000.250.500.751.00AccuracyΔAcc.:trainon100%syndata:0.818AppLevel;TestedonRealData0.00.51.0MixingRate0.000.250.500.751.00ΔAcc.:trainon100%syndata:0.329ServiceTypeLevel;TestedonRealData0.00.51.0MixingRate0.000.250.500.751.00ΔAcc.:trainon100%syndata:0.858AppLevel;TestedonSynData0.00.51.0MixingRate0.000.250.500.751.00AccuracyΔAcc.:trainon100%syndata:0.471ServiceTypeLevel;TestedonSynDataRF,NETSSMRF,NetDiffusionRF,NetShare\ffrom other models (e.g., Decision Trees and SVMs) exhibit similar patterns. Comprehensive results\n",
      "for these additional models, along with results from the non-fine-tuned version, are in Appendix A.\n",
      "Testing on Real Data. We first examine the performance our downstream models when tested\n",
      "on completely real data. Figures 2a and 2b visualize the results. Models trained on NetSSM data\n",
      "maintain consistently high classification accuracy across all mixing rates, even when the training set\n",
      "consists entirely of synthetic data. We observe substantial accuracy gain over existing approaches,\n",
      "with an improvement of approximately 0.818 and 0.329 at the application and service-type-level,\n",
      "respectively, when synthetic data constitutes 100% of the training set. This demonstrates NetSSM’s\n",
      "ability to generate realistic synthetic traffic that (1) preserves fine-grained distinctions between\n",
      "traffic patterns critical for application-level classification and (2) effectively replicates high-level\n",
      "traffic behaviors that are helpful for service-type-level classification.\n",
      "\n",
      "We observe that models trained on NetDiffusion data, the next best generator for the classification\n",
      "task, experience significant drops in accuracy for either task as mixing rate increases. In application-\n",
      "level classification, there is notable decrease in model accuracy after the 60% mixing rate, with\n",
      "the gap widening further as synthetic data becomes the sole training input. For service-type\n",
      "classification, NetDiffusion-data-trained models performs well up to an 80% mixing rate but suffer\n",
      "rapid decline in accuracy as more real data is replaced with synthetic data. This suggests that\n",
      "while NetDiffusion is more effective at capturing broader patterns, it struggles to replicate more\n",
      "fine-grained, application specific interactions.\n",
      "\n",
      "Finally, models trained on NetShare data, which consists of only traffic attributes, perform\n",
      "the worst on both tasks, with accuracy sharply declining even at low mixing rates. The lack of\n",
      "detailed packet-level data in NetShare’s output makes it ineffective for use in tasks where granular\n",
      "distinctions between traffic applications are essential. At the service-type-level, models similarly\n",
      "show reinforcing the idea that traffic attributes alone are insufficient for accurate classification.\n",
      "Despite some improvement when synthetic data dominates the training set, NetShare consistently\n",
      "lags behind the other models, further emphasizing the importance of generating detailed packet-\n",
      "level data to achieve high fidelity in traffic classification.\n",
      "Testing on Synthetic Data. Testing our downstream models on completely synthetic data yields\n",
      "similar results, as shown in Figures 2c and 2d. NetSSM consistently achieves near-perfect accuracy,\n",
      "ranging from 0.94 to 0.97 for application-level classification and from 0.99 to 1.00 for service-type-\n",
      "level classification, regardless of the mixing rate. This represents improvements of 0.858 and 0.471 in\n",
      "either task over the next best synthetic data generator. This demonstrates that NetSSM’s generated\n",
      "traces maintain internal coherence, and do not introduce artificial signals that degrade downstream\n",
      "model performance. While models trained on NetDiffusion and NetShare data perform relatively\n",
      "well at higher mixing rates, they experience notable drops in accuracy at lower mixing rates. For\n",
      "application-level classification, we observe accuracies as low as 0.10 and 0.12 for NetShare and\n",
      "NetDiffusion, respectively. For service-type-level classification, we observe minimum accuracies of\n",
      "0.17 and 0.52 for NetShare and NetDiffusion, respectively.\n",
      "\n",
      "Finally, we conduct ablations with NetSSM data that exclude the first packet in both the generated\n",
      "and real traces during downstream model training and inference. We do this to ensure that the\n",
      "downstream models were not learning solely based on the first packet, which is used as a seed from\n",
      "the real trace to generate the remainder of the synthetic trace in NetSSM. The results, presented\n",
      "in Figure 6 (and Appendix A), show that excluding the seed packet has no significant impact on\n",
      "downstream models’ performance, confirming the robustness and generalization capability of\n",
      "NetSSM’s generated traffic the beyond initial packet dependency.\n",
      "\n",
      "Overall, we find that at higher mixing rates (particularly when synthetic data comprises 80-100%\n",
      "of the training data) NetSSM data provides superior performance compared to both NetDiffu-\n",
      "sion and NetShare data. NetSSM-generated data can effectively replace real-world data in both\n",
      "\n",
      "11\n",
      "\n",
      "\fapplication and service-type-level ML classification tasks without significant losses in accuracy,\n",
      "verifying utility in downstream tasks. This resilience is particularly important when synthetic data\n",
      "dominates the training set, as models trained on NetSSM-generated data appear to remain robust,\n",
      "and can generalize well even when synthetic data constitutes the majority of the input.\n",
      "\n",
      "5.3 Semantic Similarity\n",
      "\n",
      "The existing statistical similarity and downstream utility measures for network data generators\n",
      "are largely motivated by how well these data can improve downstream ML-for-networking model\n",
      "performance. This approach may be acceptable for traffic attribute generators as their produced\n",
      "sequences are independent in scope, e.g., NetShare [49] is trained on independent sequences of\n",
      "packet sizes, and observed IP protocols in a ground truth flow to produce independent sequences\n",
      "of the same values for a synthetic flow. Here, while in reality these fields may influence each other,\n",
      "the model learns them as separate entities. However, raw packet generators should be evaluated\n",
      "more rigorously, as the surrounding, or preceding raw byte values in a packet/capture directly\n",
      "affect the generation of trace as a whole (e.g., for diffusion, and transformers/SSMs, respectively). If\n",
      "traffic attribute can be extracted from a generated trace but the capture itself cannot be manually\n",
      "examined by a packet analysis tool, this is likely not “good” synthetic data. Similarly, if a generated\n",
      "PCAP is parsable by Wireshark, but the contained communication setup between flows is out of\n",
      "order or incorrect, this traffic should not be considered for replacing real-world data.\n",
      "\n",
      "To this end, we evaluate NetSSM’s ability to produce semantically similar synthetic network\n",
      "traffic that (1) is TCP flow and session-compliant, i.e., maintains correct and realistic TCP state\n",
      "transitions across the span of a trace, and (2) captures the implicit characteristics for a given\n",
      "networked communication workload. In either analysis, we additionally evaluate NetSSM’s ability\n",
      "to produce multi-flow synthetic traces, specifically Netflix video streaming traffic, produced by a\n",
      "new NetSSM model further detailed in this section. For (1), we analyze both synthetic single-flow\n",
      "traffic generated in the previous section and the multi-flow Netflix traffic. For (2), we examine the\n",
      "multi-flow communication between end hosts and Netflix video streaming servers in our synthetic\n",
      "traces. We analyze the sending patterns between hosts and verify that the generated traces contain\n",
      "the distinctive segment download patterns of real Netflix traffic. We also verify that NetSSM can\n",
      "maintain the generation of these patterns when prompted with packets corresponding to traces\n",
      "collected under different network conditions.\n",
      "\n",
      "5.3.1\n",
      "Setup. We train NetSSM on multi-flow traffic of Netflix video streaming sessions collected\n",
      "by Bronzino et al. [4] from 66 real-world, home devices in the United States and France, with link\n",
      "capacities spanning 18 Mbps to 1 Gbps. Table 1 (bottom) provides an overview of this data. We train\n",
      "on only a subset of this data (5,882 captures) due to access constraints. We do not split captures their\n",
      "comprising single-flows, but perform all other pre-processing in an identical manner as described\n",
      "in Section 5.1.1. Training sequences consist of both TCP and UDP packets, and we further extend\n",
      "the packet representation used for this model to 106 tokens, where the 12 additional tokens as\n",
      "compared to the NetSSM variant described in Section 5.1 allow NetSSM to learn information from\n",
      "DNS headers for requests and responses present in session setup. All TCP and UDP payload is\n",
      "discarded for the reasons described in the previous case study setup, in addition to the increasing\n",
      "prevalence of encrypted DNS [12, 31, 38]. Tokenization of our training sequences follows the\n",
      "process of the statistical similarity pipeline, resulting in a dataset of 5,882 samples. Finally, we\n",
      "pre-train the multi-flow model using the same training parameters and hardware as described in\n",
      "Section 5.1.1, for 30 epochs, where cross-entropy loss converged at 0.33 nats.\n",
      "\n",
      "We then use this model to generate synthetic multi-flow network traffic. Specifically, we focus\n",
      "on examining NetSSM’s ability to capture the sending/receiving of Dynamic Adaptive Streaming\n",
      "\n",
      "12\n",
      "\n",
      "\fover HTTP [42] segments containing Netflix audio and video content, as described in the paper\n",
      "for this dataset [4], across the various collection scenarios present in the ground truth data. We\n",
      "empirically observe in the traces used to train our multi-flow NetSSM variant that the video stream\n",
      "2,250 packets.\n",
      "traffic sending patterns described in the previous section become discernible after\n",
      "Accordingly, we use a context length of 238,500 tokens (106 tokens per packet) to prompt our\n",
      "model for generation, and generation length of 10,600,000 tokens, or 10,000 packets. We choose this\n",
      "generation length to balance evaluating NetSSM’s expressiveness over a sufficiently long context,\n",
      "and time efficiency (each 10,000 packet synthetic trace takes\n",
      "20 minutes to generate). Finally, we\n",
      "randomly sample 100 PCAPs from the ground truth set, extract the first 2,250 packet contexts from\n",
      "each, and use this context to generate their corresponding synthetic captures. For analysis, we\n",
      "compare the generated traces against their ground truth counterparts, truncated to a matching\n",
      "length of 10,000 packets. We present the results of this analysis in the following subsections.\n",
      "\n",
      "∼\n",
      "\n",
      "∼\n",
      "\n",
      "5.3.2 TCP Session Compliance. Prior evaluations have focused on statistical fidelity; we further\n",
      "dissect NetSSM’s ability to reproduce correct and realistic TCP state transitions through a dedicated\n",
      "protocol compliance analysis on the session level. TCP is a stateful protocol that requires accurate\n",
      "ordering and flag usage (e.g., SYN, ACK, FIN, RST), adherence to handshake procedures, and consistent\n",
      "usage of options like Maximum Segment Size (MSS) and Selective Acknowledgment (SACK). In\n",
      "real network traffic, these behaviors may deviate from strict textbook implementations due to NAT\n",
      "devices, middlebox interventions, or partial captures. By comparing NetSSM-generated traces\n",
      "with real PCAPs (i.e., ground truth), we provide an in-depth assessment of how closely NetSSM\n",
      "approximates legitimate TCP operations while capturing the natural diversity and anomalies\n",
      "observed in practice. We also compare single-flow synthetic traces generated by NetDiffusion,\n",
      "but do not apply heuristic-based post-generation corrections, as our goal is to assess the intrinsic\n",
      "capability of each generative model rather than the thoroughness of external rule-based corrections.\n",
      "The same heuristic layer could be added to NetSSM’s output, but this would obscure how effectively\n",
      "the underlying model itself enforces TCP compliance. By omitting these extra adjustments for both\n",
      "methods, we obtain a clearer evaluation of how accurately each generator handles fundamental\n",
      "TCP requirements on its own.\n",
      "\n",
      "We consider two sets of real PCAPs and two corresponding sets of synthetic PCAPs generated\n",
      "by NetSSM, separating the evaluation into single-flow and multi-flow traffic. In the multi-flow\n",
      "evaluation, each PCAP contains multiple concurrent TCP flows, simulating common scenarios\n",
      "such as video streaming sessions accompanied by DNS lookups or additional data retrievals. In\n",
      "the single-flow evaluation, each PCAP strictly contains a single TCP flow for more controlled,\n",
      "fine-grained analysis. For both real and synthetic PCAPs, we parse each trace using a custom TCP\n",
      "compliance checker that inspects flags, sequence numbers, acknowledgment numbers, and TCP\n",
      "options. Tables 3 and 4 present the results of this checker, with the first summarizing pass/fail or\n",
      "presence checks, and the second displaying aggregate counts of the encountered TCP options.\n",
      "Handshake Correctness and Basic IP Compliance. Both synthetic (NetSSM-generated) and\n",
      "real multi-flow traces show a perfect rate of three-way handshakes (100 of 100). This demonstrates\n",
      "that NetSSM accurately reproduces the necessary SYN\n",
      "ACK exchange in concurrent-\n",
      "connection environments without failing to progress any session out of the initial state. Single-flow\n",
      "traces exhibit lower but still high handshake correctness, with NetSSM achieving 517 correct\n",
      "handshakes out of 803 and the real dataset showing 777 out of 1,108. This result can be partially\n",
      "attributed to truncated single-flow captures, either in the real set or in generation. At the IP layer,\n",
      "real multi-flow traces maintain a flawless 100 of 100 IPv4 correctness, while NetSSM’s synthetic\n",
      "multi-flow traces fall marginally short at 92 of 100. Nonetheless, both single-flow datasets meet full\n",
      "\n",
      "SYN-ACK\n",
      "\n",
      "→\n",
      "\n",
      "→\n",
      "\n",
      "13\n",
      "\n",
      "\fTable 3. TCP session compliance for real/synthetic data. NetSSM reliably replicates handshakes, se-\n",
      "quence tracking, and advanced options, and captures real-world anomalies (e.g., resets, partial teardowns).\n",
      "NetDiffusion, without heuristic fixes, struggles with core TCP states.\n",
      "\n",
      "Metric\n",
      "\n",
      "Correct handshakes found\n",
      "Correct IPv4 version\n",
      "ACK progress\n",
      "SEQ progress\n",
      "FIN seen\n",
      "FIN-ACK observed\n",
      "\n",
      "Multi-Flow\n",
      "\n",
      "Gen. (NetSSM) (n =100)* Real (n=100)\n",
      "\n",
      "100.0%\n",
      "92.0%\n",
      "99.0%\n",
      "99.0%\n",
      "63.0%\n",
      "3.0%\n",
      "\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "13.0%\n",
      "7.0%\n",
      "\n",
      "Gen. (NetSSM) (n=803)\n",
      "64.4%\n",
      "100.0%\n",
      "63.1%\n",
      "60.6%\n",
      "15.3%\n",
      "3.1%\n",
      "\n",
      "Single-Flow\n",
      "\n",
      "Real (n=1108)\n",
      "70.2%\n",
      "100.0%\n",
      "69.6%\n",
      "68.9%\n",
      "5.9%\n",
      "0.5%\n",
      "\n",
      "Gen. (NetDiff.) (n=1000)†\n",
      "0.0%\n",
      "100.0%\n",
      "0.0%\n",
      "0.0%\n",
      "51.6%\n",
      "0.0%\n",
      "\n",
      "(a) Metrics reflecting correct TCP session behavior. Higher percentages indicate better compliance.\n",
      "\n",
      "Metric\n",
      "\n",
      "Unexpected SYN after estab.\n",
      "RST in established state\n",
      "Timestamps disappeared\n",
      "Conflicting flags\n",
      "ACK beyond sent data\n",
      "MSS outside handshake\n",
      "WScale outside handshake\n",
      "SAck used w/o OK\n",
      "\n",
      "Multi-Flow\n",
      "\n",
      "Single-Flow\n",
      "\n",
      "Gen. (NetSSM) (n=100)\n",
      "\n",
      "Real (n=100)\n",
      "\n",
      "Gen. (NetSSM) (n=803)\n",
      "\n",
      "Real (n=1108)\n",
      "\n",
      "Gen. (NetDiff.) (n=1000)\n",
      "\n",
      "6.0%\n",
      "6.0%\n",
      "5.0%\n",
      "57.0%\n",
      "99.0%\n",
      "22.0%\n",
      "22.0%\n",
      "58.0%\n",
      "\n",
      "0.0%\n",
      "22.0%\n",
      "10.0%\n",
      "0.0%\n",
      "100.0%\n",
      "17.0%\n",
      "17.0%\n",
      "14.0%\n",
      "\n",
      "0.0%\n",
      "35.7%\n",
      "23.2%\n",
      "0.5%\n",
      "56.3%\n",
      "3.4%\n",
      "3.4%\n",
      "1.1%\n",
      "\n",
      "0.0%\n",
      "35.7%\n",
      "24.5%\n",
      "0.0%\n",
      "46.9%\n",
      "2.0%\n",
      "2.0%\n",
      "2.9%\n",
      "\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "34.4%\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "18.7%\n",
      "\n",
      "(b) Metrics reflecting anomalies or deviations in TCP behavior. Lower percentages indicate better compliance.\n",
      "\n",
      "No post-gen. fixes applied to NetDiffusion.\n",
      "\n",
      "†\n",
      "\n",
      "* 𝑁 ≔ # of PCAPs compared.\n",
      "\n",
      "compliance (803 of 803 synthetic, 1,108 of 1,108 real), indicating that NetSSM rarely deviates from\n",
      "the IPv4 version when generating simpler one-to-one flows.\n",
      "SEQ and ACK Progression. Most synthetic and real multi-flow traces display consistent forward\n",
      "progress for SEQ and ACK (99 of 100 and 100 of 100, respectively). Similarly, in single-flows, NetSSM\n",
      "achieves solid performance (507 of 803 and 487 of 803 for ACK and SEQ, respectively), demonstrat-\n",
      "ing that it typically avoids rewinding sequence numbers or acknowledging data never observed.\n",
      "Discrepancies from 100% in single-flow sets may occur when the generative model produces abrupt\n",
      "transitions or unusual corner cases that do not align with real flow expansions.\n",
      "FIN Behavior and Teardown. Graceful TCP teardown typically involves FIN and ACK exchanges.\n",
      "We observe NetSSM’s multi-flow traces contain FIN segments in 63 of 100 cases, substantially\n",
      "higher than the 13 of 100 in the real multi-flow dataset. This may be explained by the fact that\n",
      "our real captures often end abruptly or fail to see FIN segments if preempted. However, FIN-ACK\n",
      "occurrences remain comparatively low on both sides (three of 100 for synthetic vs. seven of 100\n",
      "for real), reflecting that full bidirectional closures are often incomplete in real-world recordings.\n",
      "Single-flow captures from NetSSM also show a higher FIN presence (123 of 803) relative to the real\n",
      "set (65 of 1,108), reinforcing the idea that synthetic flows commonly seek a finite conclusion. While\n",
      "the NetSSM generator tends to inject FIN states more proactively, this does not always culminate\n",
      "in the final ACK needed for a perfectly closed connection. Nevertheless, such partial-teardown\n",
      "patterns mirror typical divergences seen in real sessions.\n",
      "RST States and Unexpected Flags. Abnormal flags such as RST or conflicting flags (e.g., FIN with\n",
      "SYN) can signal mid-connection resets, error conditions, or rare misconfigurations. In the multi-flow\n",
      "dataset, real traces display 22 of 100 instances that contain RST in the established state, whereas\n",
      "NetSSM only displays six instances. To contrast, in single-flow data, synthetic captures feature 287\n",
      "RST occurrences across 803 instances, while real single-flow instances see 396 out of 1,108, indicating\n",
      "that abrupt resets are a frequent real-world phenomenon, especially in scenarios with ephemeral or\n",
      "forcibly terminated sessions. Conflicting flags are almost exclusively found in synthetic multi-flow\n",
      "\n",
      "14\n",
      "\n",
      "\fTable 4. TCP option counts for real/synthetic data. Comparative counts of major TCP options across\n",
      "multi-flow and single-flow traces. NetSSM aligns best with real traffic as compared to NetDiffusion.\n",
      "\n",
      "TCP Option\n",
      "\n",
      "MSS\n",
      "WScale\n",
      "SAckOK\n",
      "Timestamp\n",
      "SAck\n",
      "\n",
      "Multi-Flow\n",
      "\n",
      "Single-Flow\n",
      "\n",
      "Generated (NetSSM)\n",
      "\n",
      "Real\n",
      "\n",
      "Generated (NetSSM)\n",
      "\n",
      "Real\n",
      "\n",
      "Generated (NetDiffusion)\n",
      "\n",
      "47,607\n",
      "47,402\n",
      "47,333\n",
      "786,569\n",
      "32,251\n",
      "\n",
      "10,844\n",
      "10,824\n",
      "10,824\n",
      "718,878\n",
      "75,353\n",
      "\n",
      "2,183\n",
      "2,181\n",
      "2,165\n",
      "39,993\n",
      "1,025\n",
      "\n",
      "1,654\n",
      "1,654\n",
      "1,638\n",
      "1,856,168\n",
      "39,097\n",
      "\n",
      "47,497\n",
      "66,981\n",
      "350,189\n",
      "451,055\n",
      "132,688\n",
      "\n",
      "traces (57 of 100), suggesting that NetSSM’s concurrency logic sometimes merges states or “double-\n",
      "flags” certain segments under complex conditions. Although real captures typically avoid these\n",
      "direct conflicts, their presence in synthetic flows can be beneficial for testing anomaly detection\n",
      "systems, as real devices or middleboxes can occasionally emit similarly malformed packets in\n",
      "pathological cases.\n",
      "Timestamps and Option Usage Outside the Handshake. TCP options convey various ne-\n",
      "gotiation parameters (e.g., MSS, Window Scale, SACK Permitted). Table 3 shows that timestamps\n",
      "disappear at times (five of 100 NetSSM and 10 of 100 real in multi-flow traces; 186 of 803 NetSSM\n",
      "and 271 of 1,108 real in single-flow traces), reflecting partial captures or toggled timestamp usage.\n",
      "This is understandable as real systems may fail to consistently include timestamps in every packet\n",
      "once a connection is established. Regarding MSS and WScale usage, 22 of 100 NetSSM multi-flow\n",
      "instances include these options beyond the initial handshake, a figure close to the 17 of 100 in real\n",
      "multi-flow traces. Single-flow sessions likewise display 27 of 803 instances in NetSSM and 22 of\n",
      "1,108 in real traces, respectively. Although specifying MSS or WScale outside the SYN handshake is\n",
      "often considered non-standard, both synthetic and real data confirm that such anomalies occur in\n",
      "real-world traffic. Finally, “SACK used w/o OK” emerges in 58 of 100 NetSSM multi-flow instances\n",
      "and 14 of 100 real multi-flow traces, respectively, and 9 of 803 NetSSM single-flow and 32 of\n",
      "1,108 real single-flow traces, respectively. While ideally SACK blocks should appear only if SACK\n",
      "Permitted is negotiated, real network traces also show sporadic violations.\n",
      "\n",
      "Table 4 shows that NetSSM’s multi-flow traces exhibit significantly higher raw counts for MSS,\n",
      "WScale, and SAckOK compared to their real counterparts, potentially reflecting how multi-flow\n",
      "generation amplifies the number of handshake or handshake-like packets emitted by NetSSM.\n",
      "By contrast, Timestamp usage is extremely frequent for both real and synthetic traces, although\n",
      "single-flow real PCAPs exceed synthetic usage by a large margin. These differences underline\n",
      "that captured durations or concurrency levels can strongly influence aggregated option counts,\n",
      "making perfect numeric alignment a challenging goal. Rather than detracting from fidelity, these\n",
      "variants illustrate that NetSSM can produce either condensed or expanded views of typical network\n",
      "conditions, including advanced TCP features that manifest over extended sessions.\n",
      "Comparison with NetDiffusion Single-Flow. In addition to comparing NetSSM with real\n",
      "network captures, we also report results for NetDiffusion single-flow traces generated without\n",
      "its heuristic-based post-processing. NetDiffusion achieves 100% IP version correctness but has 0%\n",
      "(0/1000) correct three-way handshakes, suggesting that the raw generation does not properly model\n",
      "TCP’s initial SYN\n",
      "ACK progression. Similarly, the lack of sequence or acknowledgment\n",
      "progression in NetDiffusion data (0% for both SEQ and ACK progress) indicates minimal adherence\n",
      "to standard TCP ordering in the absence of further heuristics. Although 51.6% of NetDiffusion’s\n",
      "single-flow traces include a FIN, none of these are followed by a FIN-ACK, reinforcing an incomplete\n",
      "teardown or the model’s inability to produce such behavior. NetDiffusion traces also show no\n",
      "\n",
      "SYN-ACK\n",
      "\n",
      "→\n",
      "\n",
      "→\n",
      "\n",
      "15\n",
      "\n",
      "\fusage of RST states but exhibits a relatively high rate of conflicting flags (34.4%), which can create\n",
      "malformed segments, whereas NetSSM traces show far fewer conflicting flags (0.5%) in the single-\n",
      "flow case. Regarding TCP options, NetDiffusion single-flow traces incorporate certain features at\n",
      "high volumes (e.g., SAckOK or WScale), yet never re-introduce them mid-connection, unlike real\n",
      "traffic or NetSSM’s generation. Note that NetDiffusion traces do show zero occurrences of missing\n",
      "timestamps. However, this is largely because NetDiffusion relies on a post-generation sampling\n",
      "method to artificially add the timestamps into the synthetic packets. In sum, NetDiffusion’s intrinsic\n",
      "model, without correction rules, highlights partial or inaccurate TCP transitions that require manual\n",
      "fixes to achieve fully valid sessions.\n",
      "\n",
      "Our findings indicate that NetSSM synthesizes a wide range of valid TCP behaviors, captures\n",
      "standard handshake procedures in both single-flow and multi-flow contexts, and includes realistic\n",
      "anomalies such as partial teardown, reset events, and sometimes extraneous flags. Although certain\n",
      "discrepancies (e.g., more frequent conflicting flags in multi-flow, or greater FIN usage in single-flow)\n",
      "appear, these divergences often mirror real-world irregularities and can even enrich test scenarios\n",
      "for anomaly detection or intrusion prevention systems. The strong alignment on core handshake\n",
      "correctness and sequence tracking underscores NetSSM’s reliability in producing fundamentally\n",
      "valid TCP sessions. Furthermore, the inclusion of advanced TCP options (MSS, WScale, SACK) and\n",
      "timestamps—sometimes used in atypical ways—suggests that NetSSM is not merely generating\n",
      "“ideal” flows but is replicating many of the complexities and edge cases intrinsic to real network\n",
      "data. NetDiffusion, on the other hand, provides IP-correct single-flow data but does not reliably\n",
      "implement core TCP states—particularly around handshakes, sequence increments, and FIN-ACK\n",
      "completions—unless supplemented by subsequent heuristic-based post generation.\n",
      "\n",
      "5.3.3 Application Dynamics. We evaluate NetSSM’s ability to generate traces that capture the\n",
      "session dynamics of application-level streaming traffic. To do so, we infer the video download\n",
      "segments found in both the ground truth Netflix traces and the synthetic traces generated by\n",
      "NetSSM and compare the distributions of their quantities and sizes. We extract these segments from\n",
      "either data by identifying the IP addresses corresponding to Netflix CDN endpoints. Specifically, we\n",
      "examine each ground truth capture’s DNS traffic and collect the returned addresses corresponding\n",
      "to Netflix domains (i.e., containing nflxvideo, netflix, nflxso, nflxext). We use these addresses\n",
      "to filter both the ground truth and generated captures for video stream content, as our generated\n",
      "traces do not contain the DNS payload to perform the same procedure. Finally, for each flow between\n",
      "a Netflix address and the localhost in a trace, we track the number of segments downloaded and\n",
      "the size of each segment (not including header bytes). We then analyze the one-to-one differences\n",
      "in Netflix CDN sender behavior between synthetic and real-world traffic pairs. Unfortunately, it is\n",
      "not possible to compare the multi-flow traffic patterns captured by NetSSM with other generators.\n",
      "This is because no other generator can model the multi-flow sessions from which we can derive our\n",
      "analysis. As such, we attempt to provide a well-founded comparison against a randomly generated\n",
      "distribution that matches the distribution shape of our ground truth data. More details on this\n",
      "process are found in the following analysis, and more verbose results of our analysis with additional\n",
      "sample comparisons are in Appendix B.\n",
      "Downloaded Segment Sizes. We first compare the distributions for segment sizes across 1) all\n",
      "downloaded segments and 2) the average segment size per Netflix CDN sender in the ground\n",
      "truth and NetSSM-generated traces. We find that across different network scenarios, NetSSM’s\n",
      "synthetic data closely aligns with the ground truth traffic. Figure 3 shows applying kernel density\n",
      "estimation (KDE) to the average segment sizes per sender and log-transformed sizes of all raw\n",
      "segment sizes, and the empirical cumulative distribution function (ECDF) for raw segment sizes for\n",
      "either trace type, for two sample Netflix sessions with different data bit rates. We choose to apply\n",
      "\n",
      "16\n",
      "\n",
      "\f(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "Fig. 3. Distribution of downloaded segment sizes. (a) KDE plots for the average downloaded segment\n",
      "sizes per sender, (b) KDE plots for the log-transformed sizes of all downloaded segments, and (c) ECDF\n",
      "plots for all downloaded segments (non-log-transformed) displayed on a log scale, for two sampled pairs of\n",
      "generated and ground truth Netflix video stream traces. The ground truth traces in (1) and (2) have data bit\n",
      "rates of 554 and 1,366 kbps, respectively. NetSSM’s distributions overlap significantly with the real data.\n",
      "\n",
      ",\n",
      "\n",
      ",\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      "(\n",
      "\n",
      ", and\n",
      "\n",
      "0.83, 0.71\n",
      "\n",
      "40.62, 39.08\n",
      "\n",
      "log transformation to the raw segment sizes to better visualize the two distributions, as both the\n",
      "ground truth and generated distributions of sizes are positively right-skewed with a higher volume\n",
      "of smaller segments (i.e., corresponding to session setup) than large segments (i.e., corresponding\n",
      "to actual video content download) present. We use log scale in the ECDF plots for the same reason.\n",
      "All KDE plots are created using a Gaussian kernel with (ground truth, generated) bandwidths of\n",
      "1.07, 0.99\n",
      "66.10, 55.85\n",
      "for sub-figures 1a, 1b, 2a, and 2b in Figure 3,\n",
      "(\n",
      "respectively, chosen using Scott’s rule of thumb in the scipy Python library [37, 46]. The (a) and\n",
      "(b) sub-figures well illustrate the similarity in downloaded segment sizes, where clear overlap exists\n",
      "between the segment sizes of the ground truth and synthetic data, even when considering instances\n",
      "of larger tail values. Similarly, in the ECDF plots, the generated traces overlap with the ground\n",
      "truth, as illustrated by similar magnitudes in the 25th, 50th, and 75th quartiles. In (1), the ground\n",
      "1829.50, 3465.00, 4195.25\n",
      "KB,\n",
      "truth and generated quartiles are\n",
      "and\n",
      "respectively. In (2), the ground truth and generated quartiles are\n",
      "and\n",
      "369.50, 3721.00, 14349.50\n",
      "KB, respectively. This provides further evidence that NetSSM generates\n",
      "(\n",
      "traces with similar size magnitudes across all segments, and with small and medium-sized segments\n",
      "are with similar absolute size, as compared to the ground truth.\n",
      "\n",
      "3048.75, 3465.00, 5792.00\n",
      ")\n",
      "(\n",
      "580.00, 4344.00, 41564.00\n",
      ")\n",
      "(\n",
      "\n",
      "(\n",
      "\n",
      "(\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      ")\n",
      "\n",
      "We perform additional analysis using standard statistical measures, including the two-sample\n",
      "Kolmogorov-Smirnov (K-S) and Anderson-Darling tests, Kullback-Leibler (KL) divergence, and the\n",
      "earth mover’s distance (EMD). The “Avg. Size” and “Raw Size” evaluations in Table 5 show the\n",
      "results of this analysis for downloaded segment sizes. We provide a basis for comparison against a\n",
      "positively right-skewed distribution of random values, to approximate the ground truth distribution\n",
      "shape. The values are randomly selected between the minimum and maximum values for each\n",
      "evaluation (i.e., average size, raw size, quantity) per ground truth trace. In the Mean Δ and Median\n",
      "Δ statistical measures, Δ ≔\n",
      "1, 100\n",
      ".\n",
      "]\n",
      "To contrast, Std. Dev. Δ ≔ 𝑚𝑒𝑑𝑖𝑎𝑛\n",
      ". We\n",
      ") −\n",
      "observe that as an aggregate, across all traces regardless of differing collection device type and\n",
      "\n",
      "𝑚𝑒𝑑𝑖𝑎𝑛𝐺𝑇𝑖 (\n",
      "|\n",
      "𝜎𝐺𝑇𝑖 (\n",
      "(|\n",
      "\n",
      "𝑅𝑎𝑛𝑑𝑜𝑚𝑖 (\n",
      "eval\n",
      "\n",
      "𝑚𝑒𝑑𝑖𝑎𝑛𝑁 𝑒𝑡𝑆𝑆𝑀\n",
      "\n",
      "𝑅𝑎𝑛𝑑𝑜𝑚𝑖 (\n",
      "\n",
      ", where 𝑖\n",
      "\n",
      "∈ [\n",
      "1, 100\n",
      "\n",
      "𝜎𝑁 𝑒𝑡𝑆𝑆𝑀\n",
      "\n",
      "where 𝑖\n",
      "\n",
      "eval\n",
      "\n",
      "eval\n",
      "\n",
      "eval\n",
      "\n",
      ") −\n",
      "\n",
      "∈ [\n",
      "\n",
      ")|)\n",
      "\n",
      ")|\n",
      "\n",
      "]\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "\n",
      "17\n",
      "\n",
      "−2000200400600Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.006DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.00.10.20.3DensityGroundTruthGenerated102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.−1000100200300Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.0060.008DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.000.050.100.15DensityGroundTruthGenerated102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.\fTable 5. Statistical comparison of real/synthetic distributions for segment size and count. Compar-\n",
      "ison of raw and average size, and segment count between ground-truth and synthetic traces. NetSSM’s\n",
      "distributions align reasonably with those of the real data.\n",
      "\n",
      "Comp. w/\n",
      "Ground\n",
      "Truth\n",
      "\n",
      "Evaluation\n",
      "\n",
      "NetSSM Avg. Size\n",
      "Random Avg. Size\n",
      "NetSSM Raw Size\n",
      "Random Raw Size\n",
      "NetSSM # Segments\n",
      "Random # Segments\n",
      "\n",
      "Statistical Measures\n",
      "\n",
      "K–S Test\n",
      "\n",
      "Anderson-Darling Test KL Divergence\n",
      "\n",
      "EMD\n",
      "\n",
      "Mean Δ Median Δ Std. Dev. Δ Stat.\n",
      "\n",
      "4.15\n",
      "5.40\n",
      "2.56\n",
      "4.43\n",
      "43.78\n",
      "30.81\n",
      "\n",
      "1.28\n",
      "3.86\n",
      "1.28\n",
      "3.58\n",
      "1.00\n",
      "13.15\n",
      "\n",
      "79.08\n",
      "103.27\n",
      "209.80\n",
      "256.95\n",
      "15.71\n",
      "6.25\n",
      "\n",
      "0.22\n",
      "1.00\n",
      "0.21\n",
      "0.80\n",
      "0.17\n",
      "0.42\n",
      "\n",
      "↓\n",
      "\n",
      "↑\n",
      "\n",
      "p-value\n",
      "\n",
      "0.82\n",
      "0.00\n",
      "0.03*\n",
      "0.00\n",
      "0.95\n",
      "0.26\n",
      "\n",
      "Stat.\n",
      "\n",
      "↓\n",
      "0.29\n",
      "11.90\n",
      "6.86\n",
      "62.74\n",
      "0.41\n",
      "2.08\n",
      "\n",
      "↑\n",
      "\n",
      "p-value\n",
      "\n",
      "0.73\n",
      "0.00\n",
      "0.01†\n",
      "0.00\n",
      "0.76\n",
      "0.35\n",
      "\n",
      "Stat. (nats)\n",
      "\n",
      "2.67\n",
      "4.62\n",
      "1.14\n",
      "2.86\n",
      "2.97\n",
      "6.83\n",
      "\n",
      "↓\n",
      "\n",
      "Dist.\n",
      "\n",
      "↓\n",
      "30.75\n",
      "44.50\n",
      "72.94\n",
      "88.74\n",
      "8.04\n",
      "15.72\n",
      "\n",
      "Values for the statistic, p-value, and distance are the median values.\n",
      "\n",
      "*Mean value is 0.19, †Mean value is 0.13.\n",
      "\n",
      "link speed, that NetSSM’s synthetic traces have lower deltas for all three statistical measures for\n",
      "both raw segment size and average segment size per CDN sender. To continue, the results for\n",
      "the statistical tests (K-S, Anderson-Darling) and information theory and distance measures (KL\n",
      "divergence, EMD) further separate NetSSM’s output from the random distribution.\n",
      "\n",
      "The two-sample K-S and Anderson-Darling tests evaluate the likelihood that data come from\n",
      "the same underlying distribution, calculating a statistic representing the largest distance found\n",
      "between ECDFs for either data (with the Anderson-Darling test giving additional weighting to\n",
      "differences in the distribution of tail values), and a p-value detailing the chance of observing a test\n",
      "statistic as extreme as the statistic if the null hypothesis that the two samples are from the same\n",
      "distribution is true. For both tests, across all evaluations, we choose a confidence interval of 0.95\n",
      "wherein we accept the null hypothesis if the p-value is greater than 0.05 and reject it otherwise.\n",
      "We find that for either test and evaluation, the median statistic value for the random distribution is\n",
      "significantly higher than for the generated traces. Specifically, in the K-S tests, the values are exactly\n",
      "and close to (0.80), the maximum possible value of 1.0. The Anderson-Darling tests’ statistics are\n",
      "similarly near an order of magnitude (or greater) larger than those of the generated. Examining the\n",
      "tests’ p-values, the null hypothesis is accepted in the average size evaluation for both tests of the\n",
      "synthetic trace (p-values 0.82 and 0.73), but rejected in either test for the random distribution with\n",
      "values of 0.00. In the raw segment size evaluation, the median p-values are near zero or zero for\n",
      "either test in our generated data and the random distribution. However, we observe mean p-values\n",
      "of 0.19 and 0.13 that could plausibly accept the null hypothesis in our generated traces for both\n",
      "tests. In contrast, the mean p-values for the random distributions remain zero for both tests.\n",
      "\n",
      "Finally, the KL divergence and EMD compare the similarity or dissimilarity between two proba-\n",
      "bility distributions, where the KL divergence is the amount of information lost when using one\n",
      "distribution to approximate the other (given that the set of possible outcomes is consistent be-\n",
      "tween distributions), and EMD is the minimum cost to transform one distribution to the other. We\n",
      "find across both size evaluations and measures that the generated traces can be more efficiently\n",
      "transformed to the ground truth than the random distribution.\n",
      "Downloaded Segment Count. We also evaluate the number of segments downloaded both in\n",
      "NetSSM’s synthetic traces and in the ground truth. Similar to evaluation of segments’ sizes, we\n",
      "find that NetSSM produces data that closely aligns with the ground truth traffic. Figure 4 shows\n",
      "the KDE plots for the log-transformed number of downloaded segments, and the ECDF plots for\n",
      "the raw number of downloaded segments, shown on a log scale. Specifically, the ground truth\n",
      "traces depicted by this figure are the same traces whose segment sizes are visualized in Figure 3,\n",
      "captured at two different data bit rates. We choose to apply log-transformation to, and show\n",
      "the number of segments in log scale for better analysis of overall patterns and visualization, as\n",
      "similar to downloaded segment sizes, the distribution for number of segments downloaded is\n",
      "\n",
      "18\n",
      "\n",
      "\f(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "Fig. 4. Distribution of downloaded segment count. KDE plots for the log-transformed number of down-\n",
      "loaded segments sent per sender, and ECDF plots for the number of downloaded segments sent per sender\n",
      "(non-log-transformed) displayed on a log scale, for two sampled pairs of generated and ground truth Netflix\n",
      "video stream traces. The ground truth traces in (1) and (2) have to data bit rates of 554 and 1,366 kbps,\n",
      "respectively. NetSSM’s distributions overlap significantly with the real data.\n",
      "\n",
      "positively right-skewed. There again exists clear overlap in the KDE plots for number of segments\n",
      "downloaded between the ground truth and synthetic data, though it appears NetSSM’s traces\n",
      "may not completely capture the tail end cases of higher volume senders. The quartile values\n",
      "from the ECDFs further support the overlap, with only small deltas between the ground truth\n",
      "4.50, 10.00, 12.00\n",
      "and generated. In scenario (1), the ground truth and generated quartiles are\n",
      ")\n",
      "segments downloaded, respectively. Similarly, in scenario (2), the ground\n",
      "and\n",
      "segments\n",
      "truth quartiles are\n",
      "downloaded, respectively.\n",
      "\n",
      ")\n",
      "7.00, 10.00, 12.75\n",
      ")\n",
      "\n",
      ", and the generated quartiles are\n",
      "\n",
      "5.75, 9.50, 11.75\n",
      ")\n",
      "\n",
      "3.25, 8.00, 10.75\n",
      "\n",
      "(\n",
      "\n",
      "(\n",
      "\n",
      "(\n",
      "\n",
      "(\n",
      "\n",
      "We next examine the “# Segments” evaluation in Table 5, compared against the same positively\n",
      "right-skewed distribution of random values, and same definitions of Δ for Mean Δ, Median Δ and\n",
      "Std. Dev. Δ respectively, as described in our analysis of download segment sizes. We observe the\n",
      "Median Δ of number of downloaded segments in the generated trace aligns more closely than the\n",
      "random distribution with the ground truth. However, the Mean Δ and Std. Dev. Δ of number of\n",
      "downloaded segments is lower when comparing the ground truth to the random distribution. This\n",
      "can be attributed to the relatively narrow range of values to be randomly selected from alongside\n",
      "the low number of CDN senders to emulate quantity of number of downloaded segments for.\n",
      "\n",
      "In the remaining statistical hypothesis tests and measures of similarity, we observe that NetSSM\n",
      "produces traces that appear to closely mimic, but not copy the ground truth traces. Using the\n",
      "same null hypothesis (two samples are from the same distribution) and 0.05 p-value threshold as\n",
      "described for segment sizes, the median p-values for both the K-S and Anderson-Darling tests are\n",
      "0.95 and 0.76 respectively (as compared to 0.26 and 0.35 from the random distribution), indicating\n",
      "strong evidence for accepting the null hypothesis that the generated and ground truth traces are\n",
      "sampled from the same underlying distribution. Similarly, the median KL divergence and EMD\n",
      "indicate that distribution for the number of segments extracted from generated traces can be more\n",
      "efficiently transformed to match that of the ground truth, than the random distribution.\n",
      "\n",
      "We find that the synthetic traces generated by NetSSM closely mimic the application dynamics\n",
      "of real-world video streaming data. NetSSM captures both the raw sizes of downloaded segments,\n",
      "and number of segments sent per CDN sender at fine-granularity, and this capability persists\n",
      "across various configurations and conditions (e.g., data bit rate and corresponding bandwidth). This\n",
      "supports our belief that NetSSM is able to more comprehensively model interactions not only\n",
      "between packets, but between multiple-flows.\n",
      "\n",
      "6 DISCUSSION, LIMITATIONS, AND FUTURE WORK\n",
      "\n",
      "Improving NetSSM. Synthetic traces generated by NetSSM do not currently include the temporal\n",
      "interarrival times (IATs) between packets in a capture. This quality is an important strength of\n",
      "\n",
      "19\n",
      "\n",
      "024No.ofSegments(LogTransformed)0.00.10.20.30.40.5DensityGroundTruthGenerated101No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.−20246No.ofSegments(LogTransformed)0.00.10.20.30.4DensityGroundTruthGenerated100101102No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.\fPCAPs that allows one to extract various temporal-dependent flow and packet-level statistics\n",
      "commonly preferred for use in downstream analysis and ML-based tasks. Generating both raw\n",
      "packet data and their corresponding IATs is a difficult task, as these values are not contained in\n",
      "packets’ contents but are assigned at capture time by packet capturing tools at the OS level. Further,\n",
      "while generating byte values produces discrete values, IATs are continuous data. An improvement on\n",
      "NetSSM should describe an architecture that produces these two data in parallel while maintaining\n",
      "the workload dependencies induced by either onto the other.\n",
      "Improving Metrics of Synthetic Data Quality. Prior work measured the quality of their synthetic\n",
      "traces by (1) calculating the statistical similarity between their generated data and ground truth\n",
      "real traffic and (2) evaluating the performance of downstream ML-for-networking models trained\n",
      "or fine-tuned on synthetic data. In our work, we present an analysis of the semantic similarity of\n",
      "NetSSM-generated traces via a case study using a model trained on Netflix video streaming traffic.\n",
      "Through this, we aimed to provide an additional perspective on what “good” synthetic traffic may\n",
      "look like by closely mimicking – but not copying – the protocol/session compliance and sending\n",
      "behavior of session nodes. However, additional case studies of different networked communication\n",
      "workloads could make this evaluation more robust and generalizable.\n",
      "Bridging semantic similarity and downstream applications Our work evaluates semantic\n",
      "similarity using metrics such as protocol/session compliance and sending behaviors. However,\n",
      "similar to how we verified the impact of statistical similarity on classification-based downstream\n",
      "tasks, future work could focus on translating semantic similarity into measurable improvements in\n",
      "downstream applications. For example, tasks such as intrusion detection and anomaly detection\n",
      "could directly benefit from enhanced semantic fidelity, as it ensures realistic and coherent traffic\n",
      "interactions. Evaluating how semantic similarity impacts these tasks would provide deeper insights\n",
      "into its practical utility and guide improvements in synthetic traffic generation.\n",
      "\n",
      "7 CONCLUSION\n",
      "\n",
      "×\n",
      "\n",
      "and 78\n",
      "\n",
      "In this paper, we presented NetSSM, a novel Mamba SSM-based raw packet generator. To our\n",
      "knowledge, NetSSM is the first network data generator capable of producing PCAPs for sessions\n",
      "comprised of multiple interleaved flows. NetSSM’s sequential, stateful architecture enables it\n",
      "to learn from, and produce sessions 8\n",
      "longer, respectively, than the current state-of-\n",
      "×\n",
      "the-art transformer-based raw packet generator. This in turn, allows it to capture key flow-state-\n",
      "dependent session events that only manifest after substantial setup. NetSSM outperforms all\n",
      "previous generators regardless of output format (i.e., summary statistics/traffic attributes of network\n",
      "data, or similar raw PCAPs) in measures of statistical similarity and as measured by the performance\n",
      "of downstream ML-for-networking models trained on NetSSM data. We additionally evaluate\n",
      "NetSSM’s traces on a new metric of semantic similarity, which aims to better reason about the\n",
      "empirical, practical similarities between NetSSM’s synthetic output and real-world network data.\n",
      "First, we find that NetSSM’s generated traces largely adhere to valid TCP behaviors required to\n",
      "reproduce complex patterns in real traffic data. Second, we find that NetSSM can faithfully capture\n",
      "complex application dynamics of multi-flow networked communication, and does so across varying\n",
      "network conditions. We are hopeful that future research towards producing synthetic network data\n",
      "will continue to evaluate both the statistical and semantic similarities of generated traces to better\n",
      "test the usefulness of this data, whether via further developments to NetSSM or other methods.\n",
      "\n",
      "20\n",
      "\n",
      "\fREFERENCES\n",
      "\n",
      "[1] Sebastian Abt and Harald Baier. 2014. Are we missing labels? A study of the availability of ground-truth in network\n",
      "security research. In 2014 third international workshop on building analysis datasets and gathering experience returns for\n",
      "security (badgers). IEEE, 40–55.\n",
      "\n",
      "[2] Fred Baker, Bill Foster, and Chip Sharp. 2004. Cisco architecture for lawful intercept in IP networks. Internet Engineering\n",
      "\n",
      "Task Force, RFC 3924 (2004).\n",
      "\n",
      "[3] Alessio Botta, Alberto Dainotti, and Antonio Pescapé. 2012. A tool for the generation of realistic network workload\n",
      "\n",
      "for emerging networking scenarios. Computer Networks 56, 15 (2012), 3531–3547.\n",
      "\n",
      "[4] Francesco Bronzino, Paul Schmitt, Sara Ayoubi, Guilherme Martins, Renata Teixeira, and Nick Feamster. 2019. Inferring\n",
      "streaming video quality from encrypted traffic: Practical models and deployment experience. Proceedings of the ACM\n",
      "on Measurement and Analysis of Computing Systems 3, 3 (2019), 1–25.\n",
      "\n",
      "[5] Tobias Bühler, Roland Schmid, Sandro Lutz, and Laurent Vanbever. 2022. Generating representative, live network\n",
      "traffic out of millions of code repositories. In Proceedings of the 21st ACM Workshop on Hot Topics in Networks. 1–7.\n",
      "[6] Lelio Campanile, Marco Gribaudo, Mauro Iacono, Fiammetta Marulli, and Michele Mastroianni. 2020. Computer\n",
      "\n",
      "network simulation with ns-3: A systematic literature review. Electronics 9, 2 (2020), 272.\n",
      "\n",
      "[7] Andrew Chu, Xi Jiang, Shinan Liu, Arjun Bhagoji, Francesco Bronzino, Paul Schmitt, and Nick Feamster. 2024. Feasibility\n",
      "of state space models for network traffic generation. In Proceedings of the 2024 SIGCOMM Workshop on Networks for AI\n",
      "Computing. 9–17.\n",
      "\n",
      "[8] ciscotrex2023 2024. The CISCO TRex Tool. https://trex-tgn.cisco.com/. [Online; accessed 31-May-2024].\n",
      "[9] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured\n",
      "State Space Duality. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine\n",
      "Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan\n",
      "Scarlett, and Felix Berkenkamp (Eds.). PMLR, 10041–10071. https://proceedings.mlr.press/v235/dao24a.html\n",
      "\n",
      "[10] François De Keersmaeker, Yinan Cao, Gorby Kabasele Ndonda, and Ramin Sadre. 2023. A Survey of Public IoT Datasets\n",
      "\n",
      "for Network Security Research. IEEE Communications Surveys & Tutorials (2023).\n",
      "[11] Let’s Encrypt. 2024. Let’s Encrypt Stats. https://letsencrypt.org/stats/ Accessed: 2024.\n",
      "[12] Sebastián García, Karel Hynek, Dmtrii Vekshin, Tomáš Čejka, and Armin Wasicek. 2021. Large Scale Measurement on\n",
      "\n",
      "the Adoption of Encrypted DNS. arXiv:2107.04436 [cs.CR] https://arxiv.org/abs/2107.04436\n",
      "\n",
      "[13] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\n",
      "\n",
      "arXiv:2312.00752 (2023).\n",
      "\n",
      "[14] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R é. 2020. Hippo: Recurrent memory with optimal\n",
      "\n",
      "polynomial projections. Advances in neural information processing systems 33 (2020), 1474–1487.\n",
      "\n",
      "[15] Albert Gu, Karan Goel, and Christopher Ré. 2021. Efficiently modeling long sequences with structured state spaces.\n",
      "\n",
      "arXiv preprint arXiv:2111.00396 (2021).\n",
      "\n",
      "[16] Thomas R Henderson, Mathieu Lacage, George F Riley, Craig Dowell, and Joseph Kopena. 2008. Network simulations\n",
      "\n",
      "with the ns-3 simulator. SIGCOMM demonstration 14, 14 (2008), 527.\n",
      "\n",
      "[17] Paul E. Hoffman and Patrick McManus. 2018. DNS Queries over HTTPS (DoH). RFC 8484. https://doi.org/10.17487/\n",
      "\n",
      "RFC8484\n",
      "\n",
      "[18] Jordan Holland, Paul Schmitt, Nick Feamster, and Prateek Mittal. 2021. New Directions in Automated Traffic Analysis\n",
      "(CCS ’21). Association for Computing Machinery, New York, NY, USA, 3366–3383. https://doi.org/10.1145/3460120.\n",
      "3484758\n",
      "\n",
      "[19] Zi Hu, Liang Zhu, John Heidemann, Allison Mankin, Duane Wessels, and Paul E. Hoffman. 2016. Specification for DNS\n",
      "\n",
      "over Transport Layer Security (TLS). RFC 7858. https://doi.org/10.17487/RFC7858\n",
      "\n",
      "[20] Xi Jiang and Noah Apthorpe. 2021. Automating Internet of Things network traffic collection with robotic arm\n",
      "\n",
      "interactions. arXiv preprint arXiv:2110.00060 (2021).\n",
      "\n",
      "[21] Xi Jiang, Shinan Liu, Aaron Gember-Jacobson, Arjun Nitin Bhagoji, Paul Schmitt, Francesco Bronzino, and Nick Feam-\n",
      "ster. 2024. NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation. Proceedings\n",
      "of the ACM on Measurement and Analysis of Computing Systems 8, 1 (2024), 1–32.\n",
      "\n",
      "[22] Xi Jiang, Shinan Liu, Saloua Naama, Francesco Bronzino, Paul Schmitt, and Nick Feamster. 2023. AC-DC: Adaptive\n",
      "\n",
      "Ensemble Classification for Network Traffic Identification. arXiv preprint arXiv:2302.11718 (2023).\n",
      "[23] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems. (1960).\n",
      "[24] Mathieu Lacage and Thomas R Henderson. 2006. Yet another network simulator. In Proceedings of the 2006 Workshop\n",
      "\n",
      "on ns-3. 12–es.\n",
      "\n",
      "[25] Jianfeng Li, Hao Zhou, Shuohan Wu, Xiapu Luo, Ting Wang, Xian Zhan, and Xiaobo Ma. 2022.\n",
      "\n",
      "Fine-\n",
      "android app fingerprinting. In 31st USENIX Security Symposium (USENIX Security 22). 1579–\n",
      "\n",
      "FOAP\n",
      "\n",
      "{\n",
      "\n",
      "{\n",
      "\n",
      "}\n",
      "\n",
      ":\n",
      "\n",
      "Grained\n",
      "1596.\n",
      "\n",
      "} {\n",
      "\n",
      "Open-World\n",
      "\n",
      "}\n",
      "\n",
      "21\n",
      "\n",
      "\f[26] Xinjie Lin, Gang Xiong, Gaopeng Gou, Zhen Li, Junzheng Shi, and Jing Yu. 2022. ET-BERT: A Contextualized Datagram\n",
      "Representation with Pre-training Transformers for Encrypted Traffic Classification. In Proceedings of the ACM Web\n",
      "Conference 2022 (WWW ’22). ACM. https://doi.org/10.1145/3485447.3512217\n",
      "\n",
      "[27] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using gans for sharing networked time series\n",
      "data: Challenges, initial promise, and open questions. In Proceedings of the ACM Internet Measurement Conference.\n",
      "464–483.\n",
      "\n",
      "[28] Shinan Liu, Francesco Bronzino, Paul Schmitt, Arjun Nitin Bhagoji, Nick Feamster, Hector Garcia Crespo, Timothy\n",
      "Coyle, and Brian Ward. 2023. LEAF: Navigating Concept Drift in Cellular Networks. Proceedings of the ACM on\n",
      "Networking 1, CoNEXT2 (2023), 1–24.\n",
      "\n",
      "[29] Shinan Liu, Tarun Mangla, Ted Shaowang, Jinjin Zhao, John Paparrizos, Sanjay Krishnan, and Nick Feamster. 2023.\n",
      "Amir: Active multimodal interaction recognition from video and network traffic in connected environments. Proceedings\n",
      "of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 1 (2023), 1–26.\n",
      "\n",
      "[30] Shinan Liu, Ted Shaowang, Gerry Wan, Jeewon Chae, Jonatas Marques, Sanjay Krishnan, and Nick Feamster. 2024.\n",
      "ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis. arXiv preprint arXiv:2402.03694 (2024).\n",
      "[31] Chaoyi Lu, Baojun Liu, Zhou Li, Shuang Hao, Haixin Duan, Mingming Zhang, Chunying Leng, Ying Liu, Zaifeng\n",
      "Zhang, and Jianping Wu. 2019. An end-to-end, large-scale measurement of dns-over-encryption: How far have we\n",
      "come?. In Proceedings of the Internet Measurement Conference. 22–35.\n",
      "\n",
      "[32] Kyle MacMillan, Tarun Mangla, James Saxon, and Nick Feamster. 2021. Measuring the performance and network\n",
      "utilization of popular video conferencing applications. In Proceedings of the 21st ACM Internet Measurement Conference.\n",
      "229–244.\n",
      "\n",
      "[33] Xuying Meng, Chungang Lin, Yequan Wang, and Yujun Zhang. 2023. Netgpt: Generative pretrained transformer for\n",
      "\n",
      "network traffic. arXiv preprint arXiv:2304.09513 (2023).\n",
      "\n",
      "[34] Anthony Moi and Nicolas Patry. 2023. HuggingFace’s Tokenizers. https://github.com/huggingface/tokenizers\n",
      "[35] Vern Paxson. 1999. Bro: a system for detecting network intruders in real-time. Computer networks 31, 23-24 (1999),\n",
      "\n",
      "2435–2463.\n",
      "\n",
      "[36] Jian Qu, Xiaobo Ma, and Jianfeng Li. 2024. TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic Analysis\n",
      "\n",
      "and Generation. arXiv preprint arXiv:2403.05822 (2024).\n",
      "\n",
      "[37] David W Scott. 2015. Multivariate density estimation: theory, practice, and visualization. John Wiley & Sons.\n",
      "[38] Ranya Sharma, Nick Feamster, and Austin Hounsel. 2022. Measuring the Availability and Response Times of Public\n",
      "\n",
      "Encrypted DNS Resolvers. arXiv:2208.04999 [cs.CR] https://arxiv.org/abs/2208.04999\n",
      "\n",
      "[39] Taveesh Sharma, Tarun Mangla, Arpit Gupta, Junchen Jiang, and Nick Feamster. 2023. Estimating WebRTC Video\n",
      "QoE Metrics Without Using Application Headers. In Proceedings of the 2023 ACM on Internet Measurement Conference\n",
      "(Montreal QC, Canada) (IMC ’23). Association for Computing Machinery, New York, NY, USA, 485–500. https:\n",
      "//doi.org/10.1145/3618257.3624828\n",
      "\n",
      "[40] shramos. 2019. shramos/pcap-splitter. https://github.com/shramos/pcap-splitter.\n",
      "[41] Pallavi Singhal, Rajeev Mathur, and Himani Vyas. 2013. State of the Art Review of Network Traffic Classification based\n",
      "\n",
      "on Machine Learning Approach. International Journal of Computer Applications 975 (2013), 8887.\n",
      "\n",
      "[42] Iraj Sodagar. 2011. The MPEG-DASH Standard for Multimedia Streaming Over the Internet. IEEE MultiMedia 18, 4\n",
      "\n",
      "(2011), 62–67. https://doi.org/10.1109/MMUL.2011.71\n",
      "\n",
      "[43] Robin Sommer and Vern Paxson. 2010. Outside the Closed World: On Using Machine Learning for Network Intrusion\n",
      "\n",
      "Detection. In 2010 IEEE Symposium on Security and Privacy. 305–316. https://doi.org/10.1109/SP.2010.25\n",
      "\n",
      "[44] Robin Sommer and Vern Paxson. 2010. Outside the closed world: On using machine learning for network intrusion\n",
      "\n",
      "detection. In 2010 IEEE symposium on security and privacy. IEEE, 305–316.\n",
      "\n",
      "[45] Matthew Swann, Joseph Rose, Gueltoum Bendiab, Stavros Shiaeles, and Nick Savage. 2021. Tools for Network Traffic\n",
      "\n",
      "Generation–A Quantitative Comparison. arXiv preprint arXiv:2109.02760 (2021).\n",
      "\n",
      "[46] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski,\n",
      "Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod\n",
      "Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, İlhan Polat, Yu\n",
      "Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\n",
      "Charles R. Harris, Anne M. Archibald, Antô nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0\n",
      "Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020),\n",
      "261–272. https://doi.org/10.1038/s41592-019-0686-2\n",
      "\n",
      "[47] Aaron Voelker, Ivana Kajić, and Chris Eliasmith. 2019. Legendre memory units: Continuous-time representation in\n",
      "\n",
      "recurrent neural networks. Advances in neural information processing systems 32 (2019).\n",
      "\n",
      "[48] Gerry Wan, Shinan Liu, Francesco Bronzino, Nick Feamster, and Zakir Durumeric. 2024. CATO: End-to-End Optimiza-\n",
      "\n",
      "tion of ML-Based Traffic Analysis Pipelines. arXiv preprint arXiv:2402.06099 (2024).\n",
      "\n",
      "22\n",
      "\n",
      "\f[49] Yucheng Yin, Zinan Lin, Minhao Jin, Giulia Fanti, and Vyas Sekar. 2022. Practical gan-based synthetic ip header trace\n",
      "\n",
      "generation using netshare. In Proceedings of the ACM SIGCOMM 2022 Conference. 458–472.\n",
      "\n",
      "[50] Shiyuan Zhang, Tong Li, Depeng Jin, and Yong Li. 2024. NetDiff: A Service-Guided Hierarchical Diffusion Model for\n",
      "\n",
      "Network Flow Trace Generation. Proceedings of the ACM on Networking 2, CoNEXT3 (2024), 1–21.\n",
      "\n",
      "23\n",
      "\n",
      "\fA COMPREHENSIVE RESULTS ON DOWNSTREAM UTILIZATION\n",
      "\n",
      "Fig. 5. Comparative ML performance across different model choices with mixed training data proportions.\n",
      "\n",
      "Fig. 6. Comparative ML performance across different model choices with mixed training data proportions\n",
      "(Skipping first packet for NetSSM-generated traces).\n",
      "\n",
      "24\n",
      "\n",
      "0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0AccuracyApp Level; Tested on Real Data0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0Service Type Level; Tested on Real Data0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0App Level; Tested on Syn DataRF, NETSSM (base)DT, NETSSM (base)SVM, NETSSM (base)RF, NETSSM (fine-tuned)DT, NETSSM (fine-tuned)SVM, NETSSM (fine-tuned)0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0AccuracyService Type Level; Tested on Syn DataRF, NetDiffusionDT, NetDiffusionSVM, NetDiffusionRF, NetShareDT, NetShareSVM, NetShare0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0AccuracyApp Level; Tested on Real Data0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0Service Type Level; Tested on Real Data0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0App Level; Tested on Syn DataNo Seed UsedRF, NETSSM (base)DT, NETSSM (base)SVM, NETSSM (base)RF, NETSSM (fine-tuned)DT, NETSSM (fine-tuned)SVM, NETSSM (fine-tuned)0.00.20.40.60.81.0Mixing Rate0.00.20.40.60.81.0AccuracyService Type Level; Tested on Syn DataRF, NetDiffusionDT, NetDiffusionSVM, NetDiffusionRF, NetShareDT, NetShareSVM, NetShare\fB ADDITIONAL VIDEO STREAMING SEGMENT RESULTS\n",
      "\n",
      "The scenarios shown in all figures below have the following ground truth data bit rates: (1) 554\n",
      "\n",
      "kbps, (2) 1,366 kbps, (3) 2,726 kbps, (4) 2,460 kbps, (5) 1,361 kbps, and (6) 1,450 kbps.\n",
      "\n",
      "B.1 Downloaded Segment Sizes\n",
      "\n",
      "(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(c)\n",
      "\n",
      "(d)\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "Fig. 7. KDE plots for downloaded segment sizes.\n",
      "\n",
      "Figure 7 shows additional visualizations for both the average downloaded segment sizes and raw\n",
      "downloaded segments sizes. Specifically: (a) KDE plots for the average downloaded segment sizes\n",
      "per sender, (b) KDE plots for the log-transformed average downloaded segment sizes per sender, (c)\n",
      "KDE plots for the sizes of all downloaded segments and (d) KDE plots for the log-transformed sizes\n",
      "of all downloaded segments.\n",
      "\n",
      "25\n",
      "\n",
      "−2000200400600Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.006DensityGroundTruthGenerated−2.50.02.55.07.5Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.000.050.100.150.200.25DensityGroundTruthGenerated0500100015002000RawSegmentSize(KB)0.0000.0020.0040.0060.008DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.00.10.20.3DensityGroundTruthGenerated−1000100200300Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.0060.008DensityGroundTruthGenerated−2.50.02.55.07.5Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.000.050.100.150.200.25DensityGroundTruthGenerated05001000RawSegmentSize(KB)0.0000.0020.0040.0060.0080.010DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.000.050.100.15DensityGroundTruthGenerated−2000200400600800Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.0060.0080.010DensityGroundTruthGenerated−2.50.02.55.07.510.0Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.000.050.100.150.20DensityGroundTruthGenerated0500100015002000RawSegmentSize(KB)0.0000.0050.0100.0150.020DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.000.050.100.15DensityGroundTruthGenerated−1000100200300Avg.SegmentSizeperCDNSender(KB)0.00000.00250.00500.00750.01000.0125DensityGroundTruthGenerated−2.50.02.55.07.5Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.000.050.100.150.20DensityGroundTruthGenerated050010001500RawSegmentSize(KB)0.00000.00250.00500.00750.01000.0125DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.00.10.20.30.4DensityGroundTruthGenerated−2000200400600Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.006DensityGroundTruthGenerated−2.50.02.55.07.510.0Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.000.050.100.150.20DensityGroundTruthGenerated010002000RawSegmentSize(KB)0.0000.0020.0040.006DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.000.050.100.150.200.25DensityGroundTruthGenerated−1000100200300Avg.SegmentSizeperCDNSender(KB)0.0000.0020.0040.0060.0080.010DensityGroundTruthGenerated−202468Avg.Seg.SizeperCDNSender(KB,LogTrans.)0.00.10.20.3DensityGroundTruthGenerated−25002505007501000RawSegmentSize(KB)0.0000.0020.0040.006DensityGroundTruthGenerated−50510RawSegmentSize(KB,LogTransformed)0.000.050.100.15DensityGroundTruthGenerated\f(1)\n",
      "\n",
      "(4)\n",
      "\n",
      "(2)\n",
      "\n",
      "(5)\n",
      "\n",
      "(3)\n",
      "\n",
      "(6)\n",
      "\n",
      "Fig. 8. ECDF plots for all downloaded segments, across different scenarios.\n",
      "\n",
      "Figure 8 shows additional visualizations for the ECDFs of downloaded segment sizes for all\n",
      "\n",
      "downloaded segments.\n",
      "\n",
      "26\n",
      "\n",
      "102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.102103104105106RawSegmentSize(LogScale)0.00.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.\fB.2 Number of Downloaded Segments\n",
      "\n",
      "Figure 9 shows additional visualizations for the number of downloaded segments. Specifically:\n",
      "(a) KDE plots for the raw number of downloaded segments per CDN and (b) KDE plots for the\n",
      "log-transformed raw number of downloaded segments sizes per CDN sender.\n",
      "\n",
      "Figure 10 shows additional visualizations for the ECDFs of the number of downloaded segments.\n",
      "\n",
      "27\n",
      "\n",
      "\f(a)\n",
      "\n",
      "(b)\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "(6)\n",
      "\n",
      "Fig. 9. KDE plots for number of downloaded segments.\n",
      "\n",
      "28\n",
      "\n",
      "−20020406080NumberofSegments0.000.020.040.06DensityGroundTruthGenerated024No.ofSegments(LogTransformed)0.00.10.20.30.40.5DensityGroundTruthGenerated−50050100150NumberofSegments0.000.010.020.030.04DensityGroundTruthGenerated−20246No.ofSegments(LogTransformed)0.00.10.20.30.4DensityGroundTruthGenerated050100NumberofSegments0.000.010.020.030.040.05DensityGroundTruthGenerated0246No.ofSegments(LogTransformed)0.00.10.20.30.4DensityGroundTruthGenerated−250255075100NumberofSegments0.000.010.020.030.04DensityGroundTruthGenerated0246No.ofSegments(LogTransformed)0.00.10.20.30.4DensityGroundTruthGenerated−250255075NumberofSegments0.000.020.040.06DensityGroundTruthGenerated024No.ofSegments(LogTransformed)0.00.10.20.30.40.5DensityGroundTruthGenerated−20020406080NumberofSegments0.000.010.020.030.04DensityGroundTruthGenerated024No.ofSegments(LogTransformed)0.00.20.40.6DensityGroundTruthGenerated\f(1)\n",
      "\n",
      "(4)\n",
      "\n",
      "(2)\n",
      "\n",
      "(5)\n",
      "\n",
      "(3)\n",
      "\n",
      "(6)\n",
      "\n",
      "Fig. 10. ECDF plots for number of downl downloaded segments, across different scenarios.\n",
      "\n",
      "29\n",
      "\n",
      "101No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.100101102No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.100101102No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.101No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.101No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.101No.ofSegments(LogScale)0.20.40.60.81.0Cuml.%ofSegmentsGroundTruthGeneratedMaxKSDist.\n"
     ]
    }
   ],
   "source": [
    "def responsing(text: str, max_results: int) -> list:\n",
    "    all_texts = []\n",
    "    url = \"http://localhost:5000/translate\"\n",
    "    data = {\n",
    "        \"q\": text,\n",
    "        \"source\": \"ru\",\n",
    "        \"target\": \"en\"\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    result = response.json()\n",
    "    text = result['translatedText']\n",
    "    print(text)\n",
    "    arxiv_results = search_arxiv(text, max_results=5)\n",
    "    print(\"arXiv results:\")\n",
    "    for res in arxiv_results:\n",
    "        all_texts.append(extract_text_from_pdf_url(res['pdf_url']))\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "print(responsing('трансформеры', 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv results:\n",
      "- Q-Insight: Understanding Image Quality via Visual Reinforcement Learning (2)\n",
      "- DSO: Aligning 3D Generators with Simulation Feedback for Physical\n",
      "  Soundness (2)\n",
      "- Self-Evolving Multi-Agent Simulations for Realistic Clinical\n",
      "  Interactions (2)\n",
      "- TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D\n",
      "  Gaussian Splatting (2)\n",
      "- Think Before Recommend: Unleashing the Latent Reasoning Power for\n",
      "  Sequential Recommendation (2)\n"
     ]
    }
   ],
   "source": [
    "arxiv_results = search_arxiv(\"transformers in NLP\", max_results=5)\n",
    "print(\"arXiv results:\")\n",
    "for res in arxiv_results:\n",
    "    print(f\"- {res['title']} ({res['published'][:1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/pdf/2503.22679v1\n"
     ]
    }
   ],
   "source": [
    "print(arxiv_results[0]['pdf_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "r\n",
      "a\n",
      "\n",
      "M\n",
      "8\n",
      "2\n",
      "\n",
      "]\n",
      "\n",
      "V\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "1\n",
      "v\n",
      "9\n",
      "7\n",
      "6\n",
      "2\n",
      "2\n",
      ".\n",
      "3\n",
      "0\n",
      "5\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Q-Insight: Understanding Image Quality via Visual\n",
      "Reinforcement Learning\n",
      "\n",
      "Weiqi Li1, Xuanyu Zhang1, Shijie Zhao2♦(cid:66)\n",
      "\n",
      ", Yabin Zhang2, Junlin Li2, Li Zhang2, Jian Zhang1 (cid:66)\n",
      "\n",
      "1 School of Electronic and Computer Engineering, Peking University\n",
      "2 ByteDance Inc.\n",
      "\n",
      "Abstract\n",
      "\n",
      "Image quality assessment (IQA) focuses on the perceptual visual quality of images,\n",
      "playing a crucial role in downstream tasks such as image reconstruction, com-\n",
      "pression, and generation. The rapid advancement of multi-modal large language\n",
      "models (MLLMs) has significantly broadened the scope of IQA, moving toward\n",
      "comprehensive image quality understanding that incorporates content analysis,\n",
      "degradation perception, and comparison reasoning beyond mere numerical scor-\n",
      "ing. Previous MLLM-based methods typically either generate numerical scores\n",
      "lacking interpretability or heavily rely on supervised fine-tuning (SFT) using\n",
      "large-scale annotated datasets to provide descriptive assessments, limiting their\n",
      "flexibility and applicability. In this paper, we propose Q-Insight, a reinforce-\n",
      "ment learning-based model built upon group relative policy optimization (GRPO),\n",
      "which demonstrates strong visual reasoning capability for image quality under-\n",
      "standing while requiring only a limited amount of rating scores and degradation\n",
      "labels. By jointly optimizing score regression and degradation perception tasks\n",
      "with carefully designed reward functions, our approach effectively exploits their\n",
      "mutual benefits for enhanced performance. Extensive experiments demonstrate\n",
      "that Q-Insight substantially outperforms existing state-of-the-art methods in both\n",
      "score regression and degradation perception tasks, while exhibiting impressive\n",
      "zero-shot generalization to comparison reasoning tasks. Code will be available at\n",
      "https://github.com/lwq20020127/Q-Insight.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Image quality assessment (IQA) is a fundamental task in computer vision, critical for optimizing\n",
      "algorithms, enhancing user experiences, and verifying content authenticity across diverse domains,\n",
      "such as image processing [38, 21, 10, 51] and AI-generated content (AIGC) [35, 7]. Traditional IQA\n",
      "methods rely heavily on hand-crafted metrics, either through reference-based comparisons [47] or\n",
      "statistical measures of natural image properties [62]. However, these approaches typically focus on\n",
      "local image characteristics and fail to comprehensively capture global visual quality, limiting their\n",
      "reliability in complex real-world scenarios. More recently, deep learning-based IQA models [44, 18]\n",
      "have emerged, utilizing neural networks to learn hierarchical image representations. Nevertheless,\n",
      "these methods struggle to face significant challenges in out-of-distribution (OOD) generalization.\n",
      "\n",
      "With recent advances in multi-modal large language models (MLLMs)[22, 1, 45], researchers have\n",
      "begun to leverage these models’ extensive world knowledge and perceptual abilities to enhance\n",
      "IQA performance and broaden its applicability[51, 58, 57, 56, 49]. Existing MLLM-based IQA\n",
      "methods generally fall into two categories: score-based methods, such as Q-Align [51] and DeQA-\n",
      "Score [56], and description-based methods, exemplified by DepictQA [58] and DepictQA-Wild [57].\n",
      "\n",
      "♦\n",
      "\n",
      "Project Lead. (cid:66): Corresponding authors, zhaoshijie.0526@bytedance.com, zhangjian.sz@pku.edu.cn.\n",
      "\n",
      "Preprint. Under review.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fFigure 1: We present the PLCC comparison between proposed Q-Insight and other IQA metrics on\n",
      "the left and list the applications of our Q-Insight on the right. Q-Insight shows significant performance\n",
      "on the out-of-domain datasets (e.g. CSIQ) compared to methods like DeQA, while simultaneously\n",
      "achieving score regression, degradation perception, and zero-shot image comparison reasoning tasks.\n",
      "\n",
      "Score-based methods transform discrete tokens into continuous quality scores, thereby improving\n",
      "adaptability but typically sacrificing interpretability and neglecting MLLMs’ intrinsic reasoning and\n",
      "descriptive capabilities. Meanwhile, simply regressing a quality score may not be meaningful\n",
      "in certain scenarios, as image quality scores are subjective, inherently biased, and lack uniform\n",
      "standards across different datasets and content types. For example, when evaluating AIGC-generated\n",
      "data, unusual visual effects and vibrant colors often imply better quality. However, for evaluating\n",
      "super-resolution results, these same features are often considered too painterly, losing the image’s\n",
      "authenticity and fidelity. Conversely, description-based methods produce detailed textual explanations\n",
      "of image degradations and comparative assessments, ensuring interpretability yet heavily depending\n",
      "on extensive textual depiction for supervised fine-tuning. Moreover, these models cannot output\n",
      "precise scores, making them unsuitable when used as loss functions or for performing accurate ranking\n",
      "of image quality. Consequently, integrating numerical scoring and descriptive reasoning into a\n",
      "unified, interpretable MLLM-based IQA framework remains an essential yet unresolved challenge.\n",
      "\n",
      "In this paper, we move towards a comprehensive understanding of image quality by addressing\n",
      "tasks such as image description, aesthetic and compositional evaluation, degradation perception, and\n",
      "comparative reasoning across images. Rather than teaching the large model “How to score images”,\n",
      "we aim to inspire it “How to reason deeply and formulate insightful perspectives on image quality\n",
      "metrics during scoring”. To this end, we resort to Group Relative Policy Optimization (GRPO) [37],\n",
      "a reinforcement learning framework inspired by DeepSeek-R1 [14]. GRPO has recently shown to be\n",
      "highly effective in large language models (LLMs). It uses heuristic reward signals to efficiently guide\n",
      "LLMs in uncovering their intrinsic reasoning capabilities, removing extensive reliance on annotated\n",
      "reasoning chains or additional value models. Recently, researchers have also successfully adapted\n",
      "GRPO to vision-language tasks, including few-shot object detection, reasoning grounding [24], and\n",
      "medical analysis [33]. In the context of image quality understanding, the introduction of GRPO\n",
      "provides at least three distinct advantages: (1) no reliance on massive textual training data, (2) strong\n",
      "generalization to OOD evaluated images, and (3) high diversity in supporting multiple tasks. These\n",
      "benefits align well with our goal of developing a generalized image quality understanding agent.\n",
      "\n",
      "Specifically, we design Q-Insight upon the GRPO framework. In our Q-Insight, we jointly optimize\n",
      "score regression and degradation perception tasks, and carefully design three reward functions: a\n",
      "verifiable score reward for the score regression task, and degradation classification and intensity\n",
      "perception rewards for the degradation perception task. Consequently, Q-Insight effectively exhibits\n",
      "robust reasoning performance in image quality assessment tasks using only limited Mean Opinion\n",
      "Scores (MOS) and degradation labels. As shown in Fig. 1, our Q-Insight shows exceptional improve-\n",
      "ments on OOD datasets, while demonstrating comprehensive quality perception and sophisticated\n",
      "reasoning abilities, such as recognizing that a slightly out-of-focus background, which might conven-\n",
      "tionally be perceived as undesirable, actually helps emphasize the main subject of the image. Our\n",
      "empirical investigation reveals that: (1) training solely with score labels results in poor perception\n",
      "of image detail degradations (e.g., JPEG compression), while jointly training with the degradation\n",
      "perception task significantly enhances the model’s sensitivity to such degradations, and (2) score\n",
      "regression and degradation perception tasks are mutually beneficial. Extensive experiments across\n",
      "score regression and degradation perception tasks demonstrate that Q-Insight consistently outper-\n",
      "forms existing model-based IQA metrics as well as SFT-driven large language models. Moreover, it\n",
      "\n",
      "2\n",
      "\n",
      "Reasoning:<think>Theimageappearstohavegoodlighting,vibrantcolors,andsharpness.Itcapturesadynamicmomentduringanequestrianevent,whichaddsinterestandcontext.Thefocusonthehorseandriderisclear,andthere'snoapparentdistortionorblur.Thebackgroundisslightlyoutoffocus,whichhelpstoemphasizethesubject.</think><answer>rating:4.38</answer>Question:Whatisoverallqualityratingofthispicture?Score RegressionDegradation PerceptionQuestion:Analyzethegivenimageanddetermineifitcontainsanydistortionsandpresentthedistortionlevel.Reasoning:<think>Theimageshowsnoclearsignsofnoise,blur,ordarkening.Themostnoticeableissueisthelackoffinedetailandpixelrepetition,whichindicatescompression—resizingorsavingtheimageinareducedformatleadstoablockyappearance.</think><answer>distortionclass:compression,severity:obvious</answer>Image Comparison Reasoning (Zero-Shot)ReferenceQuestion:Givenreferencelow-resolutionimage,ImageAandImageB.ExplainandjustifywhetherImageAorImageBisbetter.Reasoning:<think>Bothimagesarethehighresolutionversionsofthesamelow-qualityimage.However,ImageBhasslightlymoredetailedtexturesontherocksandgrass,indicatingahigherlevelofsharpness.Intermsofcontentfidelity,bothimagesmaintainthesameelementsandcompositionastheoriginal.Overall,ImageBoffersabetterbalancebetweendetailandclarity,makingitaslightlybetterchoicethanImageA.</think><answer>ImageB</answer>Image AImage B\fexhibits impressive zero-shot generalization on unseen tasks, such as image comparison reasoning,\n",
      "highlighting the robustness and versatility of our method. In summary, our contributions are:\n",
      "❑ (1) We propose Q-Insight, the first reasoning-style model specifically designed for comprehensive\n",
      "image quality understanding. Unlike previous methods that depend heavily on detailed textual\n",
      "descriptions for supervised fine-tuning (SFT), our approach achieves superior understanding capability\n",
      "using only limited mean opinion scores or degradation labels.\n",
      "❑ (2) We introduce a unified framework that jointly optimizes image quality rating and degradation\n",
      "perception, revealing mutual benefits across tasks. Within this framework, we develop three special-\n",
      "ized rewards, including verifiable score reward, degradation classification and intensity perception\n",
      "rewards, enabling the GRPO framework to effectively generalize to low-level vision applications.\n",
      "❑ (3) Extensive experiments across diverse datasets and IQA tasks demonstrate that Q-Insight\n",
      "consistently outperforms existing model-based IQA metrics as well as SFT-driven large language\n",
      "models. Moreover, it exhibits impressive zero-shot generalization on unseen tasks, such as reference-\n",
      "based image comparison reasoning, highlighting the robustness and versatility of our method.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "Score-based IQA methods include full-reference and no-reference approaches. Full-reference\n",
      "methods [47, 39, 63] assess image quality by comparing distorted images with high-quality references\n",
      "using traditional metrics (e.g., SSIM [47]) or advanced deep-learning-based metrics [3, 4, 9, 8, 12, 34]\n",
      "like LPIPS [64]. Non-reference methods evaluate quality without reference images, shifting from\n",
      "traditional handcrafted statistics [26, 27, 28, 29, 30, 36] to deep-learning-derived quality priors [17,\n",
      "18, 23, 32, 41, 71, 72, 42, 46]. Recent multi-modal large language model (MLLM)-based methods,\n",
      "such as Q-Align [51] and DeQA-Score [56], leverage MLLMs’ knowledge and perceptual abilities to\n",
      "produce accurate scores. However, they sacrifice the intrinsic descriptive capabilities of MLLMs.\n",
      "\n",
      "Description-based IQA methods utilize the foundational knowledge of MLLMs to deliver detailed\n",
      "qualitative assessments and improved interpretability [48, 50, 58, 57, 52, 5, 68, 69, 67]. For instance,\n",
      "Q-Bench [48] and Q-Instruct [50] enhance the low-level perceptual capabilities of MLLMs through\n",
      "specialized datasets and tailored evaluation strategies. Co-Instruct [52] specifically focuses on\n",
      "comparative quality assessments among multiple images. Approaches such as DepictQA [58] and\n",
      "DepictQA-Wild [57] handle both single-image and paired-image evaluations across full-reference\n",
      "and no-reference scenarios. Q-Ground [5] emphasizes a detailed visual quality analysis through\n",
      "visual grounding. However, these methods are highly dependent on extensive textual annotations for\n",
      "supervised fine-tuning, leading to considerable costs in human labor or GPT token consumption.\n",
      "\n",
      "Reinforcement learning (RL) has emerged as an effective strategy to enhance the reasoning per-\n",
      "formance of LLMs through feedback-driven refinement [6, 40, 37, 53, 55, 16, 66]. Methods like\n",
      "RLHF [31] and RLAIF [2] employ human or AI-generated feedback to refine model behavior. In\n",
      "vision-language tasks, RL has successfully been employed to align model predictions closely with\n",
      "human preferences and reduce hallucinations [43, 59, 60, 70]. Recently, DeepSeek-R1-Zero [14] intro-\n",
      "duced Group Relative Policy Optimization (GRPO)[37], leveraging rule-based rewards to strengthen\n",
      "reasoning capabilities without supervised fine-tuning. Furthermore, Visual-RFT[24] applied GRPO\n",
      "to visual grounding, and Med-R1 [33] adopted GRPO for medical reasoning tasks. R1-VL [61]\n",
      "extends GRPO through step-wise optimization for multi-modal reasoning. Distinctly, our Q-Insight\n",
      "is the first to integrate RL-based strategies into the foundational visual quality understanding model.\n",
      "It jointly trains on multiple tasks and demonstrates mutually beneficial effects among them.\n",
      "\n",
      "3 Methodology\n",
      "\n",
      "3.1 Preliminaries\n",
      "\n",
      "Group Relative Policy Optimization (GRPO) is an innovative reinforcement learning paradigm that\n",
      "has been widely used in models such as DeepSeek R1-Zero, achieving excellent results. Unlike PPO,\n",
      "which requires an explicit critic model to evaluate the performance of the policy model, GRPO directly\n",
      "computes the advantage by comparing a group of responses sampled from the policy model, greatly\n",
      "reducing the computational burden. Specifically, given a query q, GRPO samples N distinct responses\n",
      "\n",
      "3\n",
      "\n",
      "\fFigure 2: Overview of the proposed Q-Insight framework. The policy model receives queries from\n",
      "multiple tasks and generates corresponding groups of responses accompanied by explicit reasoning\n",
      "steps. Task-specific reward functions (Rscr, Rdeg, and Rlev) are then computed, and the policy model\n",
      "is subsequently optimized jointly using the multi-task group relative policy optimization algorithm.\n",
      "\n",
      "{o(1), o(2), . . . , o(N )} from the old policy πθold . Then, the method performs the corresponding actions\n",
      "and receives the respective rewards {r(1), r(2), ..., r(N )} according to the task-specific rules. By\n",
      "calculating the mean and standard deviation of the rewards, the relative advantages of each response\n",
      "can be obtained as follows:\n",
      "\n",
      "ˆA(i) =\n",
      "\n",
      "r(i) − mean({r(1), r(2) . . . , r(N )})\n",
      "std({r(1), r(2) . . . , r(N )})\n",
      "\n",
      ",\n",
      "\n",
      "(1)\n",
      "\n",
      "where ˆA(i) represents the normalized relative quality of the i-th answer. Overall, GRPO guides the\n",
      "policy model to prioritize higher-quality answers that receive higher reward values within the group.\n",
      "After obtaining the advantage ˆA(i), GRPO calculates the ratio of the probabilities of each response\n",
      "under the new policy πθnew and the old policy πθold, denoted as ratio(i). To prevent overly large\n",
      "updates to the model and stabilize training, GRPO restricts the ratio(i) to the range [1 − δ, 1 + δ]. To\n",
      "further maintain closeness to the reference distribution πref, a KL divergence penalty weighted by β\n",
      "is adopted. Finally, the optimization objective of GRPO can be formulated as follows:\n",
      "\n",
      "J (θ) = E\n",
      "\n",
      "[q∼Q,o(i)∼πθold ] {min [ratio(i), clip (ratio(i), 1 − δ, 1 + δ)] ˆA(i) − β ⋅ DKL[πθnew∣∣πref]} ,\n",
      "(2)\n",
      "\n",
      "with ratio(i) = πθnew (o(i) ∣ q) / πθold (o(i) ∣ q),\n",
      "\n",
      "where Q denotes the candidate question set, DKL denotes the KL regularization. πref is typically a\n",
      "frozen pre-trained M-LLM. In a nutshell, GRPO effectively integrates consistent policy updates and\n",
      "strong reward signals in a balanced and controlled way.\n",
      "\n",
      "3.2 Overview of Q-Insight\n",
      "\n",
      "The overall framework of Q-Insight is illustrated in Fig. 2. During training, we jointly optimize two\n",
      "tasks: score regression and degradation perception. Specifically, the multi-modal input for each task\n",
      "comprises an image paired with a task-specific question. Given these inputs, the policy model πθ\n",
      "generates groups of answers, each accompanied by explicit reasoning steps. Subsequently, each\n",
      "answer is evaluated using its corresponding reward function: Rscr for score regression, and Rdeg and\n",
      "Rlev for degradation perception. After computing rewards for each group of answers, the policy model\n",
      "is optimized jointly via the multi-task GRPO algorithm. Additionally, a KL-divergence loss is applied\n",
      "to constrain deviations between the policy model πθ and the reference model πref. During inference,\n",
      "the trained Q-Insight generates coherent reasoning processes and outputs precise answers. Further\n",
      "details regarding multi-task GRPO and data construction are provided in Sec. 3.3 and Sec. 3.4.\n",
      "\n",
      "4\n",
      "\n",
      "Policy ModelQuestion: Analyze the distortion in the image and the distortion level.Question: What is your overall rating on the quality of this picture? Reference Modelscr(0) = 3.91scr(1) = 2.57scr(N-1) = 4.10The (i)-th Reasoning Process: This appears to be a digital artwork or stylized photograph featuring vibrant colors and crisp details, though the lighting looks artificial with minimal depth and texture. Its imaginative composition suggests artistic rendering rather than photographic capture. scr(N) = 3.78The (i)-th Reasoning Process: The image shows no visible noise, and compression artifacts. However, a noticeable lack of sharpness is evident, likely due to either distant capture or slight camera movement during exposure. This results in reduced edge contrast and detail clarity.deg(0) = blurlev(0) = slightKL-LossScore Regression RewardDegradation Perception RewardMulti-Task Group Relative Policy Optimization…deg(1) = blurlev(1) = slightdeg(N-1) = blurlev(N-1) = harddeg(N) = darklev(N) = slight…Output Group of Score Regression Task Output Group of Degradation Perception TaskMulti-Task Query\fTable 1: Prompts for Different Tasks. The system prompt is shared across all tasks, while task-\n",
      "specific prompts are additionally designed for each individual task.\n",
      "\n",
      "System Prompt: A conversation between User and Assistant. The user asks a question, and\n",
      "the Assistant solves it. The assistant first thinks about the reasoning process in the mind and\n",
      "then provides the user with the answer. The reasoning process and answer are enclosed within\n",
      "<think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here\n",
      "</think><answer> answer here </answer>.\n",
      "\n",
      "Prompt for Score Regression Task: What is your overall rating on the quality of this picture?\n",
      "The rating should be a float between 1 and 5, rounded to two decimal places, with 1 representing\n",
      "very poor quality and 5 representing excellent quality. Return the final answer in JSON format\n",
      "with the following keys: \"rating\": The score.\n",
      "\n",
      "Prompt for Degradation Perception Task: Analyze the given image and determine if it contains\n",
      "any of the following distortions: \"noise\", \"compression\", \"blur\", or \"darken\". If a distortion\n",
      "is present, classify its severity as \"slight\", \"moderate\", \"obvious\", \"serious\", or \"catastrophic\".\n",
      "Return the result in JSON format with the following keys: \"distortion_class\": The detected\n",
      "distortion (or \"null\" if none). and \"severity\": The severity level (or \"null\" if none).\n",
      "\n",
      "Prompt for Reference-Based Zero-shot Comparison Reasoning: Given reference low-quality\n",
      "image: <ref_image>, Image A: <image_A> and Image B: <image_B>. Image A and Image B\n",
      "are results from two resolution methods. Explain and justify whether Image A or Image B is\n",
      "better. You should think from the following aspects: (1) Content fidelity and consistency with the\n",
      "reference image and (2) overall quality. Your answer should be \"Image A\" or \"Image B\".\n",
      "\n",
      "3.3 Multi-Task Group Relative Policy Optimization\n",
      "\n",
      "Reward design is crucial for RL-based approaches, as it allows Q-Insight to iteratively enhance its\n",
      "reasoning capabilities. Specifically, given each input data pair, the policy model πθ generates a group\n",
      "of N responses, denoted as {o(i)}\n",
      "N\n",
      "i=1. We then evaluate these response groups using our designed\n",
      "reward functions to obtain {r(i)}\n",
      "N\n",
      "i=1. This includes a general format reward shared by all tasks, along\n",
      "with task-specific rewards customized to the distinct characteristics of each task.\n",
      "\n",
      "Format reward evaluates whether the reasoning steps are properly enclosed within “<think>” and\n",
      "“</think>” tags, and the final answer is correctly enclosed within “<answer>” and “</answer>” tags.\n",
      "Additionally, the content inside “<answer>” must follow a JSON-like format, beginning with “{”,\n",
      "ending with “}”, and containing no additional “{” or “}” characters internally. Finally, a reward score\n",
      "r(i)\n",
      "fmt for i-th response of 1 is assigned if all these conditions are satisfied. Otherwise, the reward is 0.\n",
      "Rewards for score regression task. A standard way to quantify image quality is using the Mean\n",
      "Opinion Score (MOS). Inspired by DeepSeek-R1 [14], which leverages the final answers of math\n",
      "problems as a guide to promote deeper reasoning, we similarly employ MOS scores as general targets\n",
      "to encourage detailed reasoning and critical thinking when assessing image quality. Denoting the\n",
      "predicted score of the i-th response as scr(i)\n",
      "pred and the ground-truth score as scrgt, we design a verifiable\n",
      "score reward function Rscr as follows. The reward value r(i)\n",
      "scr for the i-th response is determined by:\n",
      "\n",
      "r(i)\n",
      "scr = 1\n",
      "\n",
      "if ∣ scr(i)\n",
      "\n",
      "pred\n",
      "\n",
      "− scrgt ∣< ϵ, otherwise 0,\n",
      "\n",
      "(3)\n",
      "\n",
      "where ϵ is a predefined threshold, allowing the model’s score predictions to fluctuate within an\n",
      "acceptable range rather than requiring exact accuracy. Thus, the predicted score receives a reward of\n",
      "1 if it falls within the threshold ϵ of the ground-truth MOS, and 0 otherwise.\n",
      "\n",
      "Rewards for degradation perception task. In the degradation perception task, the model is expected\n",
      "to predict both the distortion class and its corresponding level. Denoting the predicted distortion\n",
      "pred and lev(i)\n",
      "class and level of the i-th response as deg(i)\n",
      "pred respectively, we define the degradation\n",
      "classification reward as follows. The reward value r(i)\n",
      "\n",
      "deg is determined by:\n",
      "\n",
      "r(i)\n",
      "deg = 1\n",
      "\n",
      "if deg(i)\n",
      "\n",
      "pred = deggt, otherwise 0.\n",
      "\n",
      "(4)\n",
      "\n",
      "5\n",
      "\n",
      "\fThis means the predicted distortion class receives a reward of 1 if correct, and 0 otherwise. Similarly,\n",
      "the distortion-level reward r(i)\n",
      "\n",
      "lev is determined by:\n",
      "\n",
      "r(i)\n",
      "lev = 1\n",
      "\n",
      "if deg(i)\n",
      "\n",
      "pred = deggt and lev(i)\n",
      "\n",
      "pred = levgt, otherwise 0.\n",
      "\n",
      "(5)\n",
      "\n",
      "Thus, the predicted distortion level earns a reward of 1 only if both the predicted class and the level\n",
      "exactly match the ground truth; otherwise, it receives 0.\n",
      "\n",
      "Finally, the overall reward of i-th response is calculated as:\n",
      "\n",
      "r(i) = r(i)\n",
      "fmt\n",
      "\n",
      "+ 1scr ⋅ r(i)\n",
      "\n",
      "scr\n",
      "\n",
      "+ 1deg ⋅ (α1 ⋅ r(i)\n",
      "\n",
      "deg\n",
      "\n",
      "+ α2 ⋅ r(i)\n",
      "\n",
      "lev ) ,\n",
      "\n",
      "(6)\n",
      "\n",
      "where 1scr equals 1 if the score regression task is selected (and 0 otherwise), and similarly, 1deg\n",
      "equals 1 if the degradation perception task is selected (and 0 otherwise). After computing rewards for\n",
      "all generated responses {r(1), r(2), ..., r(N )}, the policy model is updated following Eqs. (1) and (2).\n",
      "\n",
      "3.4 Data Construction\n",
      "\n",
      "We construct multi-modal training data to jointly train Q-Insight on the score regression and degra-\n",
      "dation perception tasks. The prompts designed for each task are detailed in Tab. 1. Specifically,\n",
      "a general system prompt is shared across tasks, which encourages the model to explicitly output\n",
      "its reasoning process and provide structured responses. This general prompt is supplemented by\n",
      "task-specific prompts tailored for score regression and degradation perception, respectively. For the\n",
      "score regression task, the input includes a task-specific prompt and the image to be rated, with the\n",
      "Mean Opinion Score (MOS) serving as the ground-truth. In the degradation perception task, the\n",
      "input consists of a prompt and an image characterized by a specific distortion class and severity level.\n",
      "We define five distortion categories: “noise”, “blur”, “JPEG”, “darken”, and “null”, where “null”\n",
      "indicates no distortion. Each distortion type has five severity levels: “slight”, “moderate”, “obvious”,\n",
      "“serious”, and “catastrophic”. The distortion class and corresponding severity level constitute the\n",
      "ground-truth labels. Additionally, for the zero-shot comparative reasoning scenario, inputs include a\n",
      "prompt, two images to be compared, and an optional reference image.\n",
      "\n",
      "4 Experiments\n",
      "\n",
      "4.1 Experimental Setup\n",
      "\n",
      "Datasets and Metrics. For the score regression task, we use diverse IQA datasets across four\n",
      "categories: (a) in-the-wild datasets, including KonIQ [15], SPAQ [10], and LIVE-Wild [11]; (b)\n",
      "synthetic distortion datasets, including KADID [21] and CSIQ [19]; (c) model-processed distortions,\n",
      "including PIPAL [13]; and (d) AI-generated images from AGIQA [20]. Following [56], we split\n",
      "KonIQ into training and test sets, with approximately 7000 training images. Mean Opinion Scores\n",
      "(MOS) across these datasets are normalized into the range [1, 5]. The remaining datasets are\n",
      "exclusively used to evaluate the model’s out-of-distribution (OOD) generalization capability. For\n",
      "the degradation perception task, we randomly select 7000 images from DQ-495K [57] that contain\n",
      "a single distortion for training, with an additional 1000 images reserved for testing. We adopt the\n",
      "pearson linear correlation coefficient (PLCC) and spearman rank-order correlation coefficient (SRCC)\n",
      "as metrics to evaluate performance on the score regression task, following [18, 51, 56]. For the\n",
      "degradation perception task, we employ distortion class accuracy and degradation level accuracy as\n",
      "evaluation metrics.\n",
      "\n",
      "Implementation Details. We adopt Qwen-2.5-VL-7B [1] as our base model. In the GRPO algorithm,\n",
      "the generation number N is set to 8, while the weights α1 and α2 are set to 0.25 and 0.75, respectively.\n",
      "The threshold ϵ is set to 0.35. We employ AdamW [25] as the optimizer, using an initial learning rate\n",
      "−6 that linearly decays during training. The model is trained for 10 epochs with a total batch\n",
      "of 1 × 10\n",
      "size of 128. Training is completed in approximately one day using 16 NVIDIA A100 GPUs.\n",
      "\n",
      "4.2 Score Regression\n",
      "\n",
      "We first evaluate our Q-Insight on the score regression task. We compare Q-Insight with handcrafted\n",
      "methods NIQE [28] and BRISQUE [27]; non-MLLM deep-learning methods including NIMA [44],\n",
      "\n",
      "6\n",
      "\n",
      "\fTable 2: PLCC / SRCC comparison on the score regression tasks between our Q-Insight and\n",
      "other competitive IQA methods. All methods except handcrafted ones are trained on the KonIQ\n",
      "dataset. Our Q-Insight outperforms all baseline methods across nearly all benchmarks.\n",
      "\n",
      "Category\n",
      "\n",
      "Handcrafted\n",
      "\n",
      "Non-MLLM\n",
      "Deep-learning\n",
      "\n",
      "MLLM-based\n",
      "\n",
      "Methods\n",
      "NIQE [28]\n",
      "(SPL 2012)\n",
      "BRISQUE [27]\n",
      "(TIP 2012)\n",
      "NIMA [44]\n",
      "(TIP 2018)\n",
      "HyperIQA [41]\n",
      "(CVPR 2020)\n",
      "DBCNN [65]\n",
      "(TCSVT 2020)\n",
      "MUSIQ [18]\n",
      "(ICCV 2021)\n",
      "CLIP-IQA+ [46]\n",
      "(AAAI 2023)\n",
      "ManIQA [54]\n",
      "(CVPR 2022)\n",
      "C2Score [73]\n",
      "(NeurIPS 2024)\n",
      "Qwen-SFT [1]\n",
      "(Arxiv 2025)\n",
      "Q-Align [51]\n",
      "(ICML 2024)\n",
      "DeQA [56]\n",
      "(CVPR 2025)\n",
      "Q-Insight\n",
      "(Ours)\n",
      "\n",
      "KonIQ SPAQ KADID PIPAL LiveW AGIQA\n",
      "0.560\n",
      "0.533\n",
      "/0.533\n",
      "/0.530\n",
      "0.541\n",
      "0.225\n",
      "/0.497\n",
      "/0.226\n",
      "0.715\n",
      "0.896\n",
      "/0.654\n",
      "/0.859\n",
      "0.702\n",
      "0.917\n",
      "/0.640\n",
      "/0.906\n",
      "0.730\n",
      "0.884\n",
      "/0.641\n",
      "/0.875\n",
      "0.722\n",
      "0.924\n",
      "/0.630\n",
      "/0.929\n",
      "0.736\n",
      "0.909\n",
      "/0.685\n",
      "/0.895\n",
      "0.723\n",
      "0.849\n",
      "/0.636\n",
      "/0.834\n",
      "0.777\n",
      "0.923\n",
      "/0.671\n",
      "/0.910\n",
      "0.813\n",
      "0.889\n",
      "/0.739\n",
      "/0.866\n",
      "0.772\n",
      "0.941\n",
      "/0.735\n",
      "/0.940\n",
      "0.953\n",
      "0.809\n",
      "/ 0.941\n",
      "/0.729\n",
      "0.811\n",
      "0.933\n",
      "/ 0.764\n",
      "/0.916\n",
      "\n",
      "0.679\n",
      "/0.664\n",
      "0.490\n",
      "/0.406\n",
      "0.838\n",
      "/0.856\n",
      "0.791\n",
      "/0.788\n",
      "0.812\n",
      "/0.806\n",
      "0.868\n",
      "/0.863\n",
      "0.866\n",
      "/0.864\n",
      "0.768\n",
      "/0.758\n",
      "0.867\n",
      "/0.860\n",
      "0.874\n",
      "/0.875\n",
      "0.886\n",
      "/0.887\n",
      "0.895\n",
      "/0.896\n",
      "0.907\n",
      "/ 0.905\n",
      "\n",
      "0.195\n",
      "/0.161\n",
      "0.267\n",
      "/0.232\n",
      "0.390\n",
      "/0.399\n",
      "0.410\n",
      "/0.403\n",
      "0.384\n",
      "/0.381\n",
      "0.431\n",
      "/0.431\n",
      "0.427\n",
      "/0.419\n",
      "0.457\n",
      "/0.452\n",
      "0.354\n",
      "/0.342\n",
      "0.473\n",
      "/0.442\n",
      "0.403\n",
      "/0.419\n",
      "0.472\n",
      "/ 0.478\n",
      "0.486\n",
      "/0.474\n",
      "\n",
      "0.493\n",
      "/0.449\n",
      "0.361\n",
      "/0.313\n",
      "0.814\n",
      "/0.771\n",
      "0.772\n",
      "/0.749\n",
      "0.773\n",
      "/0.755\n",
      "0.789\n",
      "/0.830\n",
      "0.832\n",
      "/0.805\n",
      "0.849\n",
      "/0.832\n",
      "0.786\n",
      "/0.772\n",
      "0.734\n",
      "/0.728\n",
      "0.853\n",
      "/0.860\n",
      "0.892\n",
      "/ 0.879\n",
      "0.893\n",
      "/0.865\n",
      "\n",
      "0.468\n",
      "/0.405\n",
      "0.429\n",
      "/0.356\n",
      "0.532\n",
      "/0.535\n",
      "0.506\n",
      "/0.468\n",
      "0.497\n",
      "/0.484\n",
      "0.575\n",
      "/0.556\n",
      "0.653\n",
      "/0.654\n",
      "0.499\n",
      "/0.465\n",
      "0.500\n",
      "/0.453\n",
      "0.668\n",
      "/0.663\n",
      "0.674\n",
      "/0.684\n",
      "0.694\n",
      "/0.687\n",
      "0.742\n",
      "/0.736\n",
      "\n",
      "CSIQ\n",
      "0.718\n",
      "/0.628\n",
      "0.740\n",
      "/0.556\n",
      "0.695\n",
      "/0.649\n",
      "0.752\n",
      "/0.717\n",
      "0.586\n",
      "/0.572\n",
      "0.771\n",
      "/0.710\n",
      "0.772\n",
      "/0.719\n",
      "0.623\n",
      "/0.627\n",
      "0.735\n",
      "/0.705\n",
      "0.674\n",
      "/0.650\n",
      "0.671\n",
      "/0.737\n",
      "0.787\n",
      "/0.744\n",
      "0.870\n",
      "/ 0.824\n",
      "\n",
      "AVG.\n",
      "0.521\n",
      "/0.481\n",
      "0.436\n",
      "/0.369\n",
      "0.697\n",
      "/0.675\n",
      "0.693\n",
      "/0.667\n",
      "0.667\n",
      "/0.645\n",
      "0.726\n",
      "/0.707\n",
      "0.742\n",
      "/0.720\n",
      "0.681\n",
      "/0.658\n",
      "0.706\n",
      "/0.673\n",
      "0.732\n",
      "/0.709\n",
      "0.705\n",
      "/0.752\n",
      "0.786\n",
      "/0.765\n",
      "0.806\n",
      "/ 0.783\n",
      "\n",
      "Figure 3: Score rating and explanation results of our Q-Insight. Q-Insight is capable of recognizing\n",
      "text, analyzing the lighting and shading conditions of an image, and understanding its composition.\n",
      "\n",
      "HyperIQA [41], DBCNN [65], MUSIQ [18], CLIP-IQA+ [46], and ManIQA [54]; and recent MLLM-\n",
      "based methods such as C2Score [73], Q-Align [51], DeQA-Score [56], and a supervised fine-tuned\n",
      "Qwen [1]. For a fair comparison, all methods (except handcrafted ones) are trained on the KonIQ\n",
      "dataset, and all MLLM-based methods utilize approximately 7B parameters. The comparison results\n",
      "in terms of PLCC and SRCC between Q-Insight and other IQA methods are presented in Tab. 2.\n",
      "Compared with the state-of-the-art method DeQA-Score, our Q-Insight performs slightly worse on the\n",
      "\n",
      "7\n",
      "\n",
      "Q-InsightQuestion: What is your overall rating on the quality of this picture? ......User<think>The image shows a clear view of the neon sign for Amoeba Music, which is well-lit against a dark background. The text is legible, and the colors are vibrant, indicating that the photo has been taken under good lighting conditions. However, there's no detail visible beyond the sign itself, and the background is not fully captured. The focus seems to be on the sign rather than the surrounding area. </think><answer> rating: 3.70 </answer>Ground Truth: 3.63Question: What is your overall rating on the quality of this picture? ......User<think>The image appears to be blurry and unfocused, which is a significant issue for image quality. It's a close-up shot of a dog, but the lack of sharpness and detail makes it difficult to appreciate the subject fully. The background is also out of focus, which is distracting. The lighting seems adequate, but the composition isn't ideal.</think><answer> rating: 2.60 </answer>Ground Truth: 2.31Q-Insight\fTable 3: Distortion prediction accuracy (Deg. Acc.) and degradation level accuracy (Lev. Acc.)\n",
      "comparison between our Q-Insight and AgenticIR [74]. Our method outperforms AgenticIR across\n",
      "all degradations, especially in Noise and JPEG Compression.\n",
      "\n",
      "Method\n",
      "\n",
      "AgenticIR [74] (ICLR 2025)\n",
      "\n",
      "Q-Insight (Ours)\n",
      "\n",
      "Metrics\n",
      "Deg. Acc.\n",
      "Lev. Acc.\n",
      "Deg. Acc.\n",
      "Lev. Acc.\n",
      "\n",
      "Noise\n",
      "0.4646\n",
      "0.1858\n",
      "1.0000\n",
      "0.5973\n",
      "\n",
      "Blur\n",
      "0.8390\n",
      "0.3219\n",
      "0.9756\n",
      "0.4438\n",
      "\n",
      "JPEG\n",
      "0.0135\n",
      "0.0000\n",
      "1.0000\n",
      "0.5541\n",
      "\n",
      "Darken\n",
      "0.7478\n",
      "0.2611\n",
      "0.9027\n",
      "0.3230\n",
      "\n",
      "Null\n",
      "0.9339\n",
      "-\n",
      "0.7603\n",
      "-\n",
      "\n",
      "Average\n",
      "0.5998\n",
      "0.1922\n",
      "0.9277\n",
      "0.4796\n",
      "\n",
      "Table 4: Ablation study on the score regression task between multi-task and single-task training.\n",
      "Ours with joint-training significantly outperforms score-only training on PLCC / SRCC metrics.\n",
      "\n",
      "Method\n",
      "Ours\n",
      "(Score-Only)\n",
      "Ours\n",
      "(Joint-Training)\n",
      "\n",
      "KonIQ\n",
      "0.918\n",
      "/ 0.895\n",
      "0.933\n",
      "/ 0.916\n",
      "\n",
      "SPAQ KADID PIPAL\n",
      "0.458\n",
      "0.702\n",
      "0.903\n",
      "/ 0.435\n",
      "/ 0.702\n",
      "/ 0.899\n",
      "0.486\n",
      "0.742\n",
      "0.907\n",
      "/ 0.474\n",
      "/ 0.736\n",
      "/ 0.905\n",
      "\n",
      "LiveW AGIQA\n",
      "0.816\n",
      "0.870\n",
      "/ 0.766\n",
      "/ 0.839\n",
      "0.893\n",
      "0.811\n",
      "/ 0.865\n",
      "/ 0.764\n",
      "\n",
      "CSIQ\n",
      "0.685\n",
      "/ 0.640\n",
      "0.870\n",
      "/ 0.824\n",
      "\n",
      "AVG.\n",
      "0.765\n",
      "/ 0.739\n",
      "0.806\n",
      "/ 0.783\n",
      "\n",
      "Table 5: Ablation study on the degradation perception task between multi-task and single-task\n",
      "training. Jointly training with score regression improves the accuracy of degradation perception.\n",
      "\n",
      "Method\n",
      "\n",
      "Ours (Degradation-Only)\n",
      "\n",
      "Ours (Joint-Training)\n",
      "\n",
      "Metrics\n",
      "Deg. Acc.\n",
      "Lev. Acc.\n",
      "Deg. Acc.\n",
      "Lev. Acc.\n",
      "\n",
      "Noise\n",
      "0.9867\n",
      "0.4343\n",
      "1.0000\n",
      "0.5973\n",
      "\n",
      "Blur\n",
      "0.9268\n",
      "0.3951\n",
      "0.9756\n",
      "0.4438\n",
      "\n",
      "JPEG\n",
      "0.9685\n",
      "0.3108\n",
      "1.0000\n",
      "0.5541\n",
      "\n",
      "Darken\n",
      "0.8805\n",
      "0.2567\n",
      "0.9027\n",
      "0.3230\n",
      "\n",
      "Null\n",
      "0.5702\n",
      "-\n",
      "0.7603\n",
      "-\n",
      "\n",
      "Average\n",
      "0.8960\n",
      "0.3492\n",
      "0.9277\n",
      "0.4796\n",
      "\n",
      "in-domain KonIQ dataset. However, on out-of-distribution (OOD) datasets, Q-Insight consistently\n",
      "outperforms all baseline methods across nearly all benchmarks, achieving approximately 0.2\n",
      "improvements in both PLCC and SRCC. This demonstrates the effectiveness and strong generalization\n",
      "capability of our approach. Fig. 3 shows two illustrative cases of the reasoning process in the score\n",
      "regression task. Notably, our Q-Insight does not merely output a numerical score; it also provides\n",
      "a comprehensive and detailed reasoning process. In the first example (top of Fig. 3), Q-Insight\n",
      "successfully recognizes the text on the neon sign and carefully analyzes details such as lighting\n",
      "conditions. Similarly, the second example (bottom of Fig. 3) highlights Q-Insight’s capability in\n",
      "understanding image composition. This further demonstrates that Q-Insight moves beyond merely\n",
      "regressing scores, critically analyzing image quality issues from multiple perspectives, and advancing\n",
      "toward a more comprehensive understanding of image quality.\n",
      "\n",
      "4.3 Distortion Perception\n",
      "\n",
      "We further evaluate Q-Insight on the distortion perception task, comparing it with AgenticIR [74],\n",
      "which fine-tunes an MLLM to perform a similar distortion prediction function. The comparative\n",
      "results are presented in Tab. 3. Notably, AgenticIR requires sequential queries for each possible\n",
      "distortion type, whereas Q-Insight identifies distortion types using only a single query. Q-Insight\n",
      "consistently outperforms AgenticIR across nearly all distortion categories, resulting in significantly\n",
      "higher average accuracy. However, for the “null” category (no distortion), our performance is slightly\n",
      "lower than AgenticIR, possibly because the reasoning mechanism of Q-Insight is more sensitive to\n",
      "the presence of potential distortions. Overall, these results highlight Q-Insight’s ability to efficiently\n",
      "and accurately perceive image distortions through a concise and unified reasoning framework.\n",
      "\n",
      "4.4 Ablation Studies\n",
      "\n",
      "Effect of multi-task training. To demonstrate the effectiveness of multi-task training, we compare\n",
      "our jointly trained Q-Insight with two single-task variants, trained separately on each task. The\n",
      "comparison results are presented in Tabs. 4 and 5. As shown in Tab. 4, the jointly trained Q-Insight\n",
      "significantly outperforms the score-only variant on nearly all datasets, particularly those with synthetic\n",
      "\n",
      "8\n",
      "\n",
      "\fFigure 4: Subjective ablation comparison between joint multi-task training and w/o joint training\n",
      "on the explanation of image scoring. With joint training, our method can better perceive degradation\n",
      "cues in images (such as pixelated appearance), thereby improving the accuracy of quality assessment.\n",
      "\n",
      "Figure 5: Image comparison reasoning results of our Q-Insight and DepictQA [58, 57]. Q-Insight\n",
      "outperforms DepictQA in comprehensive content understanding and accurate degradation perception.\n",
      "\n",
      "distortions (KADID, CSIQ) and model-processed distortions (PIPAL). This clearly indicates that\n",
      "incorporating the degradation perception task benefits the score regression task. Fig. 4 further\n",
      "illustrates the importance of multi-task training: Q-Insight can better perceive detailed degradations\n",
      "such as pixel-level artifacts, thus enhancing overall accuracy in quality assessment. Similarly, Tab. 5\n",
      "demonstrates that in the degradation perception task, the jointly trained model outperforms the\n",
      "degradation-only variant across all distortion categories. This suggests that the score regression task\n",
      "also positively contributes to the degradation perception capability. These results confirm the mutual\n",
      "benefit and effectiveness of our multi-task training approach. Furthermore, this clearly demonstrates\n",
      "that the visual quality understanding potential of MLLMs can be significantly enhanced through\n",
      "carefully designed training tasks and objectives.\n",
      "\n",
      "4.5 Zero-Shot Image Comparison Reasoning\n",
      "\n",
      "Our Q-Insight effectively generalizes to zero-shot image comparison reasoning tasks, covering\n",
      "both reference-based and non-reference-based scenarios, as illustrated in Figs. 1 and 5. In Fig. 1,\n",
      "Q-Insight conducts a reference-based comparison, where the reference image is of lower quality,\n",
      "and Images A and B are outputs from different super-resolution methods. Q-Insight compares\n",
      "these two images from both fidelity and overall quality perspectives, demonstrating its potential\n",
      "utility in image enhancement tasks. In Fig. 5, Q-Insight surpasses DepictQA [58, 57] in terms of\n",
      "\n",
      "9\n",
      "\n",
      "pw/o Joint Training<think>The image has good lighting and vibrant colors, which contribute to its overall appeal. However, there is noticeable noise present, which can detract from the image quality. The horse and rider appear slightly blurry, indicating motion or a lower resolution. Despite these issues, the image still maintains a certain level of clarity and detail, especially in the background. Therefore, I would rate the image quality as moderate.</think><answer> rating: 3.50 </answer>Question: What's your overall rating on the quality of this picture? The rating should be a float between 1 and 5, rounded to two decimal places, with 1 representing very poor quality and 5 representing excellent quality.pJoint Training<think>The image appears to be a digital illustration rather than a photograph, which can affect its quality. It has a somewhat pixelated appearance, suggesting it might not be high resolution. The colors are vibrant but the details are not sharp. The composition is interesting, capturing an action scene well, but the lack of detail and clarity brings down the overall quality.</think><answer> rating: 2.70 </answer>Question: Assess the visual quality of both the Image A and the Image B, explain and justify which one is better considering composition and degradation. Your answer should be 'Image A' or 'Image B'.User<think>Both images depict a close-up view of what appears to be fur, possibly from an animal. Image A has noticeable blur and lacks sharpness, making it difficult to discern fine details. The composition seems to focus on the texture of the fur, but the lack of clarity detracts from its aesthetic appeal.Image B, on the other hand, shows a clearer depiction of the fur's texture and detail. It has less blur and more defined edges, suggesting that it has been taken under better lighting conditions or with a higher resolution camera. The composition is similar to Image A, focusing on the same subject matter.</think><answer> Image B </answer>Ground Truth: Image BQ-InsightImage AImage BThe images depict a seal with a snowy background. In Image A, the contrast is slightly enhanced, making the seal's fur and features more pronounced. This slight increase in contrast marginally improves the visual appeal by making the image pop more without significantly altering the content. Image B is slightly brightened, which can wash out details and make the image appear less natural. Comparing the two, Image A retains more of the original content's detail and depth, making it the better-quality image between the two.Ground Truth: Image BDepictQA\fcomprehensive content understanding and accurate degradation perception. Importantly, Q-Insight\n",
      "achieves this without relying on specialized training on large-scale textual datasets, highlighting its\n",
      "strong generalization ability enabled by the GRPO-based framework and multi-task training strategy.\n",
      "\n",
      "5 Conclusion\n",
      "\n",
      "In this paper, we introduce Q-Insight, a novel GRPO-based model for comprehensive image quality\n",
      "understanding. It jointly optimizes score regression and degradation perception tasks using only a\n",
      "limited amount of labeled data. Unlike traditional methods that rely on extensive textual annotations or\n",
      "purely numerical scoring, our framework combines numerical accuracy with interpretative reasoning,\n",
      "significantly improving the perceptual analysis capabilities of image quality models. Extensive\n",
      "experiments show that Q-Insight consistently outperforms existing state-of-the-art methods across\n",
      "various datasets and tasks, demonstrating impressive zero-shot generalization to image comparison\n",
      "reasoning. Looking ahead, Q-Insight can extend its capabilities to a wide range of tasks, such as image\n",
      "aesthetic assessments, and serve as a powerful discriminative signal to improve image enhancement\n",
      "models. As a unified model for scoring, perception, comparison, and reasoning, Q-Insight can act\n",
      "as a central hub, coordinating image reconstruction tools and providing valuable insights into the\n",
      "enhancement process. This integrated and automated system has the potential to revolutionize image\n",
      "quality understanding and enhancement, providing a unified solution that can transform how image\n",
      "quality is evaluated, improved, and applied across various fields.\n",
      "\n",
      "References\n",
      "\n",
      "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\n",
      "Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923,\n",
      "2025.\n",
      "\n",
      "[2] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\n",
      "Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:\n",
      "Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.\n",
      "\n",
      "[3] Sebastian Bosse, Dominique Maniry, Klaus-Robert Müller, Thomas Wiegand, and Wojciech\n",
      "Samek. Deep neural networks for no-reference and full-reference image quality assessment.\n",
      "IEEE Transactions on image processing, 27(1):206–219, 2017.\n",
      "\n",
      "[4] Yue Cao, Zhaolin Wan, Dongwei Ren, Zifei Yan, and Wangmeng Zuo. Incorporating semi-\n",
      "supervised and positive-unlabeled learning for boosting full reference image quality assessment.\n",
      "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n",
      "5851–5861, 2022.\n",
      "\n",
      "[5] Chaofeng Chen, Sensen Yang, Haoning Wu, Liang Liao, Zicheng Zhang, Annan Wang, Wenxiu\n",
      "Sun, Qiong Yan, and Weisi Lin. Q-ground: Image quality grounding with large multi-modality\n",
      "models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages\n",
      "486–495, 2024.\n",
      "\n",
      "[6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\n",
      "reinforcement learning from human preferences. Advances in neural information processing\n",
      "systems, 30, 2017.\n",
      "\n",
      "[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\n",
      "\n",
      "Advances in neural information processing systems, 34:8780–8794, 2021.\n",
      "\n",
      "[8] Keyan Ding, Yi Liu, Xueyi Zou, Shiqi Wang, and Kede Ma. Locally adaptive structure and\n",
      "texture similarity for image quality assessment. In Proceedings of the 29th ACM International\n",
      "Conference on multimedia, pages 2483–2491, 2021.\n",
      "\n",
      "[9] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli. Image quality assessment: Unifying\n",
      "structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence,\n",
      "44(5):2567–2581, 2020.\n",
      "\n",
      "[10] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment\n",
      "of smartphone photography. In Proceedings of the IEEE/CVF conference on computer vision\n",
      "and pattern recognition, pages 3677–3686, 2020.\n",
      "\n",
      "10\n",
      "\n",
      "\f[11] Deepti Ghadiyaram and Alan C Bovik. Live in the wild image quality challenge database.\n",
      "Online: http://live. ece. utexas. edu/research/ChallengeDB/index. html [Mar, 2017], 2015.\n",
      "\n",
      "[12] Abhijay Ghildyal and Feng Liu. Shift-tolerant perceptual similarity metric.\n",
      "\n",
      "In European\n",
      "\n",
      "Conference on Computer Vision, pages 91–107. Springer, 2022.\n",
      "\n",
      "[13] Haoming GU, Jinjinand Cai, Haoyu Chen, Xiaoxing Ye, Ren Jimmy S, and Chao Dong. Pipal:\n",
      "a large-scale image quality assessment dataset for perceptual image restoration. In European\n",
      "Conference on Computer Vision, pages 633–651. Springer, 2020.\n",
      "\n",
      "[14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n",
      "Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\n",
      "llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n",
      "\n",
      "[15] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid\n",
      "database for deep learning of blind image quality assessment. IEEE Transactions on Image\n",
      "Processing, 29:4041–4056, 2020.\n",
      "\n",
      "[16] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jia-\n",
      "jun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint\n",
      "arXiv:2409.12186, 2024.\n",
      "\n",
      "[17] Le Kang, Peng Ye, Yi Li, and David Doermann. Convolutional neural networks for no-reference\n",
      "image quality assessment. In Proceedings of the IEEE conference on computer vision and\n",
      "pattern recognition, pages 1733–1740, 2014.\n",
      "\n",
      "[18] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image\n",
      "quality transformer. In Proceedings of the IEEE/CVF international conference on computer\n",
      "vision, pages 5148–5157, 2021.\n",
      "\n",
      "[19] Eric C Larson and Damon M Chandler. Most apparent distortion: full-reference image quality\n",
      "assessment and the role of strategy. Journal of electronic imaging, 19(1):011006–011006, 2010.\n",
      "[20] Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo Min, Xiaohong Liu, Guangtao\n",
      "Zhai, and Weisi Lin. Agiqa-3k: An open database for ai-generated image quality assessment.\n",
      "IEEE Transactions on Circuits and Systems for Video Technology, 34(8):6833–6846, 2023.\n",
      "[21] Hanhe Lin, Vlad Hosu, and Dietmar Saupe. Kadid-10k: A large-scale artificially distorted iqa\n",
      "database. In 2019 Eleventh International Conference on Quality of Multimedia Experience\n",
      "(QoMEX), pages 1–3. IEEE, 2019.\n",
      "\n",
      "[22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances\n",
      "\n",
      "in neural information processing systems, 36:34892–34916, 2023.\n",
      "\n",
      "[23] Xialei Liu, Joost Van De Weijer, and Andrew D Bagdanov. Rankiqa: Learning from rankings\n",
      "for no-reference image quality assessment. In Proceedings of the IEEE international conference\n",
      "on computer vision, pages 1040–1049, 2017.\n",
      "\n",
      "[24] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and\n",
      "Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785,\n",
      "2025.\n",
      "\n",
      "[25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\n",
      "\n",
      "arXiv:1711.05101, 2017.\n",
      "\n",
      "[26] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and Ming-Hsuan Yang. Learning a no-reference\n",
      "quality metric for single-image super-resolution. Computer Vision and Image Understanding,\n",
      "158:1–16, 2017.\n",
      "\n",
      "[27] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality\n",
      "assessment in the spatial domain. IEEE Transactions on image processing, 21(12):4695–4708,\n",
      "2012.\n",
      "\n",
      "[28] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a “completely blind” image\n",
      "\n",
      "quality analyzer. IEEE Signal processing letters, 20(3):209–212, 2012.\n",
      "\n",
      "[29] Anush Krishna Moorthy and Alan Conrad Bovik. A two-step framework for constructing blind\n",
      "\n",
      "image quality indices. IEEE Signal processing letters, 17(5):513–516, 2010.\n",
      "\n",
      "[30] Anush Krishna Moorthy and Alan Conrad Bovik. Blind image quality assessment: From natural\n",
      "scene statistics to perceptual quality. IEEE transactions on Image Processing, 20(12):3350–\n",
      "3364, 2011.\n",
      "\n",
      "11\n",
      "\n",
      "\f[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n",
      "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\n",
      "follow instructions with human feedback. Advances in neural information processing systems,\n",
      "35:27730–27744, 2022.\n",
      "\n",
      "[32] Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu, and Yuan Zhang. Blind predicting similar\n",
      "quality map for image quality assessment. In Proceedings of the IEEE conference on computer\n",
      "vision and pattern recognition, pages 6373–6382, 2018.\n",
      "\n",
      "[33] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen,\n",
      "Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability\n",
      "of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634,\n",
      "2025.\n",
      "\n",
      "[34] Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep Sen. Pieapp: Perceptual image-error\n",
      "assessment through pairwise preference. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 1808–1817, 2018.\n",
      "\n",
      "[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition, pages 10684–10695, 2022.\n",
      "\n",
      "[36] Michele A Saad, Alan C Bovik, and Christophe Charrier. Blind image quality assessment: A\n",
      "natural scene statistics approach in the dct domain. IEEE transactions on Image Processing,\n",
      "21(8):3339–3352, 2012.\n",
      "\n",
      "[37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\n",
      "Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\n",
      "reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n",
      "\n",
      "[38] H Sheikh. Live image quality assessment database release 2.\n",
      "\n",
      "http://live. ece. utexas.\n",
      "\n",
      "edu/research/quality, 2005.\n",
      "\n",
      "[39] Hamid R Sheikh and Alan C Bovik. Image information and visual quality. IEEE Transactions\n",
      "\n",
      "on image processing, 15(2):430–444, 2006.\n",
      "\n",
      "[40] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur\n",
      "Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of\n",
      "go without human knowledge. nature, 550(7676):354–359, 2017.\n",
      "\n",
      "[41] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang.\n",
      "Blindly assess image quality in the wild guided by a self-adaptive hyper network. In Proceedings\n",
      "of the IEEE/CVF conference on computer vision and pattern recognition, pages 3667–3676,\n",
      "2020.\n",
      "\n",
      "[42] Simeng Sun, Tao Yu, Jiahua Xu, Wei Zhou, and Zhibo Chen. Graphiqa: Learning distortion\n",
      "graph representations for blind image quality assessment. IEEE Transactions on Multimedia,\n",
      "25:2912–2925, 2022.\n",
      "\n",
      "[43] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang\n",
      "Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models\n",
      "with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023.\n",
      "\n",
      "[44] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE transactions on\n",
      "\n",
      "image processing, 27(8):3998–4011, 2018.\n",
      "\n",
      "[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\n",
      "thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\n",
      "and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n",
      "\n",
      "[46] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look\n",
      "and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37,\n",
      "pages 2555–2563, 2023.\n",
      "\n",
      "[47] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\n",
      "from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–\n",
      "612, 2004.\n",
      "\n",
      "[48] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi\n",
      "Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: A benchmark for general-purpose\n",
      "foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023.\n",
      "\n",
      "12\n",
      "\n",
      "\f[49] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin\n",
      "Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visual\n",
      "abilities for multi-modality foundation models. In Proceedings of the IEEE/CVF conference on\n",
      "computer vision and pattern recognition, pages 25490–25500, 2024.\n",
      "\n",
      "[50] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin\n",
      "Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, et al. Q-instruct: Improving low-level visual\n",
      "abilities for multi-modality foundation models. In Proceedings of the IEEE/CVF conference on\n",
      "computer vision and pattern recognition, pages 25490–25500, 2024.\n",
      "\n",
      "[51] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan\n",
      "Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring\n",
      "via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023.\n",
      "\n",
      "[52] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li,\n",
      "Annan Wang, Wenxiu Sun, Qiong Yan, et al. Towards open-ended visual quality comparison.\n",
      "In European Conference on Computer Vision, pages 360–377. Springer, 2024.\n",
      "\n",
      "[53] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng\n",
      "Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward\n",
      "mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024.\n",
      "[54] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang,\n",
      "and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality\n",
      "In Proceedings of the IEEE/CVF conference on computer vision and pattern\n",
      "assessment.\n",
      "recognition, pages 1191–1200, 2022.\n",
      "\n",
      "[55] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan\n",
      "Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language\n",
      "models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024.\n",
      "\n",
      "[56] Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language\n",
      "models to regress accurate image quality scores using score distribution. arXiv preprint\n",
      "arXiv:2501.11561, 2025.\n",
      "\n",
      "[57] Zhiyuan You, Jinjin Gu, Zheyuan Li, Xin Cai, Kaiwen Zhu, Chao Dong, and Tianfan Xue.\n",
      "Descriptive image quality assessment in the wild. arXiv preprint arXiv:2405.18842, 2024.\n",
      "[58] Zhiyuan You, Zheyuan Li, Jinjin Gu, Zhenfei Yin, Tianfan Xue, and Chao Dong. Depicting\n",
      "beyond scores: Advancing image quality assessment through multi-modal language models. In\n",
      "European Conference on Computer Vision, pages 259–276. Springer, 2024.\n",
      "\n",
      "[59] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan\n",
      "Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior\n",
      "alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pages 13807–13816, 2024.\n",
      "\n",
      "[60] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen\n",
      "He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai\n",
      "feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024.\n",
      "\n",
      "[61] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng\n",
      "Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group\n",
      "relative policy optimization. arXiv preprint arXiv:2503.12937, 2025.\n",
      "\n",
      "[62] Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-enriched completely blind image quality\n",
      "\n",
      "evaluator. IEEE Transactions on Image Processing, 24(8):2579–2591, 2015.\n",
      "\n",
      "[63] Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. Fsim: A feature similarity index for\n",
      "\n",
      "image quality assessment. IEEE transactions on Image Processing, 20(8):2378–2386, 2011.\n",
      "\n",
      "[64] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\n",
      "sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\n",
      "conference on computer vision and pattern recognition, pages 586–595, 2018.\n",
      "\n",
      "[65] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and Zhou Wang. Blind image quality\n",
      "assessment using a deep bilinear convolutional neural network. IEEE Transactions on Circuits\n",
      "and Systems for Video Technology, 2020.\n",
      "\n",
      "[66] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao\n",
      "Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024.\n",
      "\n",
      "13\n",
      "\n",
      "\f[67] Zicheng Zhang, Ziheng Jia, Haoning Wu, Chunyi Li, Zijian Chen, Yingjie Zhou, Wei Sun,\n",
      "Xiaohong Liu, Xiongkuo Min, Weisi Lin, et al. Q-bench-video: Benchmarking the video quality\n",
      "understanding of lmms. arXiv preprint arXiv:2409.20063, 2024.\n",
      "\n",
      "[68] Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li,\n",
      "Zongyu Wang, Xuezhi Cao, Xiongkuo Min, et al. Q-eval-100k: Evaluating visual quality and\n",
      "alignment level for text-to-vision content. arXiv preprint arXiv:2503.02357, 2025.\n",
      "\n",
      "[69] Zicheng Zhang, Haoning Wu, Ziheng Jia, Weisi Lin, and Guangtao Zhai. Teaching lmms for\n",
      "\n",
      "image quality scoring and interpreting. arXiv preprint arXiv:2503.09197, 2025.\n",
      "\n",
      "[70] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond\n",
      "hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization.\n",
      "arXiv preprint arXiv:2311.16839, 2023.\n",
      "\n",
      "[71] Heliang Zheng, Huan Yang, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Learning conditional\n",
      "knowledge distillation for degraded-reference image quality assessment. In Proceedings of the\n",
      "IEEE/CVF International Conference on Computer Vision, pages 10242–10251, 2021.\n",
      "\n",
      "[72] Hancheng Zhu, Leida Li, Jinjian Wu, Weisheng Dong, and Guangming Shi. Metaiqa: Deep\n",
      "meta-learning for no-reference image quality assessment. In Proceedings of the IEEE/CVF\n",
      "conference on computer vision and pattern recognition, pages 14143–14152, 2020.\n",
      "\n",
      "[73] Hanwei Zhu, Haoning Wu, Yixuan Li, Zicheng Zhang, Baoliang Chen, Lingyu Zhu, Yuming\n",
      "Fang, Guangtao Zhai, Weisi Lin, and Shiqi Wang. Adaptive image quality assessment via\n",
      "teaching large multimodal model to compare. In The Thirty-eighth Annual Conference on\n",
      "Neural Information Processing Systems, 2024.\n",
      "\n",
      "[74] Kaiwen Zhu, Jinjin Gu, Zhiyuan You, Yu Qiao, and Chao Dong. An intelligent agentic system\n",
      "\n",
      "for complex image restoration problems. arXiv preprint arXiv:2410.17809, 2024.\n",
      "\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(extract_text_from_pdf_url(arxiv_results[0]['pdf_url']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
