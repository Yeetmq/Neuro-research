{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debian/miniconda3/envs/deep_learning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"/home/debian/develop/denis/Neuro-research/BART/bart-finetuned/final_model\"\n",
    "CHUNKS_DIR = \"/home/debian/develop/denis/Neuro-research/BART/data\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "SEED = 42\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры финальных датасетов:\n",
      "Train: 216086 samples\n",
      "Val: 27010 samples\n",
      "Test: 27013 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import aiohttp\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "arxiv = load_dataset(\"scientific_papers\", \"arxiv\", \n",
    "                     split=\"train\", \n",
    "                     trust_remote_code=True, \n",
    "                     storage_options={'client_kwargs': {'timeout': aiohttp.ClientTimeout(total=3600)}},\n",
    "                     cache_dir='/home/debian/.cache/huggingface/datasets')\n",
    "\n",
    "\n",
    "arxiv = arxiv.remove_columns(['section_names'])\n",
    "arxiv = arxiv.rename_column('abstract', 'summary')\n",
    "\n",
    "def load_filtered_dataset(data_root=\"/home/debian/develop/denis/Neuro-research/BART/converted_data/data/test\"):\n",
    "    data_path = Path(data_root)\n",
    "    return Dataset.from_json([\n",
    "        str(p) for p in data_path.rglob(\"*.json\")\n",
    "    ])\n",
    "\n",
    "patent_dataset = load_filtered_dataset()\n",
    "\n",
    "\n",
    "patent_dataset = patent_dataset.remove_columns(['publication_number', 'application_number'])\n",
    "patent_dataset = patent_dataset.rename_column('abstract', 'summary')\n",
    "patent_dataset = patent_dataset.rename_column('description', 'article')\n",
    "\n",
    "\n",
    "\n",
    "def split_and_combine_datasets(arxiv_ds: Dataset, \n",
    "                              patent_ds: Dataset, \n",
    "                              seed: int = 42,\n",
    "                              train_ratio: float = 0.8,\n",
    "                              val_ratio: float = 0.1) -> tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Разделяет каждый датасет на train/val/test и объединяет соответствующие части\n",
    "    \n",
    "    Параметры:\n",
    "    arxiv_ds: Датасет arXiv\n",
    "    patent_ds: Датасент патентов\n",
    "    seed: Сид для воспроизводимости\n",
    "    train_ratio: Доля тренировочных данных (0.0-1.0)\n",
    "    val_ratio: Доля валидационных данных (0.0-1.0)\n",
    "    \n",
    "    Возвращает:\n",
    "    (train, val, test) - объединенные датасеты\n",
    "    \"\"\"\n",
    "    \n",
    "    assert np.isclose(train_ratio + val_ratio + (1 - train_ratio - val_ratio), 1.0), \"Пропорции должны суммироваться к 1\"\n",
    "    \n",
    "    def split_single(ds: Dataset) -> tuple[Dataset, Dataset, Dataset]:\n",
    "        train_test = ds.train_test_split(\n",
    "            test_size=1-train_ratio, \n",
    "            seed=seed,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_test = train_test['test'].train_test_split(\n",
    "            test_size=val_ratio/(val_ratio + (1 - train_ratio - val_ratio)), \n",
    "            seed=seed,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return train_test['train'], val_test['train'], val_test['test']\n",
    "    \n",
    "    arxiv_train, arxiv_val, arxiv_test = split_single(arxiv_ds)\n",
    "    patent_train, patent_val, patent_test = split_single(patent_ds)\n",
    "    \n",
    "    combined_train = concatenate_datasets([arxiv_train, patent_train])\n",
    "    combined_val = concatenate_datasets([arxiv_val, patent_val])\n",
    "    combined_test = concatenate_datasets([arxiv_test, patent_test])\n",
    "    \n",
    "    return combined_train, combined_val, combined_test\n",
    "\n",
    "train_ds, val_ds, test_ds = split_and_combine_datasets(\n",
    "    arxiv_ds=arxiv,\n",
    "    patent_ds=patent_dataset,\n",
    "    seed=42,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1\n",
    ")\n",
    "\n",
    "print(f\"Размеры финальных датасетов:\")\n",
    "print(f\"Train: {len(train_ds)} samples\")\n",
    "print(f\"Val: {len(val_ds)} samples\")\n",
    "print(f\"Test: {len(test_ds)} samples\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_optimized(chunk_dir):\n",
    "    chunk_dirs = [\n",
    "        os.path.join(chunk_dir, d)\n",
    "        for d in sorted(os.listdir(chunk_dir))\n",
    "        if d.startswith(\"chunk\") and os.path.isdir(os.path.join(chunk_dir, d))\n",
    "    ]\n",
    "\n",
    "    chunk_files = []\n",
    "    for d in chunk_dirs:\n",
    "        filename = \"data-00000-of-00001.arrow\"\n",
    "        file_path = os.path.join(d, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            chunk_files.append(file_path)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {file_path} not found in directory {d}\")\n",
    "\n",
    "    if not chunk_files:\n",
    "        raise ValueError(f\"No valid chunk files found in {chunk_dir}\")\n",
    "\n",
    "    print(f\"Loading {len(chunk_files)} chunks from {chunk_dir}\")\n",
    "    return concatenate_datasets([\n",
    "        Dataset.from_file(f) for f in chunk_files\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "with open('/home/debian/develop/denis/Neuro-research/BART/page_content.txt', 'r', encoding='utf-8') as f:\n",
    "    example = f.read()\n",
    "\n",
    "for example in tqdm(test_ds, desc=\"Generating predictions\"):\n",
    "    inputs = tokenizer(\n",
    "        example, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(pred_text)\n",
    "    references.append(example[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the clustering properties of local , @xmath0  mjy , radio sources are investigated for a sample of 820 objects drawn from the joint use of the first and 2df galaxy redshift surveys . to this aim \\n , we present 271 new @xmath1 spectroscopic counterparts of first radio sources to be added to those already introduced in magliocchetti et al . \\n ( 2002 ) . \\n the two - point correlation function for the local radio population is found to be entirely consistent with estimates obtained for the whole sample of 2dfgrs galaxies . from measurements of the redshift - space correlation function \\n @xmath2 we derive a redshift - space clustering length @xmath3  mpc , while from the projected correlation function @xmath4 we estimate the parameters of the real - space correlation function @xmath5 , @xmath6  mpc and @xmath7 , where @xmath8 is assumed . \\n different results are instead obtained if we only consider sources that present signatures of agn activity in their spectra . \\n these objects are shown to be very strongly correlated , with @xmath9  mpc and @xmath10 , a steeper slope than has been claimed in other recent works . \\n no difference is found in the clustering properties of radio - agns of different radio luminosity . \\n comparisons with models for @xmath11 show that agn - fuelled sources reside in dark matter halos more massive than @xmath12 , higher the corresponding figure for radio - quiet qsos . \\n this value can be converted into a minimum black hole mass associated with radio - loud , agn - fuelled objects of @xmath13 . \\n the above results then suggest  at least for relatively faint radio objects  the existence of a threshold black hole mass associated with the onset of significant radio activity such as that of radio - loud agns ; however , once the activity is triggered , there appears to be no evidence for a connection between black hole mass and level of radio output .    \\n galaxies : active  galaxies : starburst  galaxies : statistics , distances and redshifts  cosmology : observations  radio continuum galaxies '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we investigate the clustering properties of @xmath0  mjy radio galaxies drawn from the joint use of the first and 2df galaxy redshift surveys as illustrated in magliocchetti et al . \\n ( 2002 ) . by doing this \\n , we not only extend the peacock & nicholson ( 1991 ) measurements to a statistically more significant sample involving less local objects , but we also probe much lower flux densities where the population contains radio - emitting sources that differ from typical agns ( such as galaxies undergoing intense star formation ) . in addition to the above analysis , we also estimate the two - point correlation function ( both in redshift space and real space ) for the homogeneous sample of radio galaxies ( sources that present signatures of agn activity in their optical spectra ) . '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: 0.3656\n",
      "ROUGE-2: 0.1436\n",
      "ROUGE-L: 0.2116\n",
      "\n",
      "BLEU Score:\n",
      "BLEU: 0.0076\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "rouge_results = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "bleu_results = bleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=[[ref] for ref in references]\n",
    ")\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\nBLEU Score:\")\n",
    "print(f\"BLEU: {bleu_results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'summary'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "def create_test_dataset(text_file: str) -> Dataset:\n",
    "    # Прочитать содержимое файла\n",
    "    with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Создать структуру для датасета\n",
    "    data = {\n",
    "        \"article\": [content],  # Весь текст в одной строке\n",
    "        \"summary\": [\"\"]        # Пустые строки для заполнения\n",
    "    }\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "test_dataset = create_test_dataset('/home/debian/develop/denis/Neuro-research/BART/example.txt')\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m MAX_INPUT_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "        example, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpred_text\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_text' is not defined"
     ]
    }
   ],
   "source": [
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A method to create a generic generic II chatbot (General AI chatbot). The method comprises the steps of: (a) generating a generic 2 chatbot; (b) creating a generic 3 chatbot from the generic 2 set of 2 chatbots; (c) constructing a generic 4 chatbot based on the generic 3 set of 4 chatbots, and (d) creating the generic 4 set of 3 chatbots based on a generic 5 chatbot, (e) generating an generic 3-set of 2-set 2-chats based on generic 3 sets of 2.5 An artificial neural network (ANT', ' we present the architectural design of the model architec-ture (GENMO) and elucidates how it unifies motion estimation and generation within a single model. The model is characterized by the following steps: (a) transforming a noisy motion se-quence xt with the conditions C and condtion masks M into a clean motion sequence x0 through a series of carefully de-signed components. The initial processing stage consists of an additive fusion block that converts xt into a sequence of motion tokens. This block utilizes dedicated mul-tilayer perceptrons (MLPs) to process each condition type type in C', ' we propose a dual-mode training paradigm for a diffusion model trained with the standard DDPM objective to generate motion sequences that satisfy the condition set C and mask M, so it can be used as a motion estima-tion model when provided with video cvideo or 2D skeleton c2d conditions. However, we found that such a generative training objective is not enough to generate accurate mo-tion sequences that are consistent with the input video. We observe a fundamental difference between GENMO and other diffusion models that are trained with this objective that satisfies the condition. \\n we show that GENMO can theoretically be trained with', \" a model for reciprocal generation of three-dimensional human motion and texts is presented. The model is based on a text-to-text transformer and includes the steps of: (a) providing a text to text representation of the motion capture data; (b) encoding the text to the text representation; (c) encoding a text in the form of an image; (d) encoding and encoding the image into a text representation, and (e) encoding it into an image of the image; and (f) encoding an image in the frame into a frame representing the frame', ' human motion in the wild using imus\", ' the model for multidimensional human motion GENMO comprises 16 layers, each of which consists of a ROPE-based Transformer block followed by a multi-text injection block. The ROPE block incorporates a LayerNorm, an ROPE attention layer with residual connections, and an MLP layer. Each atten-tion unit features 8 attention heads to capture diverse mo-tion patterns. The number of neurons in the MLP layers is dmlp = 1024. The multi- text injection block maintains a similar architecture to the ROPE and Transformer blocks, but replaces the standard attention with multiple-text attention, which processes', ' we present a novel method-ology for estimating human pose esti-mation under challenging conditions of extreme occlusion and truncation , simulated through strategic placement of random occlusions patches and frame truncations. Due to the constraints of our fea-ture representation , which lacks global root information, we only overwrite the local body poses and global root ori-entation for the keyframes . \\n we evaluate our approach on both the HumanML3D and Motion-X test sets under twoexperimental conditions: sampling either 2 or 5 keyframes from each test motion. For Motion, we utilize the recon']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "model_path = \"/home/debian/develop/denis/Neuro-research/BART/bart-finetuned/final_model\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def chunk_text(text, chunk_size=512, overlap=64):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        if len(chunks) == 512:\n",
    "            return chunks\n",
    "    return chunks\n",
    "\n",
    "def generate_summaries(chunks):\n",
    "    model.eval()\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "with open('/home/debian/develop/denis/Neuro-research/BART/test.txt', 'r', encoding='utf-8') as f:\n",
    "    example = f.read()\n",
    "\n",
    "text = example\n",
    "chunks = chunk_text(text, chunk_size=512, overlap=100)\n",
    "summaries = generate_summaries(chunks)\n",
    "\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT version of version 3.5, modified using the GPT model to follow the command model (InstructGPT model), was designed to create a special generic II chatbot (General AI chatbot).ChatGPT. Notes [to correct code] 12Vaswani A., Shazeer N., Parmar N.,USzkoreit B., Jones L., Gomez A. N., Kaiser . .,Polosukhin I.Attenation is All you Need,R. Advances in National Information Processing Systems 30I. In this paper , we present a method to create a generic generic II chatbot (General AI chatbot). The method comprises the steps of: (a) generating a generic 2 chatbot; (b) creating a generic 3 chatbot from the generic 2 set of 2 chatbots; (c) constructing a generic 4 chatbot based on the generic 3 set of 4 chatbots, and (d) creating the generic 4 set of 3 chatbots based on a generic 5 chatbot, (e) generating an generic 3-set of 2-set 2-chats based on generic 3 sets of 2.5 An artificial neural network (ANT) is a type of equipment used in mechanical translation, part in national language processing (NLP)The role of experts in the implementation of the ANT is discussed. The ANT can be used in a variety of applications, from mechanical translation to artificial neural networks (AI) to artificial intelligence (AI). An ANT has been used in the development of an AI. It has been shown that it can be implemented in a wide range of applications such as mechanical translation and artificial intelligence .    in this article \n",
      " , we will present an analysis of an ANT based on Transformers in Machine Learning are a type of equipment used in mechanical translation, part in national language processing (NLP)The role of experts in the field of human rights in the work of human human rights and fundamental freedoms in the research community in the fields of Human rights, fundamental freedoms and human rights, and fundamental freedom in the practice of human language processing. They can process all in mind positions in parallel, making them more effective.Long-Range Dependences: The self-attenistics are the modus operandi of the type to build patterns between technical standards, which is chalking for transferable articles.Scal The Transformer is used as a model when working on Cloud TPU. It is a black box. The Transformer has a coder component, decoded component, and the connections between them. Coding component is a stack of encoders; in the illustration below, we've shown 6 encoder above each other (there's nothing magical, you can experiment with any other number). Decoded component is an decoder in the same quantity. All encodered are identical in structure, though they have different weights. Each can be divided into two sub-layers: The input sequence that enters the enc In this article, we describe how the neural network of a neural network can be used to process the input sequence of a trained model into the output of the trained model. In particular, we show how the network of neural networks can be applied to the input sequences of the training model, and how the networks of neural network are used to generate the output sequence of the train model. Then we present a simple example of a shorter sentence that we want to translate. In this example, we use the concept of internal attention to describe how neural networks are used in neural networks. The neural network is designed to process each word (each The concept of Attention is All You Need is an important concept in the field of neural networks. It is a concept that has been widely used in the past to describe the understanding of other relevant words in a particular sentence. In this article \n",
      " we describe how this concept can be applied to a neural network and how it can be used to encode a word in the context of a given sentence. The concept is based on the idea that a word can be encoded in a way that allows the neural network to look at the other entry positions of the sequence and find a clue to help better encode the word. If you are familiar An encoder has a dimension of 512. They are not required to be smaller, but in our case, the choice of this model architecture is based on the desire to compute in a layer of multiple (multi-health vectors of embeds) than the embed vectors. The second step in calculating internal attention is to get a coefficient (score). Suppose we calculate internal attention for the first word in our example Thinking. We need to evaluate each word in the incoming sentence relative to this word. The coefficient determines how much to focus on other parts of the input sentence at the time the word is encoded in a particular entry. The In this article, we present an algorithm for calculating the layer of internal attention in a matrix form. The algorithm includes the steps of forming from the embeddings of the matrix X and multiplying it into the balance matrices that we have learned (WQ, WK, WV). Each row in matrix X corresponds to the word in the in-off sentence. We see again the difference in the dimensions of the ambedding vectors (512, or 4 squares in the figure) and the vectors of qkv (64, or 3 squares). With matrices, we can squeeze steps 2-6 into a In this paper, we present a model for the presentation of order in sequence using position coding. The model is based on the idea that the model remembers and coexamples the order of the words in the input sentence. To solve this problem, the Transformer adds a vector to each incoming embedding. These vectors have a certain pattern that can be used in the process of projecting them into QKV vectors and the scalar work when calculating attention. We add the position coding vectors, the values of which follow a particular pattern. If we believe that the embedding has a dimension of 4, then the actual position coding The encoder architecture of the Transformer is described in terms of two stacks of encoders and decoders. The encoder part of the decoder part has a residual connection around it, which follows a residual link between the encoder and decoder layers of attention. The decoder layer of attention is used by all decoder in their encoder layer to focus on the appropriate locations in the incoming sentence. The decoding phase begins. Each phase of the decode phase returns the output element (in this case, the translation sentence in English). \n",
      "e steps are repeated until a special symbol indicates that the Trans In this paper, we describe the process of learning a model using a neural network. The neural network is a simple fully connected neural network that translates the vector created by the decoder grid into a much larger vector called the logic vector (logits vector). This is done by masking all the entries after the current (inf) before the softmax stage in calculating the internal attention. The encoder layer works as multiple attention, except that it creates a request matrix from the layer below and takes the keys and values of the exit of the encoders. This is the linear layer and the next layer of softmax.  in this paper, we describe the main points that are going on during model learning. At the time of learning, the still untrained model will go through the exact same algorithm we described earlier. But because we're training it on a marked learning frame, we can compare the output to the available reference frame. For visualization, let's assume that our dictionary consists of only six words (a, am, i, tanks, student and (end of sentence). Our model output dictionary is created at the pre-process stage even before the start of the training. As only we have identified our dictionary, we use a vector of GENMO is a model for human motion estimation and generation in a single framework and supports diverse conditioning signals in-cluding monocular videos, 2D keypoints, text . \n",
      " it is useful and would be a starting point in the study of Transformer. If you want to study the topic in more depth, you suggest the following steps: read the article Actence is All You Need, posts on Transformer (Transformer: A Novel Neural Network Architecture for Language Understunning).  GENMO is a unified General-ist Model for Human MOtion that bridges motion estima-tion and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy unobserved conditioning signals. Leveraging the synergy be-tween regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided train-ing objective that exploits in-the-wild videos with 2D anno-tations and text descriptions to enhance generative diver-sity. Furthermore, our novel architecture handles variable- Genera-tive models are a generalist frame-work that successfully handles multiple human motion tasks within a single model. This unified approach creates synergistic benefits:generative priors improve estimated motions under chal-lenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO’s effectiveness as an efficient frame-works that successfully handle multiple human motions tasks within one model.  generative priors enhance plausibility in challenging motion estimation scenarios while diverse video data enrich generative diversity without requiring ground-truth 3D annotations. \n",
      " GENMO is built upon a diffusion model framework in-corporating a novel dual-mode training paradigm: (1) es-timation mode, where we feed the GENMO diffusion de-noiser with zero-initialized noise and the largest diffusion-timestep, forcing the model to produce maximum likelihood-estimation (MLE) of the motion based on the conditional signals. (2) generation mode, follows traditional diffusion training by sampling noisy motions and GENMO is the first generalist model unify-ing state-of-the-art global motion estimation with flex-ible human motion generation conditioned on videos, music, text, 2D keypoints, and 3D keyframes. \n",
      " our architecture design supports seamless generation of variable-length motions conditioned on arbitrary num-bers and combinations of multimodal inputs without requiring complex post-processing steps. We propose a novel dual-mode training paradigm to ex-plore the synergy between regression and diffusion, and introduce an estimation-guided training objective that effectively leverages in- the-wild videos with 2D annotations Human motion generation has progressed significantly in recent years [3, 7, 10–12, 20, 21, 23, 24, 53, 57, 62, 70, 92] leveraging multiple input modalities without com-plex post-processing. However, most existing methods focus solely on generative tasks without supporting estimation. For in-stance, the method [85] supports video input but treats it as a generative task, resulting in motions that loosely imitate video content rather than precisely matching it. In contrast, our method jointly handles generation and estimation tasks, yielding more precise video-conditioned results. For long GENMO unifies motion estimation and generation by formulating both tasks as conditional motion generation. Specifically, it synthesizes a human motion sequence x of length N based on a set of condition signals C and a setof corresponding condition masks M, where N can be ar-bitrarily large. The condition set C includes one or more of the following: video feature cvideo ∈ RN ×dvideo, camera feature ccam ∉ RN ×dcam, 2D MO unifies motion estimation and generation by formulating both tasks as conditional motion generation. Specifically, it synthesizes a human motion sequence x of the same length N based on a set of condition signals C and a setof corresponding condition masks M, where N can be ar-bitrarily large. The condition set C includes one or more of the following: video feature cvideo ∈ RN ×dvideo, cameramotion ccam , 2D skeleton c2d ∈ N ×d2d, music ctext ∈ RM ×dtext that describes the mo-tion where M is the number of text tokens.  we present the architectural design of the model architec-ture (GENMO) and elucidates how it unifies motion estimation and generation within a single model. The model is characterized by the following steps: (a) transforming a noisy motion se-quence xt with the conditions C and condtion masks M into a clean motion sequence x0 through a series of carefully de-signed components. The initial processing stage consists of an additive fusion block that converts xt into a sequence of motion tokens. This block utilizes dedicated mul-tilayer perceptrons (MLPs) to process each condition type type in C A novel multi-text injection block that facili-tates text-conditioned motion generation while accommo-dating multiple text inputs (K) with user-specified time window boundaries is proposed. The multi-input injection block comprises a trans-former block with our proposed multi -text attention mech-anism at its core. As depicted in Figure 3, the multi-context at-tention mechanism processes K text embedding sequences (text, c2c1text ) alongside the input motion feature se-quence fin to generate the output feature sequence fout:text, . . . , cK. Multi-text attention enables flexible conditioning with multiple text inputs, each constrained to its specified time window. Although the mask introduces disconti-nuities at time window boundaries, GENMO successfully generates smooth motion sequences through the subsequent RoPE-based transformer block, which effectively captures and models temporal motion dynamics. Inference with Arbitrary Motion Length. Our ar-chitecture employs relative positional embedding rather than absolute embeddings for motion sequences, allowing the generation of motions of arbitrary length in a single diffusion forward pass while naturally incorporating mul-tiple text inputs across different time spans. During infer-ence, we adopt GENMO is a diffusion model trained with the standard DDPM objective to generate motion sequences that satisfy the condition set C and mask M, so it can be used as a motion estima-tion model when provided with video cvideo or 2D skeleton c2d conditions. However, we found that such a generative training objective is not enough to generate accurate mo-tion sequences that are consistent with the input video. We observe a fundamental difference between GENMO and other diffusion models that are trained with this objective. \n",
      " we show that GENMO can theoretically be trained with a generic training objective that satisfies the condition  we propose a dual-mode training paradigm, which consists of (1) an estimation mode and (2) a generation mode. In the estimation mode, we formu-late the problem as a regression task, employing maxi-mum likelihood estimation to learn the conditional distribu-tion q(x|C, M). This approach yields the following mean-square error (MSE) objective:Lest = Ez∼N (0,I)(cid):2)(cid:13)(did:4)x0 − G(z, T, C, M)(  we introduce a new generation mode for motion se-quences based on geometric regularization losses . \n",
      " the generation mode involves decoding the predicted motion sequences into SMPL joints and vertices, followed by the application of constraints on world-space and camera-space vertices po-sitions, world-spatial joint positions, and joint contacts. In scenarios where only 2D annotations are available, we employ a 2D reprojection loss to effectively regularize the predicted motions sequences. For data with clean 3D annotations x0, we can directly employ the standard diffusion objective in Eq. 4 to train the generation  we propose a generation-guided generation-training strategy based on a pseudo-clean mo-tion from the estimation mode using video or 2D skeleton as conditions: ˆx0 = G(z, T, C). Subsequently, we sample a noisy motion sequence  \n",
      " through the forward diffusion process: q(cid:0)ˆxt|ˆX0)(Cid:1). We then apply a 2D reprojection loss on the predicted clean motion using the 2D keypoint annota-tions x2d:Lgen-2D. For the gen-er GENMO is trained on diverse datasets with strong conditioning signals that render the motion distribution more deterministic, such as video or 2D skeletons, we utilize both the estimation and genera-tion modes to train GENMO. Conversely, when training ondatasets with abstract conditions that result in more gener-ative motion distributions , such as text and music, we ex-clusively employ the generation mode. This mode selection technique is applied to both 3D and 2D data. GENMO is a method for recovering global hu-man motion from videos with dynamic cameras. The method is based on the uni-fied motion generation and estimation framework, where the generative prior enhances the quality of reconstructed motions. GENMO also demonstrates superior performance on the RICH dataset compared to all existing methods. Ex-tensive qualitative results are provided in the supplementary GENMO is benchmarked against SOTA methods and a specialized variant of our model trained exclusively on AIST++ for music-to-dance generation. Notably, our generalist model, jointly trainedacross multiple estimation and generation tasks, demon-strates substantially enhanced motion diversity, physicalplausibility, and motion-music correlation, as evidenced by superior Divk, Divm, PFC, and BAS metrics. While this performance differential is expected given that our generalists model was trained on considerably more heterogeneous motion data spanning multiple tasks and domains. \n",
      " Qualitative Results. We provide extensive qualitative re-s  we evaluate the text-to-motion generation capabilities of GENMO on both Hu-manML3D (Table 4) and Motion-X (Table 5) datasets. Our generalist model was trained on considerably more heterogeneous motion data spanning multiple tasks and domains. We evaluate the global motion quality on the EMDB-2 [31] dataset and RICH [27]. \n",
      " we provide extensive qualitative re-sults in the supplementary videos, demonstrating the effec-tiveness and versatility of the GENMO.    [ firstpage ]   we evaluate the camera-space motion quality on the 3DPW [73], RICH [27] and EMDB-1 [31] datasets . \n",
      " we find that there is a strong correlation between the motion of the camera and the position of the object. \n",
      " this correlation can be explained by the fact that the object moves in a direction perpendicular to the axis of the plane and the direction of the axis .   the 3DPW [73], RICH [27] and EMDB-1 [31] datasets have been used to study the dynamics of a model in a 2D environment . \n",
      " the models have been studied in the 2d environment and in the 3D environment using the 3-D model .    in this paper \n",
      " we present the results of the 2-D modeling of the model in 2D and 3-d environments using the 4-d model and the 3 -D model with the 4 -d model with a 4 - d model. \n",
      " our results are in good agreement with  we present the results of a systematic study of the dynamics of motion in a two - dimensional system . \n",
      " we find that the dynamics can be described by the following equation :    ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) ( 10 ) ( 11 ) ( 12 ) ( 13 ) ( 14 ) ( 15 ) ( 16 ) ( 17 ) ( 18 ) ( 19 ) ( 20 ) ( 21 ) ( 22 ) ( 23 ) ( 24 ) ( 26 ) ( 28 ) ( 27 ) ( 29 ) ( 30 ) The concept of text-to-motion generation is evaluated on the AIST++ [42] dataset . \n",
      " it is shown that the concept can be applied to a wide range of applications. \n",
      " examples are given.    [ firstpage ]  GENMO utilizes SMPL pa-rameters to represent human motion for unified estimation and generation, whereas SOTA methods employ the Hu-manML3D representation — the same representation used by the encoders of the FID and R-Precision metrics. This representation mismatch introduces an inherent disad-vantage for GENMO, as it necessitates bidirectional conver-sion of ground . \n",
      " the results indicate that GENMO exhibits enhanced motion gener-ation performance within GENMO’s framework for text-conditioned motion generation tasks. This discrepancy can be stemmed from our representation choice: GENMO utilizes  we propose a new method for the generation of human motion using the standard diffusion objective (Lgen-2D) for training and conversion of our generated motions to the HumanML3D format during evaluation. \n",
      " this method is based on the re-gression baseline, which is the same representation used by the encoders of the FID and R-Precision metrics during training. This new method introduces an inherent disad-vantage for GENMO, as it necessitates bidirectional conver-sion of ground-truth motions from HumanML 3D to SMPL during training and converting the generated motion to the A variant of our proposed estimation mode is trained exclusively with the generation mode (“Diffusion-only”). \n",
      " we evaluate the efficacy of our method on the RICH and EMDB datasets, using direct model pre-dictions without post-processing for static joints. The re-sults demonstrate that omitting the estimation objective sig-n achieves superior performance through its unified estimation and generation training compared to the diffusion-only baseline. Furthermore, the incorporation of additional 2D-only data and joint training with video-conditioned motion estimation substantially enhances mo-tion in-betweening quality. To assess the The impact of inference steps on motion generation and esti-mation performance. To assess the efficacy of our proposed estimation mode, we evaluate a variant of our method trained exclusively with the generation mode (“Diffusion-only”). Table 7 presents quantitative compar-isons of global human motion estimation performance on the RICH and EMDB datasets, using direct model pre-dictions without post-processing for static joints. The re-sults demonstrate that omitting the estimation objective sig-nificantly degrades global motion estimation performances, confirming the method’s crucial role in en-hancing consistency between predicted GENMO is a generalist frame-work for human motion modeling that bridges the gap be-tween motion estimation and generation tasks. \n",
      " GENMO can effectively leverage shared representa-tions to enable synergistic benefits: generative priors en-hance motion estimation robustness under challenging con-ditions, while diverse video data enriches the generativecapabilities. GENMO is not only capable of handling multiple human motion tasks within a single framework but also achieves superior results compared to task-specific models. As with any other work, GENMO has some limitations.Currently, it relies on off-the  we present a model for 3D human motion in which the motion of the human body is described by a diffusion model. \n",
      " the diffusion model is based on a set of diffusion models and is capable of predicting the shape of the body by means of the diffusion models . \n",
      " this model can be used to predict the facial expressions and hand articulation of 3D humans in a wide range of situations.    2  Human motion prediction from a single image is described in terms of a latent consistency model. In this model, \n",
      " the motion diffusion in latent space is described as a function of the length of the image and the shape of the object. \n",
      " this model can be used to predict 3d human pose and shape from a multi-image video. In particular, it can be applied to 3d geometry in the presence of a static feature in the image. In the present paper, we present the diffusion priors for 3d geometric geometry esti-mation from a one-dimensional image.    [ firstpage A model for the reciprocal gener-ation of 3d human motions and texts is presented. The model is based on the notion of a three-dimensional ( 3d ) human motion and texts. \n",
      " the model includes the steps of: (a) generating diverse and natural 3d motions from text; and (b) generating a 3D human motion from text from the text; (c) generating the 3d motion from a text from a textual description; (d) constructing the 3D motion from the textual description, and (e) creating a 3d image of the 3-dimensional human motion; (f A model for the reciprocal gener-ation of 3d human motions and texts is presented in the framework of a variational autoen-coder for human motion synthesis . \n",
      " the model is based on a two time-scale update rule and a local nash equilib-rium. The model can be trained by a two-time scale update rule or by a three-time scaling update rule. \n",
      " it is shown that the model can also be trained with a three time scale update .    in this paper \n",
      " we present a model for reciprocal generation of three-dimensional human motion and texts. A model for 3D human motion and shape in natural environments is presented. The model is based on a diffusion model with hierarchical semantic graphs. The diffusion model is able to recover the shape and shape of the human body in the presence of a natural environment. \n",
      " the diffusion model can be used to reconstruct the shape of a human body from a set of images of the natural environment . \n",
      " it is shown that the model can also be used for reconstructing the shape or shape of an object in the environment by means of the diffusion process.    in this paper \n",
      " we present a model for the 3D Human and motion estimation from in-the-wild videos is an important task in computer vision and pattern recognition (CVPR). In this paper \n",
      " we present a novel method for the estimation of human and motion in the presence of an object. The method includes the steps of: (a) generating an image of the object; (b) generating a frame of the image; (c) generating the frame of a frame; and (d) constructing a frame for the frame; (e) creating a frame in the frame, and (f) encoding the frame into a frame representing the frame. (g) generating In this paper we present a new algorithm for 3d human pose and shape estimation. The algorithm is based on a combination of apertures with invert-ible neural networks and a head-mounted camera. \n",
      " the algorithm can be applied to a wide range of human motion datasets. The algorithms can be implemented in a variety of ways. For example, the algorithm has been applied to 3d motion capture with smartwatches and to a large-scale 3d expressive whole-body human motion dataset. In this paper \n",
      " we present an algorithm that can be used to perform 3d image estimation with the help of Motion-x: An ad-vanced multimodal, multitask framework for motion com-prehension and generation. In NeurIPS, 2023. 2, 3, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 34 , 35, 34    [ firstpage ]  A 3d human motion model for robust pose estimation is presented in the context of text-conditioned 3D human motion generation. The model is based on a text-to-text transformer and includes the steps of: (a) providing a text to text representation of the motion capture data; (b) encoding the text to the text representation; (c) encoding a text in the form of an image; (d) encoding and encoding the image into a text representation, and (e) encoding it into an image of the image; and (f) decoding the image to encode the image in the image representation. Human motion recovery via gravity-viewcoordinates using wasserstein GAN is described in the context of human motion gen-eration . \n",
      " it is shown that a 3d human motion model for robust pose estimation and shape estimation can be achieved by using a three-dimensional ( 3d ) model with accu-rate 3d motion. \n",
      " the 3d model can be used to reconstruct the world-grounded human motion recovery by using an adaptive neural network (ICDSP). \n",
      " this model can also be used as a model for the human motion generation by actor-critic gpt with chore Computer vision and Pattern Recognition, pages 11050–11059, 2022. 2, 7, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 36 , 37, 37 , 38, 42, 39, 41, 42 , 42, 43, 44, 42    [ firstpage ]  Human motion in the wild using imus and a moving camera is studied in the context of a 3d human motion dif-fusion model . \n",
      " the model includes the steps of: (a) moving the camera (b) in a direction perpendicular to the direction of the human motion (c) and (d) moving a camera (c), and (e) moving an object (d), such that the motion of the object (e.g. a human) is determined by the movement of the moving object (c); (f) moving in the direction perpendicular thereto and (g) moving with the Human motion generation in 3d scenes with text control is described in terms of the following steps: (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (v) (r) (q) (p) (a), (b), (c), (d), (e). (f), (g), (f). (g). (c). (d). (e.g. (d ) (f ) (h). ) (g ) (c ) (d. (e), (h), ( In this paper we present a model for multi-modal human motion reconstruction via diffusion. \n",
      " the model is based on a diffusion model for the human mesh recov-ery with dynamic cameras. The model consists of two components: a first component and a second component. The first component is composed of a first and second components. The second component is comprised of a third component, consisting of a fourth component. In the second component, the third component consists of a fifth component and the fourth component are composed of an eighth component.    in this paper \n",
      " we present the model for multidimensional human motion GENMO comprises 16 layers, each of which consists of a ROPE-based Transformer block followed by a multi-text injection block. The ROPE block incorporates a LayerNorm, an ROPE attention layer with residual connections, and an MLP layer. Each atten-tion unit features 8 attention heads to capture diverse mo-tion patterns. The number of neurons in the MLP layers is dmlp = 1024. The multi- text injection block maintains a similar architecture to the ROPE and Transformer blocks, but replaces the standard attention with multiple-text attention, which processes text embedding sequences to enrich and update GENMO is trained from scratch on a diverse set of mixed motion datasets, includ-ing motion estimation datasets AMASS [48], BED-LAM [5], Human3.6M [28], 3DPW [73], music-to-dance dataset AIST++[42], and text-to -motion datasets Hu-manML3D [17] and Motion-X [44]. Since motion data in AMASS and Human3D are represented in their own format, we con-vert them to SMPL parameters with inverse kinematics [39]for training. For AMASS  we implement a strategy analogous to Lgen-2D: we first generate pseudo-clean global human trajectories from the estimation mode, then utilize these to produce noisy mo-tions for training the generation mode, with loss computa-tion restricted to local poses. For AIST++, training incor-porates video frames, 2D keypoints, and music as condi-tions. Regarding the camera condition, we utilize ground-truth camera trajectories as the input condition for datasets that either provide such trajectories or feature static cam-eras; for datasets lacking labeled camera trajectoryories, we employ DROID  we evaluate the generation capabilities of a music-to-dance generation model on the AIST++ [42] dataset. We configure the sequence length to N = 120 for training, while maintaining support for variable sequence lengths during inference. The model is trained from scratch for 500 epochs using the AdamW op-timizer [46], with a mini-batch size of 128 per GPU dis-tributed across 2 A100 GPUs. Following established protocols [42, 71], our evalu-ation encompasses four key aspects: motion quality, gener-1212-representation diversity, physical plausibility, and motion-  we report the vari-ance across five different inference trials on HumanML3D using the pre-trained text and motion encoders from [17] after converting our motionrepresentation to the HumanML 3D format. This conversion process involves first recovering the SMPL parameters from our raw representation and subsequently deriving the humanML3d-format representation as described in [17],employing the neutral gender SMPL model. Consistent with established evaluation protocols, \n",
      " we report that vari-ances across five separate inference trials have been measured using the same one-in-all checkpoint is employed for evaluation as used in Motion-X text prompts lack frame-based keywords, neces-sitating a different approach to text encoding. We employed the method-ology established in prior diffusion-based approaches [39] for motion in-betweening evaluation, wherein the noisy motion is overwritten with desired poses at specified keyframes prior to each denoising step. The same one-in-all checkpoint is employed for evaluation as used in all other tasks. Due to the constraints of our fea-ture representation, which lacks global root information, we only overwrite the local body poses and global root ori-entation for the key  we present a novel method-ology for estimating human pose esti-mation under challenging conditions of extreme occlusion and truncation, simulated through strategic placement of random occlusions patches and frame truncations. Due to the constraints of our fea-ture representation, which lacks global root information, we only overwrite the local body poses and global root ori-entation for the keyframes. We evaluate our approach on both the HumanML3D and Motion-X test sets under twoexperimental conditions: sampling either 2 or 5 keyframes from each test motion. For Motion, we utilize the recon-structed  we introduce a unified framework that seamlessly inte-grates motion generation and estimation within a single co-herent model. In contrast to previous methods that com-promise generative capabilities during the fine-tuning pro-cess, our framework maintains both the stochastic diversity essential for high-quality generation and the deterministic precision required for accurate estimation. This dual ca-pability represents a significant advancement in leveraging generative priors for human motion understanding.   \n",
      " [ firstpage ]   we present a new method for studying the motion of a human body in the presence of an external magnetic field . \n",
      " the method is based on the use of a single magnetic field , which is used to measure the magnetic field of the body .    in this paper \n",
      " we show that the method can be applied to a wide range of physical phenomena , including the effects of magnetic field and magnetic field on the dynamics of a body . in particular , we demonstrate that the technique can be used to study the behavior of a system in the absence of magnetic fields , and that it can also be used for the study of\n"
     ]
    }
   ],
   "source": [
    "print(*summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5933 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "4082\n",
      "-------------\n",
      "1232\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model_path = \"/home/debian/develop/denis/Neuro-research/BART/bart-finetuned/final_model\"\n",
    "CHUNKS_DIR = \"/home/debian/develop/denis/Neuro-research/BART/data\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "SEED = 42\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = model.to(device)\n",
    "def chunk_text(text, chunk_size=512, overlap=64):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        if len(chunks) == 512:\n",
    "            return chunks\n",
    "    return chunks\n",
    "\n",
    "def generate_summaries(chunks):\n",
    "    model.eval()\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def get_summarisation(content_file_path: str, ):\n",
    "    with open(content_file_path, 'r', encoding='utf-8') as f:\n",
    "        example = f.read()\n",
    "\n",
    "    text = example\n",
    "    while len(text.split()) > 2048:\n",
    "        print('-------------')\n",
    "        print(len(text.split()))\n",
    "        chunks = chunk_text(text, chunk_size=512, overlap=100)\n",
    "        summaries = generate_summaries(chunks)\n",
    "        text = ''\n",
    "        for item in summaries:\n",
    "            text += item\n",
    "        print('-------------')\n",
    "        print(len(text.split()))\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "content_file_path='/home/debian/develop/denis/Neuro-research/test/input.txt'\n",
    "summaries = get_summarisation(content_file_path=content_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232\n"
     ]
    }
   ],
   "source": [
    "len(summaries)\n",
    "\n",
    "text = ''\n",
    "for i in summaries:\n",
    "    text+=i\n",
    "print(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a family of neural networks that have revolutionized natural language processing (NLP) and have become the backbone of many modern AI applications. Transformers are designed to process sequential data, most prominently associated with large language models (LLMs), which have achieved elite performance in other fields of AI, such as computer vision, speech recognition, and time series forecasting. Transformers are characterized by their self-attention mechanism, which allows for parallelization and the ability to generate query, key, and value vectors for each part of an input sequence simultaneously. This quality of transformers has enabled them to process long-range dependencies and make decisions about how and when to focus on specific time steps of that sequence. Transformers have also achieved elite performance in other fields of AI, such as computer vision, speech recognition, and time series forecasting. This paper discusses the transformer architecture and the accuracy records it set for machine translation. Theorems (3-5 complex sentences). The paper forwards, specialized in theo\n",
      "- Javier, and aims for large language generation model. This article isfit for large-scale andelite database of\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_structured_report(summaries):\n",
    "    prompt = \"\"\"<|user|>: Write a single continuous paragraph that seamlessly integrates these key points:\n",
    "    {}\n",
    "    \n",
    "    Guidelines:\n",
    "    1. Connect ideas using transitional phrases (\"furthermore\", \"however\", \"this suggests\")\n",
    "    2. Maintain logical flow between sentences\n",
    "    3. Avoid section headings or bullet points\n",
    "    4. Use academic linking: \"Building on this\", \"An important corollary\"\n",
    "    5. Keep technical terminology\n",
    "    \n",
    "    Output must be one cohesive paragraph, not thesises (3-5 complex sentences). <|assistant|>:\"\"\".format(\"\\n\".join(f\"- {s}\" for s in summaries))\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.4,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    generated_text = response.split(\"<|assistant|>:\")[-1].strip()\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "report = generate_structured_report(summaries)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A method to create a generic generic II chatbot (General AI chatbot) is presented. The method comprises the steps of:\n",
      "    1. Generating a generic 2 chatbot:\n",
      "        a. Creating a generic 2 set of 2 chatbots:\n",
      "            b. Generating a generic 3 chatbot:\n",
      "                c. Creating a generic 4 chatbot:\n",
      "                    d. Creating the generic 4 set of 3 chatbots:\n",
      "                        e. Generating an generic 3-set of 2-set 2-chats based on generic 3 sets of 2.5\n",
      "    2. Architectural design of the model architecture (GENMO)\n",
      "        a. Elucidates how GENMO unifies motion estimation and generation within a single model\n",
      "        b. Presents the steps of transforming a noisy motion sequence xt with the conditions C and condition masks M into a clean motion sequence x0 through a series of carefully de-signed components\n",
      "        c. Presents a dual-mode training paradigm for a diffusion model trained with the standard DDPM objective to generate motion sequences that satisfy the condition set C and mask\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
